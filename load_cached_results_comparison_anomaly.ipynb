{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cached Results Comparison - Anomaly Data\n",
    "\n",
    "This notebook compares synthetic anomaly data generated by various methods against:\n",
    "1. **Train Dataset**: `/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv`\n",
    "2. **Extended Dataset**: `/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS_EXTENDED/all_incidents_anomalies_extended_reinforced.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Analysis timestamp: {TIMESTAMP}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Define paths to all synthetic anomaly dataset results directories.\n",
    "\n",
    "**IMPORTANT**: Modify the `SYNTHETIC_ANOMALY_DATASETS` dictionary below to include your synthetic data paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION - Modify these paths to match your anomaly data\n# =============================================================================\n\n# Base path for results\nRESULTS_BASE_PATH = '/home/smotaali/BGP_Traffic_Generation/results_evaluation_anomaly'\n\n# Output directory\nOUTPUT_DIR = f'{RESULTS_BASE_PATH}/comprehensive_evaluation/{TIMESTAMP}'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n# Reference datasets for anomaly comparison\n# 'same' = comparison against the same/train dataset (all_incidents_anomalies_reinforced_v2.csv)\n# 'diff' = comparison against a different/extended dataset (all_incidents_anomalies_extended_reinforced.csv)\nREFERENCE_DATASETS = {\n    'same': '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv',\n    'diff': '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS_EXTENDED/all_incidents_anomalies_extended_reinforced.csv'\n}\n\n# =============================================================================\n# SYNTHETIC ANOMALY DATASETS - ADD YOUR PATHS HERE\n# Format: 'MethodName': {\n#     'same': 'path/to/results/comparing/against/same/train/dataset',\n#     'diff': 'path/to/results/comparing/against/different/extended/dataset'\n# }\n# =============================================================================\n\nSYNTHETIC_ANOMALY_DATASETS = {\n    # MODIFY THESE PATHS TO YOUR ACTUAL RESULT DIRECTORIES\n    # Each result directory should contain enhanced_v3_summary.csv and PNG files\n\n    # GAN Default Values\n    'GAN_LSTM_default': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_LSTM_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_LSTM_same'\n    },\n    'GAN_TimeGAN_default': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_TIME_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_TIME_same'\n    },\n    'GAN_DoppelGanger_default': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_Doppelganger_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_Doppelganger_same'\n    },\n\n    # GAN Enhanced/Tuned Parameters\n    'GAN_LSTM_enhanced': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_LSTM_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_LSTM_same'\n    },\n    'GAN_TimeGAN_enhanced': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_TIME_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_TIME_same'\n    },\n    'GAN_DoppelGanger_enhanced': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_Doppelganger_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_Doppelganger_same'\n    },\n\n    # SMOTE Variants\n    'SMOTE_normal': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/normal_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/normal_same'\n    },\n    'SMOTE_borderline': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/borderline_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/borderline_same'\n    },\n    'SMOTE_kmeans': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/kmeans_v3_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/kmeans_v3_same'\n    },\n    'SMOTE_adasyn': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/adasyn_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/adasyn_same'\n    },\n\n    # Hybrid (SMOTE + GAN)\n    'Hybrid_SMOTE_GAN': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/synthetic_hybrid/compare_hybrid_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/synthetic_hybrid/compare_hybrid_same'\n    },\n\n    # Copula\n    'Copula': {\n        'diff': '/home/smotaali/BGP_Traffic_Generation/results_zend/copula_diff',\n        'same': '/home/smotaali/BGP_Traffic_Generation/results_zend/copula_same'\n    }\n}\n\n# PNG files to collect and combine\nPNG_FILES = {\n    'tsne': 'enhanced_v3_tsne_overlay.png',\n    'correlation_comparison': 'enhanced_v3_correlation_comparison.png',\n    'correlation_scatter': 'enhanced_v3_correlation_scatter.png',\n    'distribution_comparison': 'enhanced_v3_distribution_comparison.png',\n    'distribution_tests': 'enhanced_v3_distribution_tests.png',\n    'effect_sizes': 'enhanced_v3_effect_sizes.png',\n    'pca_centroid': 'enhanced_v3_pca_centroid_analysis.png',\n    'quality_dashboard': 'enhanced_v3_quality_dashboard.png',\n    'top_k_worst': 'enhanced_v3_top_k_worst_features.png',\n    'calibration': 'calibration_check_visualization.png'\n}\n\n# Key metrics configuration\nKEY_METRICS = {\n    'Mean KS Statistic': {'direction': 'lower', 'weight': 1.5},\n    'Mean Wasserstein Distance': {'direction': 'lower', 'weight': 1.5},\n    'Weighted Wasserstein Distance': {'direction': 'lower', 'weight': 1.5},\n    'PCA Centroid Distance': {'direction': 'lower', 'weight': 1.0},\n    'Correlation Similarity (Pearson)': {'direction': 'higher', 'weight': 2.0},\n    'Correlation Similarity (Spearman)': {'direction': 'higher', 'weight': 2.0},\n    'Distribution Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n    'Distribution Score (Unweighted)': {'direction': 'higher', 'weight': 1.0},\n    'Correlation Score': {'direction': 'higher', 'weight': 1.5},\n    'Effect Size Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n    'Effect Size Score (Unweighted)': {'direction': 'higher', 'weight': 1.0},\n    'Wasserstein Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n    'KS Excellent Features': {'direction': 'higher', 'weight': 1.0},\n    'KS Good or Better Features': {'direction': 'higher', 'weight': 1.0},\n    'Negligible Effect Features': {'direction': 'higher', 'weight': 1.0},\n}\n\nprint(f\"Output directory: {OUTPUT_DIR}\")\nprint(f\"Number of methods configured: {len(SYNTHETIC_ANOMALY_DATASETS)}\")\nprint(f\"Number of plot types to combine: {len(PNG_FILES)}\")\nprint(f\"\\nReference datasets:\")\nfor name, path in REFERENCE_DATASETS.items():\n    exists = os.path.exists(path) if path else False\n    status = 'EXISTS' if exists else 'NOT FOUND'\n    print(f\"  {name}: {path} [{status}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_summary_csv(directory: str) -> Optional[str]:\n    \"\"\"Find the enhanced_v3_summary.csv file in a directory.\"\"\"\n    if not os.path.exists(directory):\n        return None\n    summary_file = os.path.join(directory, 'enhanced_v3_summary.csv')\n    if os.path.exists(summary_file):\n        return summary_file\n    \n    # Try to find any summary.csv file\n    for f in os.listdir(directory):\n        if 'summary' in f.lower() and f.endswith('.csv'):\n            return os.path.join(directory, f)\n    \n    return None\n\n\ndef load_summary_csv(filepath: str) -> Optional[Dict]:\n    \"\"\"Load a summary CSV and convert to dictionary.\"\"\"\n    try:\n        df = pd.read_csv(filepath)\n        if 'Metric' in df.columns and 'Value' in df.columns:\n            result = {}\n            for _, row in df.iterrows():\n                metric = row['Metric']\n                value = row['Value']\n                # Handle strings like \"100.0/100\"\n                if isinstance(value, str) and '/' in value:\n                    try:\n                        value = float(value.split('/')[0])\n                    except:\n                        pass\n                else:\n                    try:\n                        value = float(value)\n                    except:\n                        pass\n                result[metric] = value\n            return result\n        else:\n            # Try first two columns\n            result = {}\n            cols = df.columns.tolist()\n            for _, row in df.iterrows():\n                metric = row[cols[0]]\n                value = row[cols[1]]\n                if isinstance(value, str) and '/' in value:\n                    try:\n                        value = float(value.split('/')[0])\n                    except:\n                        pass\n                else:\n                    try:\n                        value = float(value)\n                    except:\n                        pass\n                result[metric] = value\n            return result\n    except Exception as e:\n        print(f\"Error loading {filepath}: {e}\")\n        return None\n\n\ndef load_all_results(datasets: Dict) -> Dict:\n    \"\"\"Load all cached results from the dataset directories.\"\"\"\n    results = {}\n    \n    for method_name, variants in datasets.items():\n        results[method_name] = {}\n        for variant_name, path in variants.items():\n            summary_file = find_summary_csv(path)\n            if summary_file:\n                data = load_summary_csv(summary_file)\n                if data:\n                    results[method_name][variant_name] = data\n                    print(f\"✓ Loaded: {method_name} - {variant_name}\")\n                else:\n                    print(f\"✗ Failed to parse: {method_name} - {variant_name}\")\n            else:\n                print(f\"✗ Not found: {method_name} - {variant_name} ({path})\")\n    \n    return results\n\n\ndef create_comparison_dataframe(results: Dict, evaluation_type: str = 'same') -> pd.DataFrame:\n    \"\"\"Create a comparison DataFrame for a specific evaluation type.\"\"\"\n    rows = []\n    \n    for method_name, variants in results.items():\n        # Handle special cases like methods with only 'generated'\n        if evaluation_type in variants:\n            data = variants[evaluation_type]\n        elif 'generated' in variants:\n            data = variants['generated']\n        else:\n            continue\n        \n        row = {'Method': method_name}\n        row.update(data)\n        rows.append(row)\n    \n    if not rows:\n        return pd.DataFrame()\n    \n    df = pd.DataFrame(rows)\n    df.set_index('Method', inplace=True)\n    return df\n\n\ndef calculate_overall_score(row: pd.Series, metrics: Dict = KEY_METRICS) -> float:\n    \"\"\"Calculate an overall weighted score for a method.\"\"\"\n    score = 0\n    total_weight = 0\n    \n    for metric, config in metrics.items():\n        if metric in row and pd.notna(row[metric]):\n            value = row[metric]\n            weight = config['weight']\n            direction = config['direction']\n            \n            if direction == 'higher':\n                if 'Score' in metric:\n                    normalized = value / 100\n                elif 'Correlation' in metric:\n                    normalized = value\n                else:\n                    normalized = min(value / 100, 1.0)\n            else:\n                if 'KS' in metric:\n                    normalized = max(0, 1 - value)\n                elif 'Wasserstein' in metric or 'Distance' in metric:\n                    normalized = max(0, 1 - value / 2)\n                else:\n                    normalized = max(0, 1 - value)\n            \n            score += normalized * weight\n            total_weight += weight\n    \n    return (score / total_weight * 100) if total_weight > 0 else 0\n\n\ndef create_ranking_table(df: pd.DataFrame, metrics: List[str] = None) -> pd.DataFrame:\n    \"\"\"Create a ranking table for methods across metrics.\"\"\"\n    if metrics is None:\n        metrics = list(KEY_METRICS.keys())\n    \n    available_metrics = [m for m in metrics if m in df.columns]\n    rankings = pd.DataFrame(index=df.index)\n    \n    for metric in available_metrics:\n        if metric in df.columns:\n            config = KEY_METRICS.get(metric, {'direction': 'higher'})\n            ascending = config['direction'] == 'lower'\n            rankings[metric] = df[metric].rank(ascending=ascending, na_option='bottom')\n    \n    rankings['Average Rank'] = rankings.mean(axis=1)\n    rankings = rankings.sort_values('Average Rank')\n    \n    return rankings\n\nprint(\"CSV helper functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions for PNG Image Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def find_png_file(directory: str, png_filename: str) -> Optional[str]:\n    \"\"\"Find a specific PNG file in a directory.\"\"\"\n    if not os.path.exists(directory):\n        return None\n    \n    filepath = os.path.join(directory, png_filename)\n    if os.path.exists(filepath):\n        return filepath\n    \n    # Try to find similar named files\n    base_name = png_filename.replace('.png', '').replace('enhanced_v3_', '')\n    for f in os.listdir(directory):\n        if f.endswith('.png') and base_name in f.lower():\n            return os.path.join(directory, f)\n    \n    return None\n\n\ndef collect_all_pngs(datasets: Dict, png_files: Dict) -> Dict:\n    \"\"\"Collect all PNG file paths organized by plot type and method.\"\"\"\n    collected = {plot_type: {} for plot_type in png_files.keys()}\n    \n    for method_name, variants in datasets.items():\n        for variant_name, path in variants.items():\n            # Create a unique key for this method+variant\n            if variant_name == 'generated':\n                key = method_name\n            else:\n                key = f\"{method_name}_{variant_name}\"\n            \n            for plot_type, png_filename in png_files.items():\n                png_path = find_png_file(path, png_filename)\n                if png_path:\n                    collected[plot_type][key] = png_path\n    \n    return collected\n\n\ndef load_image_safe(filepath: str) -> Optional[np.ndarray]:\n    \"\"\"Safely load an image file.\"\"\"\n    try:\n        img = Image.open(filepath)\n        return np.array(img)\n    except Exception as e:\n        print(f\"Error loading {filepath}: {e}\")\n        return None\n\n\ndef create_combined_plot_grid(images_dict: Dict[str, str], plot_title: str, \n                               output_path: str, ncols: int = 3,\n                               figsize_per_image: Tuple[int, int] = (6, 5)):\n    \"\"\"\n    Create a grid of images with method names as titles.\n    \"\"\"\n    if not images_dict:\n        print(f\"No images found for {plot_title}\")\n        return None\n    \n    n_images = len(images_dict)\n    nrows = (n_images + ncols - 1) // ncols\n    \n    fig_width = figsize_per_image[0] * ncols\n    fig_height = figsize_per_image[1] * nrows\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height))\n    fig.suptitle(plot_title, fontsize=16, fontweight='bold', y=1.02)\n    \n    # Flatten axes for easy iteration\n    if nrows == 1 and ncols == 1:\n        axes = [axes]\n    elif nrows == 1 or ncols == 1:\n        axes = axes.flatten()\n    else:\n        axes = axes.flatten()\n    \n    # Sort methods for consistent ordering\n    sorted_methods = sorted(images_dict.keys())\n    \n    for idx, method_name in enumerate(sorted_methods):\n        img_path = images_dict[method_name]\n        img = load_image_safe(img_path)\n        \n        if img is not None:\n            axes[idx].imshow(img)\n            # Create a cleaner title\n            display_name = method_name.replace('_', ' ').replace(' same', ' (same)').replace(' diff', ' (diff)')\n            axes[idx].set_title(display_name, fontsize=10, fontweight='bold')\n        else:\n            axes[idx].text(0.5, 0.5, f'Failed to load\\n{method_name}', \n                          ha='center', va='center', fontsize=10)\n            axes[idx].set_title(method_name, fontsize=10)\n        \n        axes[idx].axis('off')\n    \n    # Hide empty subplots\n    for idx in range(n_images, len(axes)):\n        axes[idx].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n    plt.show()\n    \n    print(f\"Saved: {output_path}\")\n    return fig\n\n\ndef create_side_by_side_comparison(images_same: Dict[str, str], images_diff: Dict[str, str],\n                                    plot_title: str, output_path: str):\n    \"\"\"\n    Create side-by-side comparison of same vs diff dataset evaluations.\n    \"\"\"\n    # Get common methods\n    methods_same = {k.replace('_same', ''): v for k, v in images_same.items()}\n    methods_diff = {k.replace('_diff', ''): v for k, v in images_diff.items()}\n    common_methods = sorted(set(methods_same.keys()) & set(methods_diff.keys()))\n    \n    if not common_methods:\n        print(f\"No common methods found for {plot_title}\")\n        return None\n    \n    n_methods = len(common_methods)\n    fig, axes = plt.subplots(n_methods, 2, figsize=(14, 5 * n_methods))\n    fig.suptitle(f\"{plot_title}\\nSame Dataset vs Different Dataset\", \n                 fontsize=16, fontweight='bold', y=1.01)\n    \n    if n_methods == 1:\n        axes = axes.reshape(1, 2)\n    \n    for idx, method in enumerate(common_methods):\n        # Same dataset image\n        img_same = load_image_safe(methods_same[method])\n        if img_same is not None:\n            axes[idx, 0].imshow(img_same)\n        axes[idx, 0].set_title(f\"{method.replace('_', ' ')} - Same\", fontsize=10, fontweight='bold')\n        axes[idx, 0].axis('off')\n        \n        # Diff dataset image\n        img_diff = load_image_safe(methods_diff[method])\n        if img_diff is not None:\n            axes[idx, 1].imshow(img_diff)\n        axes[idx, 1].set_title(f\"{method.replace('_', ' ')} - Diff\", fontsize=10, fontweight='bold')\n        axes[idx, 1].axis('off')\n    \n    plt.tight_layout()\n    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n    plt.show()\n    \n    print(f\"Saved: {output_path}\")\n    return fig\n\nprint(\"PNG helper functions defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Cached Results (CSV Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading results from enhanced_v3_summary.csv files...\")\n",
    "print(\"=\" * 60)\n",
    "results = load_all_results(SYNTHETIC_ANOMALY_DATASETS)\n",
    "print(\"\\nLoading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect All PNG Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collecting PNG files from all directories...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_pngs = collect_all_pngs(SYNTHETIC_ANOMALY_DATASETS, PNG_FILES)\n",
    "\n",
    "print(\"\\nPNG files found:\")\n",
    "for plot_type, images in all_pngs.items():\n",
    "    print(f\"  {plot_type}: {len(images)} images\")\n",
    "    for method, path in images.items():\n",
    "        print(f\"    - {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Comparison DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create comparison DataFrames\ndf_same = create_comparison_dataframe(results, 'same')\ndf_diff = create_comparison_dataframe(results, 'diff')\n\n# For methods which only have 'generated', include it in both\nfor method, variants in results.items():\n    if 'generated' in variants and method not in df_same.index:\n        row = variants['generated']\n        row_df = pd.DataFrame([row], index=[method])\n        df_same = pd.concat([df_same, row_df])\n        df_diff = pd.concat([df_diff, row_df])\n\nprint(f\"Same dataset (train): {len(df_same)} methods\")\nprint(f\"Different dataset (extended): {len(df_diff)} methods\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select key metrics to display\ndisplay_metrics = [\n    'Mean KS Statistic',\n    'Mean Wasserstein Distance',\n    'Correlation Similarity (Pearson)',\n    'Correlation Similarity (Spearman)',\n    'PCA Centroid Distance',\n    'Distribution Score (Weighted)',\n    'Correlation Score',\n    'Effect Size Score (Weighted)',\n    'KS Excellent Features',\n    'KS Good or Better Features',\n    'Negligible Effect Features'\n]\n\navailable_display = [m for m in display_metrics if m in df_same.columns]\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"SAME DATASET COMPARISON (Train)\")\nprint(\"(all_incidents_anomalies_reinforced_v2.csv)\")\nprint(\"=\" * 80)\nif not df_same.empty and available_display:\n    display(df_same[available_display].round(4).style.background_gradient(cmap='RdYlGn', axis=0))\nelse:\n    print(\"No data available for Same dataset comparison.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "available_display_diff = [m for m in display_metrics if m in df_diff.columns]\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"DIFFERENT DATASET COMPARISON (Extended)\")\nprint(\"(all_incidents_anomalies_extended_reinforced.csv)\")\nprint(\"=\" * 80)\nif not df_diff.empty and available_display_diff:\n    display(df_diff[available_display_diff].round(4).style.background_gradient(cmap='RdYlGn', axis=0))\nelse:\n    print(\"No data available for Different dataset comparison.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overall Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate overall scores\nscores_same = df_same.apply(calculate_overall_score, axis=1).sort_values(ascending=False) if not df_same.empty else pd.Series()\nscores_diff = df_diff.apply(calculate_overall_score, axis=1).sort_values(ascending=False) if not df_diff.empty else pd.Series()\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"OVERALL RANKINGS - Same Dataset (Train)\")\nprint(\"(all_incidents_anomalies_reinforced_v2.csv)\")\nprint(\"=\" * 60)\nif not scores_same.empty:\n    for rank, (method, score) in enumerate(scores_same.items(), 1):\n        print(f\"{rank:2d}. {method:35s}: {score:.2f}\")\nelse:\n    print(\"No data available.\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"OVERALL RANKINGS - Different Dataset (Extended)\")\nprint(\"(all_incidents_anomalies_extended_reinforced.csv)\")\nprint(\"=\" * 60)\nif not scores_diff.empty:\n    for rank, (method, score) in enumerate(scores_diff.items(), 1):\n        print(f\"{rank:2d}. {method:35s}: {score:.2f}\")\nelse:\n    print(\"No data available.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Combined PNG Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. t-SNE Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all t-SNE plots\n",
    "if 'tsne' in all_pngs and all_pngs['tsne']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"t-SNE OVERLAY PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['tsne'],\n",
    "        't-SNE Overlay Comparison - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_tsne_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No t-SNE plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Correlation Comparison Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all correlation comparison plots\n",
    "if 'correlation_comparison' in all_pngs and all_pngs['correlation_comparison']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION COMPARISON PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['correlation_comparison'],\n",
    "        'Correlation Comparison - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_correlation_comparison_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No correlation comparison plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Correlation Scatter Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all correlation scatter plots\n",
    "if 'correlation_scatter' in all_pngs and all_pngs['correlation_scatter']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION SCATTER PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['correlation_scatter'],\n",
    "        'Correlation Scatter - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_correlation_scatter_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No correlation scatter plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Distribution Comparison Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all distribution comparison plots\n",
    "if 'distribution_comparison' in all_pngs and all_pngs['distribution_comparison']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DISTRIBUTION COMPARISON PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['distribution_comparison'],\n",
    "        'Distribution Comparison - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_distribution_comparison_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No distribution comparison plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Distribution Tests Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all distribution test plots\n",
    "if 'distribution_tests' in all_pngs and all_pngs['distribution_tests']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DISTRIBUTION TESTS PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['distribution_tests'],\n",
    "        'Distribution Tests - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_distribution_tests_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No distribution test plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Effect Sizes Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all effect sizes plots\n",
    "if 'effect_sizes' in all_pngs and all_pngs['effect_sizes']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EFFECT SIZES PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['effect_sizes'],\n",
    "        'Effect Sizes - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_effect_sizes_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No effect sizes plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. PCA Centroid Analysis Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all PCA centroid plots\n",
    "if 'pca_centroid' in all_pngs and all_pngs['pca_centroid']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PCA CENTROID ANALYSIS PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['pca_centroid'],\n",
    "        'PCA Centroid Analysis - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_pca_centroid_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No PCA centroid plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Quality Dashboard Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all quality dashboard plots\n",
    "if 'quality_dashboard' in all_pngs and all_pngs['quality_dashboard']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"QUALITY DASHBOARD PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['quality_dashboard'],\n",
    "        'Quality Dashboard - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_quality_dashboard_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(8, 7)\n",
    "    )\n",
    "else:\n",
    "    print(\"No quality dashboard plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Top K Worst Features Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all top k worst features plots\n",
    "if 'top_k_worst' in all_pngs and all_pngs['top_k_worst']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP K WORST FEATURES PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['top_k_worst'],\n",
    "        'Top K Worst Features - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_top_k_worst_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No top k worst features plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Calibration Check Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all calibration check plots\n",
    "if 'calibration' in all_pngs and all_pngs['calibration']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CALIBRATION CHECK PLOTS - ALL ANOMALY METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['calibration'],\n",
    "        'Calibration Check - All Anomaly Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_calibration_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No calibration check plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Separated by Evaluation Type (vs Train vs vs Extended)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. t-SNE Plots - vs Train Dataset Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter for same only\ntsne_same = {k: v for k, v in all_pngs.get('tsne', {}).items() \n             if '_same' in k or 'generated' in k or ('_same' not in k and '_diff' not in k)}\n\nif tsne_same:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"t-SNE PLOTS - SAME DATASET (Train)\")\n    print(\"(all_incidents_anomalies_reinforced_v2.csv)\")\n    print(\"=\" * 80)\n    \n    create_combined_plot_grid(\n        tsne_same,\n        't-SNE Overlay - Same Dataset (Train)',\n        os.path.join(OUTPUT_DIR, 'combined_tsne_same.png'),\n        ncols=3,\n        figsize_per_image=(7, 6)\n    )\nelse:\n    print(\"No t-SNE plots for Same dataset.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. t-SNE Plots - vs Extended Dataset Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Filter for diff only\ntsne_diff = {k: v for k, v in all_pngs.get('tsne', {}).items() if '_diff' in k}\n\nif tsne_diff:\n    print(\"\\n\" + \"=\" * 80)\n    print(\"t-SNE PLOTS - DIFFERENT DATASET (Extended)\")\n    print(\"(all_incidents_anomalies_extended_reinforced.csv)\")\n    print(\"=\" * 80)\n    \n    create_combined_plot_grid(\n        tsne_diff,\n        't-SNE Overlay - Different Dataset (Extended)',\n        os.path.join(OUTPUT_DIR, 'combined_tsne_diff.png'),\n        ncols=3,\n        figsize_per_image=(7, 6)\n    )\nelse:\n    print(\"No t-SNE plots for Different dataset.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metric Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Overall Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Overall Score Comparison\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\n\nif not scores_same.empty:\n    # Same dataset\n    colors_same = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores_same)))\n    axes[0].barh(range(len(scores_same)), scores_same, color=colors_same)\n    axes[0].set_yticks(range(len(scores_same)))\n    axes[0].set_yticklabels(scores_same.index)\n    axes[0].set_xlabel('Overall Score')\n    axes[0].set_xlim(0, 100)\n    axes[0].set_title('Same Dataset (Train) - Overall Score\\n(all_incidents_anomalies_reinforced_v2.csv)', fontsize=12)\n    for i, (idx, v) in enumerate(scores_same.items()):\n        axes[0].text(v, i, f' {v:.1f}', va='center', fontsize=9)\nelse:\n    axes[0].text(0.5, 0.5, 'No data available', ha='center', va='center', fontsize=14)\n    axes[0].set_title('Same Dataset - Overall Score')\n\nif not scores_diff.empty:\n    # Different dataset\n    colors_diff = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores_diff)))\n    axes[1].barh(range(len(scores_diff)), scores_diff, color=colors_diff)\n    axes[1].set_yticks(range(len(scores_diff)))\n    axes[1].set_yticklabels(scores_diff.index)\n    axes[1].set_xlabel('Overall Score')\n    axes[1].set_xlim(0, 100)\n    axes[1].set_title('Different Dataset (Extended) - Overall Score\\n(all_incidents_anomalies_extended_reinforced.csv)', fontsize=12)\n    for i, (idx, v) in enumerate(scores_diff.items()):\n        axes[1].text(v, i, f' {v:.1f}', va='center', fontsize=9)\nelse:\n    axes[1].text(0.5, 0.5, 'No data available', ha='center', va='center', fontsize=14)\n    axes[1].set_title('Different Dataset - Overall Score')\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'overall_comparison_anomaly.png'), dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Performance Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap - Same Dataset\nkey_metric_names = list(KEY_METRICS.keys())\navailable_heatmap = [m for m in key_metric_names if m in df_same.columns]\n\nif len(available_heatmap) >= 2 and not df_same.empty:\n    plot_data = df_same[available_heatmap].copy()\n    \n    # Normalize each column\n    for col in plot_data.columns:\n        config = KEY_METRICS.get(col, {'direction': 'higher'})\n        values = plot_data[col]\n        min_val, max_val = values.min(), values.max()\n        if max_val > min_val:\n            normalized = (values - min_val) / (max_val - min_val)\n            if config['direction'] == 'lower':\n                normalized = 1 - normalized\n            plot_data[col] = normalized\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    sns.heatmap(plot_data, annot=True, fmt='.2f', cmap='RdYlGn',\n                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score (higher=better)'})\n    ax.set_title('Method Performance Heatmap (Same Dataset - Train)\\n(Normalized: 1=best, 0=worst)', fontsize=14)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'heatmap_same.png'), dpi=150)\n    plt.show()\nelse:\n    print(\"Not enough data for heatmap (Same dataset).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap - Different Dataset\navailable_heatmap_diff = [m for m in key_metric_names if m in df_diff.columns]\n\nif len(available_heatmap_diff) >= 2 and not df_diff.empty:\n    plot_data = df_diff[available_heatmap_diff].copy()\n    \n    for col in plot_data.columns:\n        config = KEY_METRICS.get(col, {'direction': 'higher'})\n        values = plot_data[col]\n        min_val, max_val = values.min(), values.max()\n        if max_val > min_val:\n            normalized = (values - min_val) / (max_val - min_val)\n            if config['direction'] == 'lower':\n                normalized = 1 - normalized\n            plot_data[col] = normalized\n    \n    fig, ax = plt.subplots(figsize=(14, 8))\n    sns.heatmap(plot_data, annot=True, fmt='.2f', cmap='RdYlGn',\n                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score (higher=better)'})\n    ax.set_title('Method Performance Heatmap (Different Dataset - Extended)\\n(Normalized: 1=best, 0=worst)', fontsize=14)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'heatmap_diff.png'), dpi=150)\n    plt.show()\nelse:\n    print(\"Not enough data for heatmap (Different dataset).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Key Metric Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Key Metric Bar Charts\nkey_metrics_to_plot = [\n    ('Mean KS Statistic', 'lower'),\n    ('Correlation Similarity (Pearson)', 'higher'),\n    ('Correlation Score', 'higher'),\n    ('Distribution Score (Weighted)', 'higher')\n]\n\navailable_to_plot = [(m, d) for m, d in key_metrics_to_plot if m in df_same.columns]\n\nif available_to_plot and not df_same.empty:\n    n_plots = len(available_to_plot)\n    ncols = min(2, n_plots)\n    nrows = (n_plots + ncols - 1) // ncols\n    \n    fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 6 * nrows))\n    if n_plots == 1:\n        axes = [axes]\n    else:\n        axes = axes.flatten()\n\n    for idx, (metric, direction) in enumerate(available_to_plot):\n        values = df_same[metric].dropna().sort_values(ascending=(direction == 'lower'))\n        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(values)))\n        if direction == 'lower':\n            colors = colors[::-1]\n        \n        axes[idx].barh(range(len(values)), values, color=colors)\n        axes[idx].set_yticks(range(len(values)))\n        axes[idx].set_yticklabels(values.index)\n        axes[idx].set_xlabel(metric)\n        axes[idx].set_title(f'{metric} (Same Dataset - Train)')\n        \n        for i, (method, v) in enumerate(values.items()):\n            axes[idx].text(v, i, f' {v:.4f}', va='center', fontsize=8)\n    \n    # Hide unused axes\n    for idx in range(len(available_to_plot), len(axes)):\n        axes[idx].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'key_metrics_comparison_anomaly.png'), dpi=150)\n    plt.show()\nelse:\n    print(\"No key metrics available to plot.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Ranking Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create and display ranking tables\nrankings_same = pd.DataFrame()\nrankings_diff = pd.DataFrame()\n\nif not df_same.empty:\n    rankings_same = create_ranking_table(df_same)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"RANKING TABLE - Same Dataset (Train)\")\n    print(\"(Lower rank = better performance)\")\n    print(\"=\" * 80)\n    display(rankings_same.round(2).style.background_gradient(cmap='RdYlGn_r', axis=0))\nelse:\n    print(\"No data for ranking table (Same dataset).\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if not df_diff.empty:\n    rankings_diff = create_ranking_table(df_diff)\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"RANKING TABLE - Different Dataset (Extended)\")\n    print(\"(Lower rank = better performance)\")\n    print(\"=\" * 80)\n    display(rankings_diff.round(2).style.background_gradient(cmap='RdYlGn_r', axis=0))\nelse:\n    print(\"No data for ranking table (Different dataset).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Best Methods by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "categories = {\n    'Distribution Similarity': ['Mean KS Statistic', 'Mean Wasserstein Distance'],\n    'Correlation Preservation': ['Correlation Similarity (Pearson)', 'Correlation Similarity (Spearman)'],\n    'Overall Scores': ['Distribution Score (Weighted)', 'Correlation Score', 'Effect Size Score (Weighted)']\n}\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"BEST METHODS BY METRIC CATEGORY - ANOMALY DATA\")\nprint(\"=\" * 80)\n\nfor eval_type, df in [('Same Dataset (Train)', df_same), ('Different Dataset (Extended)', df_diff)]:\n    if df.empty:\n        continue\n    print(f\"\\n{eval_type}:\")\n    print(\"-\" * 40)\n    \n    for category, metrics in categories.items():\n        print(f\"\\n  {category}:\")\n        for metric in metrics:\n            if metric in df.columns:\n                config = KEY_METRICS.get(metric, {'direction': 'higher'})\n                if config['direction'] == 'higher':\n                    best = df[metric].idxmax()\n                    value = df[metric].max()\n                else:\n                    best = df[metric].idxmin()\n                    value = df[metric].min()\n                print(f\"    {metric}: {best} ({value:.4f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save all results\nif not df_same.empty:\n    df_same.to_csv(os.path.join(OUTPUT_DIR, 'comparison_same.csv'))\nif not df_diff.empty:\n    df_diff.to_csv(os.path.join(OUTPUT_DIR, 'comparison_diff.csv'))\nif not rankings_same.empty:\n    rankings_same.to_csv(os.path.join(OUTPUT_DIR, 'rankings_same.csv'))\nif not rankings_diff.empty:\n    rankings_diff.to_csv(os.path.join(OUTPUT_DIR, 'rankings_diff.csv'))\n\n# Save overall scores\nif not scores_same.empty:\n    pd.DataFrame({'Method': scores_same.index, 'Overall Score': scores_same.values}).to_csv(\n        os.path.join(OUTPUT_DIR, 'overall_scores_same.csv'), index=False)\nif not scores_diff.empty:\n    pd.DataFrame({'Method': scores_diff.index, 'Overall Score': scores_diff.values}).to_csv(\n        os.path.join(OUTPUT_DIR, 'overall_scores_diff.csv'), index=False)\n\nprint(f\"\\nAll results saved to: {OUTPUT_DIR}\")\nprint(\"\\nSaved files:\")\nif os.path.exists(OUTPUT_DIR):\n    for f in sorted(os.listdir(OUTPUT_DIR)):\n        print(f\"  - {f}\")\nelse:\n    print(\"  (Output directory not yet created - run cells to generate results)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\" * 80)\nprint(\"SUMMARY - ANOMALY DATA COMPARISON\")\nprint(\"=\" * 80)\n\nprint(f\"\\nReference Datasets:\")\nprint(f\"  Same (Train): {REFERENCE_DATASETS.get('same', 'Not set')}\")\nprint(f\"  Diff (Extended): {REFERENCE_DATASETS.get('diff', 'Not set')}\")\n\nprint(f\"\\nTotal methods configured: {len(SYNTHETIC_ANOMALY_DATASETS)}\")\nprint(f\"Methods with Same dataset results: {len(df_same)}\")\nprint(f\"Methods with Different dataset results: {len(df_diff)}\")\n\nprint(\"\\nPNG plots collected:\")\nfor plot_type, images in all_pngs.items():\n    print(f\"  {plot_type}: {len(images)} images\")\n\nif not scores_same.empty:\n    print(\"\\nTop 3 Methods Overall:\")\n    print(\"\\n  Same Dataset (Train):\")\n    for rank, (method, score) in enumerate(scores_same.head(3).items(), 1):\n        print(f\"    {rank}. {method}: {score:.2f}\")\n\nif not scores_diff.empty:\n    print(\"\\n  Different Dataset (Extended):\")\n    for rank, (method, score) in enumerate(scores_diff.head(3).items(), 1):\n        print(f\"    {rank}. {method}: {score:.2f}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Analysis complete!\")\nprint(\"=\" * 80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}