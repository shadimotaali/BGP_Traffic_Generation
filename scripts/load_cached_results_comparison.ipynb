{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cached Results Comparison\n",
    "\n",
    "This notebook loads pre-computed evaluation results from `enhanced_v3_summary.csv` files\n",
    "and combines all visualization plots (t-SNE, correlation, distribution, etc.) into unified comparison grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Analysis timestamp: {TIMESTAMP}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Define paths to all synthetic dataset results directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these paths to match your data\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR = f'./cached_results_comparison_{TIMESTAMP}'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SYNTHETIC_DATASETS = {\n",
    "    # SCAPY (direct generation - only one version)\n",
    "    'SCAPY': {\n",
    "        'generated': '/home/smotaali/BGP_Traffic_Generation/results_zend/Scapy_enhanced_1215_v3'\n",
    "    },\n",
    "\n",
    "    # GAN Default Values\n",
    "    'GAN_LSTM_default': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_LSTM_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_LSTM_rrc04'\n",
    "    },\n",
    "    'GAN_TimeGAN_default': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_TIME_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_TIME_rrc04'\n",
    "    },\n",
    "    'GAN_DoppelGanger_default': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_Doppelganger_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_Doppelganger_rrc04'\n",
    "    },\n",
    "\n",
    "    # GAN Enhanced/Tuned Parameters\n",
    "    'GAN_LSTM_enhanced': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_LSTM_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_LSTM_rrc04'\n",
    "    },\n",
    "    'GAN_TimeGAN_enhanced': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_TIME_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_TIME_rrc04'\n",
    "    },\n",
    "    'GAN_DoppelGanger_enhanced': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_Doppelganger_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_Doppelganger_rrc04'\n",
    "    },\n",
    "\n",
    "    # SMOTE Variants\n",
    "    'SMOTE_normal': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/normal_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/normal_rrc04'\n",
    "    },\n",
    "    'SMOTE_borderline': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/borderline_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/borderline_rrc04'\n",
    "    },\n",
    "    'SMOTE_kmeans': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/kmeans_v3_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/kmeans_v3_rrc04'\n",
    "    },\n",
    "    'SMOTE_adasyn': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/adasyn_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/adasyn_rrc04'\n",
    "    },\n",
    "\n",
    "    # Hybrid (SMOTE + GAN)\n",
    "    'Hybrid_SMOTE_GAN': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/synthetic_hybrid/compare_hybrid_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/synthetic_hybrid/compare_hybrid_rrc04'\n",
    "    },\n",
    "\n",
    "    # Copula\n",
    "    'Copula': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/copula_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/copula_rrc04'\n",
    "    }\n",
    "}\n",
    "\n",
    "# PNG files to collect and combine\n",
    "PNG_FILES = {\n",
    "    'tsne': 'enhanced_v3_tsne_overlay.png',\n",
    "    'correlation_comparison': 'enhanced_v3_correlation_comparison.png',\n",
    "    'correlation_scatter': 'enhanced_v3_correlation_scatter.png',\n",
    "    'distribution_comparison': 'enhanced_v3_distribution_comparison.png',\n",
    "    'distribution_tests': 'enhanced_v3_distribution_tests.png',\n",
    "    'effect_sizes': 'enhanced_v3_effect_sizes.png',\n",
    "    'pca_centroid': 'enhanced_v3_pca_centroid_analysis.png',\n",
    "    'quality_dashboard': 'enhanced_v3_quality_dashboard.png',\n",
    "    'top_k_worst': 'enhanced_v3_top_k_worst_features.png',\n",
    "    'calibration': 'calibration_check_visualization.png'\n",
    "}\n",
    "\n",
    "# Key metrics configuration\n",
    "KEY_METRICS = {\n",
    "    'Mean KS Statistic': {'direction': 'lower', 'weight': 1.5},\n",
    "    'Mean Wasserstein Distance': {'direction': 'lower', 'weight': 1.5},\n",
    "    'Weighted Wasserstein Distance': {'direction': 'lower', 'weight': 1.5},\n",
    "    'PCA Centroid Distance': {'direction': 'lower', 'weight': 1.0},\n",
    "    'Correlation Similarity (Pearson)': {'direction': 'higher', 'weight': 2.0},\n",
    "    'Correlation Similarity (Spearman)': {'direction': 'higher', 'weight': 2.0},\n",
    "    'Distribution Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Distribution Score (Unweighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Correlation Score': {'direction': 'higher', 'weight': 1.5},\n",
    "    'Effect Size Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Effect Size Score (Unweighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Wasserstein Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'KS Excellent Features': {'direction': 'higher', 'weight': 1.0},\n",
    "    'KS Good or Better Features': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Negligible Effect Features': {'direction': 'higher', 'weight': 1.0},\n",
    "}\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Number of methods configured: {len(SYNTHETIC_DATASETS)}\")\n",
    "print(f\"Number of plot types to combine: {len(PNG_FILES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for CSV Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_csv(directory: str) -> Optional[str]:\n",
    "    \"\"\"Find the enhanced_v3_summary.csv file in a directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return None\n",
    "    summary_file = os.path.join(directory, 'enhanced_v3_summary.csv')\n",
    "    if os.path.exists(summary_file):\n",
    "        return summary_file\n",
    "    \n",
    "    # Try to find any summary.csv file\n",
    "    for f in os.listdir(directory):\n",
    "        if 'summary' in f.lower() and f.endswith('.csv'):\n",
    "            return os.path.join(directory, f)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_summary_csv(filepath: str) -> Optional[Dict]:\n",
    "    \"\"\"Load a summary CSV and convert to dictionary.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        if 'Metric' in df.columns and 'Value' in df.columns:\n",
    "            result = {}\n",
    "            for _, row in df.iterrows():\n",
    "                metric = row['Metric']\n",
    "                value = row['Value']\n",
    "                # Handle strings like \"100.0/100\"\n",
    "                if isinstance(value, str) and '/' in value:\n",
    "                    try:\n",
    "                        value = float(value.split('/')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                result[metric] = value\n",
    "            return result\n",
    "        else:\n",
    "            # Try first two columns\n",
    "            result = {}\n",
    "            cols = df.columns.tolist()\n",
    "            for _, row in df.iterrows():\n",
    "                metric = row[cols[0]]\n",
    "                value = row[cols[1]]\n",
    "                if isinstance(value, str) and '/' in value:\n",
    "                    try:\n",
    "                        value = float(value.split('/')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                result[metric] = value\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_results(datasets: Dict) -> Dict:\n",
    "    \"\"\"Load all cached results from the dataset directories.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method_name, variants in datasets.items():\n",
    "        results[method_name] = {}\n",
    "        for variant_name, path in variants.items():\n",
    "            summary_file = find_summary_csv(path)\n",
    "            if summary_file:\n",
    "                data = load_summary_csv(summary_file)\n",
    "                if data:\n",
    "                    results[method_name][variant_name] = data\n",
    "                    print(f\"✓ Loaded: {method_name} - {variant_name}\")\n",
    "                else:\n",
    "                    print(f\"✗ Failed to parse: {method_name} - {variant_name}\")\n",
    "            else:\n",
    "                print(f\"✗ Not found: {method_name} - {variant_name}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_comparison_dataframe(results: Dict, evaluation_type: str = 'same_rrc05') -> pd.DataFrame:\n",
    "    \"\"\"Create a comparison DataFrame for a specific evaluation type.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for method_name, variants in results.items():\n",
    "        # Handle special cases like SCAPY which only has 'generated'\n",
    "        if evaluation_type in variants:\n",
    "            data = variants[evaluation_type]\n",
    "        elif 'generated' in variants:\n",
    "            data = variants['generated']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        row = {'Method': method_name}\n",
    "        row.update(data)\n",
    "        rows.append(row)\n",
    "    \n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.set_index('Method', inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_overall_score(row: pd.Series, metrics: Dict = KEY_METRICS) -> float:\n",
    "    \"\"\"Calculate an overall weighted score for a method.\"\"\"\n",
    "    score = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for metric, config in metrics.items():\n",
    "        if metric in row and pd.notna(row[metric]):\n",
    "            value = row[metric]\n",
    "            weight = config['weight']\n",
    "            direction = config['direction']\n",
    "            \n",
    "            if direction == 'higher':\n",
    "                if 'Score' in metric:\n",
    "                    normalized = value / 100\n",
    "                elif 'Correlation' in metric:\n",
    "                    normalized = value\n",
    "                else:\n",
    "                    normalized = min(value / 100, 1.0)\n",
    "            else:\n",
    "                if 'KS' in metric:\n",
    "                    normalized = max(0, 1 - value)\n",
    "                elif 'Wasserstein' in metric or 'Distance' in metric:\n",
    "                    normalized = max(0, 1 - value / 2)\n",
    "                else:\n",
    "                    normalized = max(0, 1 - value)\n",
    "            \n",
    "            score += normalized * weight\n",
    "            total_weight += weight\n",
    "    \n",
    "    return (score / total_weight * 100) if total_weight > 0 else 0\n",
    "\n",
    "\n",
    "def create_ranking_table(df: pd.DataFrame, metrics: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Create a ranking table for methods across metrics.\"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = list(KEY_METRICS.keys())\n",
    "    \n",
    "    available_metrics = [m for m in metrics if m in df.columns]\n",
    "    rankings = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        if metric in df.columns:\n",
    "            config = KEY_METRICS.get(metric, {'direction': 'higher'})\n",
    "            ascending = config['direction'] == 'lower'\n",
    "            rankings[metric] = df[metric].rank(ascending=ascending, na_option='bottom')\n",
    "    \n",
    "    rankings['Average Rank'] = rankings.mean(axis=1)\n",
    "    rankings = rankings.sort_values('Average Rank')\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "print(\"CSV helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions for PNG Image Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_png_file(directory: str, png_filename: str) -> Optional[str]:\n",
    "    \"\"\"Find a specific PNG file in a directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        return None\n",
    "    \n",
    "    filepath = os.path.join(directory, png_filename)\n",
    "    if os.path.exists(filepath):\n",
    "        return filepath\n",
    "    \n",
    "    # Try to find similar named files\n",
    "    base_name = png_filename.replace('.png', '').replace('enhanced_v3_', '')\n",
    "    for f in os.listdir(directory):\n",
    "        if f.endswith('.png') and base_name in f.lower():\n",
    "            return os.path.join(directory, f)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def collect_all_pngs(datasets: Dict, png_files: Dict) -> Dict:\n",
    "    \"\"\"Collect all PNG file paths organized by plot type and method.\"\"\"\n",
    "    collected = {plot_type: {} for plot_type in png_files.keys()}\n",
    "    \n",
    "    for method_name, variants in datasets.items():\n",
    "        for variant_name, path in variants.items():\n",
    "            # Create a unique key for this method+variant\n",
    "            if variant_name == 'generated':\n",
    "                key = method_name\n",
    "            else:\n",
    "                key = f\"{method_name}_{variant_name}\"\n",
    "            \n",
    "            for plot_type, png_filename in png_files.items():\n",
    "                png_path = find_png_file(path, png_filename)\n",
    "                if png_path:\n",
    "                    collected[plot_type][key] = png_path\n",
    "    \n",
    "    return collected\n",
    "\n",
    "\n",
    "def load_image_safe(filepath: str) -> Optional[np.ndarray]:\n",
    "    \"\"\"Safely load an image file.\"\"\"\n",
    "    try:\n",
    "        img = Image.open(filepath)\n",
    "        return np.array(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def create_combined_plot_grid(images_dict: Dict[str, str], plot_title: str, \n",
    "                               output_path: str, ncols: int = 3,\n",
    "                               figsize_per_image: Tuple[int, int] = (6, 5)):\n",
    "    \"\"\"\n",
    "    Create a grid of images with method names as titles.\n",
    "    \n",
    "    Args:\n",
    "        images_dict: Dictionary mapping method names to image file paths\n",
    "        plot_title: Overall title for the combined plot\n",
    "        output_path: Path to save the combined figure\n",
    "        ncols: Number of columns in the grid\n",
    "        figsize_per_image: Size for each subplot\n",
    "    \"\"\"\n",
    "    if not images_dict:\n",
    "        print(f\"No images found for {plot_title}\")\n",
    "        return None\n",
    "    \n",
    "    n_images = len(images_dict)\n",
    "    nrows = (n_images + ncols - 1) // ncols\n",
    "    \n",
    "    fig_width = figsize_per_image[0] * ncols\n",
    "    fig_height = figsize_per_image[1] * nrows\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(fig_width, fig_height))\n",
    "    fig.suptitle(plot_title, fontsize=16, fontweight='bold', y=1.02)\n",
    "    \n",
    "    # Flatten axes for easy iteration\n",
    "    if nrows == 1 and ncols == 1:\n",
    "        axes = [axes]\n",
    "    elif nrows == 1 or ncols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Sort methods for consistent ordering\n",
    "    sorted_methods = sorted(images_dict.keys())\n",
    "    \n",
    "    for idx, method_name in enumerate(sorted_methods):\n",
    "        img_path = images_dict[method_name]\n",
    "        img = load_image_safe(img_path)\n",
    "        \n",
    "        if img is not None:\n",
    "            axes[idx].imshow(img)\n",
    "            # Create a cleaner title\n",
    "            display_name = method_name.replace('_', ' ').replace('same rrc05', '(rrc05)').replace('diff rrc04', '(rrc04)')\n",
    "            axes[idx].set_title(display_name, fontsize=10, fontweight='bold')\n",
    "        else:\n",
    "            axes[idx].text(0.5, 0.5, f'Failed to load\\n{method_name}', \n",
    "                          ha='center', va='center', fontsize=10)\n",
    "            axes[idx].set_title(method_name, fontsize=10)\n",
    "        \n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_images, len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: {output_path}\")\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_side_by_side_comparison(images_same: Dict[str, str], images_diff: Dict[str, str],\n",
    "                                    plot_title: str, output_path: str):\n",
    "    \"\"\"\n",
    "    Create side-by-side comparison of same vs different dataset evaluations.\n",
    "    \"\"\"\n",
    "    # Get common methods\n",
    "    methods_same = {k.replace('_same_rrc05', ''): v for k, v in images_same.items()}\n",
    "    methods_diff = {k.replace('_diff_rrc04', ''): v for k, v in images_diff.items()}\n",
    "    common_methods = sorted(set(methods_same.keys()) & set(methods_diff.keys()))\n",
    "    \n",
    "    if not common_methods:\n",
    "        print(f\"No common methods found for {plot_title}\")\n",
    "        return None\n",
    "    \n",
    "    n_methods = len(common_methods)\n",
    "    fig, axes = plt.subplots(n_methods, 2, figsize=(14, 5 * n_methods))\n",
    "    fig.suptitle(f\"{plot_title}\\nSame Dataset (rrc05) vs Different Dataset (rrc04)\", \n",
    "                 fontsize=16, fontweight='bold', y=1.01)\n",
    "    \n",
    "    if n_methods == 1:\n",
    "        axes = axes.reshape(1, 2)\n",
    "    \n",
    "    for idx, method in enumerate(common_methods):\n",
    "        # Same dataset image\n",
    "        img_same = load_image_safe(methods_same[method])\n",
    "        if img_same is not None:\n",
    "            axes[idx, 0].imshow(img_same)\n",
    "        axes[idx, 0].set_title(f\"{method.replace('_', ' ')} - Same (rrc05)\", fontsize=10, fontweight='bold')\n",
    "        axes[idx, 0].axis('off')\n",
    "        \n",
    "        # Different dataset image\n",
    "        img_diff = load_image_safe(methods_diff[method])\n",
    "        if img_diff is not None:\n",
    "            axes[idx, 1].imshow(img_diff)\n",
    "        axes[idx, 1].set_title(f\"{method.replace('_', ' ')} - Diff (rrc04)\", fontsize=10, fontweight='bold')\n",
    "        axes[idx, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Saved: {output_path}\")\n",
    "    return fig\n",
    "\n",
    "print(\"PNG helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load All Cached Results (CSV Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading results from enhanced_v3_summary.csv files...\")\n",
    "print(\"=\" * 60)\n",
    "results = load_all_results(SYNTHETIC_DATASETS)\n",
    "print(\"\\nLoading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Collect All PNG Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Collecting PNG files from all directories...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_pngs = collect_all_pngs(SYNTHETIC_DATASETS, PNG_FILES)\n",
    "\n",
    "print(\"\\nPNG files found:\")\n",
    "for plot_type, images in all_pngs.items():\n",
    "    print(f\"  {plot_type}: {len(images)} images\")\n",
    "    for method, path in images.items():\n",
    "        print(f\"    - {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Comparison DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrames\n",
    "df_same = create_comparison_dataframe(results, 'same_rrc05')\n",
    "df_diff = create_comparison_dataframe(results, 'diff_rrc04')\n",
    "\n",
    "# For SCAPY which only has 'generated', include it in both\n",
    "for method, variants in results.items():\n",
    "    if 'generated' in variants and method not in df_same.index:\n",
    "        row = variants['generated']\n",
    "        row_df = pd.DataFrame([row], index=[method])\n",
    "        df_same = pd.concat([df_same, row_df])\n",
    "        df_diff = pd.concat([df_diff, row_df])\n",
    "\n",
    "print(f\"Same dataset (rrc05): {len(df_same)} methods\")\n",
    "print(f\"Different dataset (rrc04): {len(df_diff)} methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key metrics to display\n",
    "display_metrics = [\n",
    "    'Mean KS Statistic',\n",
    "    'Mean Wasserstein Distance',\n",
    "    'Correlation Similarity (Pearson)',\n",
    "    'Correlation Similarity (Spearman)',\n",
    "    'PCA Centroid Distance',\n",
    "    'Distribution Score (Weighted)',\n",
    "    'Correlation Score',\n",
    "    'Effect Size Score (Weighted)',\n",
    "    'KS Excellent Features',\n",
    "    'KS Good or Better Features',\n",
    "    'Negligible Effect Features'\n",
    "]\n",
    "\n",
    "available_display = [m for m in display_metrics if m in df_same.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAME DATASET (rrc05) COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "if not df_same.empty and available_display:\n",
    "    display(df_same[available_display].round(4).style.background_gradient(cmap='RdYlGn', axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_display_diff = [m for m in display_metrics if m in df_diff.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIFFERENT DATASET (rrc04) COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "if not df_diff.empty and available_display_diff:\n",
    "    display(df_diff[available_display_diff].round(4).style.background_gradient(cmap='RdYlGn', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Overall Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall scores\n",
    "scores_same = df_same.apply(calculate_overall_score, axis=1).sort_values(ascending=False)\n",
    "scores_diff = df_diff.apply(calculate_overall_score, axis=1).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL RANKINGS - Same Dataset (rrc05)\")\n",
    "print(\"=\" * 60)\n",
    "for rank, (method, score) in enumerate(scores_same.items(), 1):\n",
    "    print(f\"{rank:2d}. {method:30s}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL RANKINGS - Different Dataset (rrc04)\")\n",
    "print(\"=\" * 60)\n",
    "for rank, (method, score) in enumerate(scores_diff.items(), 1):\n",
    "    print(f\"{rank:2d}. {method:30s}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Combined PNG Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. t-SNE Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all t-SNE plots\n",
    "if 'tsne' in all_pngs and all_pngs['tsne']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"t-SNE OVERLAY PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['tsne'],\n",
    "        't-SNE Overlay Comparison - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_tsne_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No t-SNE plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Correlation Comparison Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all correlation comparison plots\n",
    "if 'correlation_comparison' in all_pngs and all_pngs['correlation_comparison']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION COMPARISON PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['correlation_comparison'],\n",
    "        'Correlation Comparison - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_correlation_comparison_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No correlation comparison plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Correlation Scatter Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all correlation scatter plots\n",
    "if 'correlation_scatter' in all_pngs and all_pngs['correlation_scatter']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CORRELATION SCATTER PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['correlation_scatter'],\n",
    "        'Correlation Scatter - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_correlation_scatter_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No correlation scatter plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Distribution Comparison Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all distribution comparison plots\n",
    "if 'distribution_comparison' in all_pngs and all_pngs['distribution_comparison']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DISTRIBUTION COMPARISON PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['distribution_comparison'],\n",
    "        'Distribution Comparison - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_distribution_comparison_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No distribution comparison plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Distribution Tests Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all distribution test plots\n",
    "if 'distribution_tests' in all_pngs and all_pngs['distribution_tests']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DISTRIBUTION TESTS PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['distribution_tests'],\n",
    "        'Distribution Tests - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_distribution_tests_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No distribution test plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Effect Sizes Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all effect sizes plots\n",
    "if 'effect_sizes' in all_pngs and all_pngs['effect_sizes']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EFFECT SIZES PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['effect_sizes'],\n",
    "        'Effect Sizes - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_effect_sizes_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No effect sizes plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. PCA Centroid Analysis Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all PCA centroid plots\n",
    "if 'pca_centroid' in all_pngs and all_pngs['pca_centroid']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PCA CENTROID ANALYSIS PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['pca_centroid'],\n",
    "        'PCA Centroid Analysis - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_pca_centroid_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No PCA centroid plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Quality Dashboard Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all quality dashboard plots\n",
    "if 'quality_dashboard' in all_pngs and all_pngs['quality_dashboard']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"QUALITY DASHBOARD PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['quality_dashboard'],\n",
    "        'Quality Dashboard - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_quality_dashboard_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(8, 7)\n",
    "    )\n",
    "else:\n",
    "    print(\"No quality dashboard plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Top K Worst Features Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all top k worst features plots\n",
    "if 'top_k_worst' in all_pngs and all_pngs['top_k_worst']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TOP K WORST FEATURES PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['top_k_worst'],\n",
    "        'Top K Worst Features - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_top_k_worst_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No top k worst features plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Calibration Check Plots Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all calibration check plots\n",
    "if 'calibration' in all_pngs and all_pngs['calibration']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CALIBRATION CHECK PLOTS - ALL METHODS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        all_pngs['calibration'],\n",
    "        'Calibration Check - All Methods',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_calibration_all.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No calibration check plots found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Separated by Evaluation Type (rrc05 vs rrc04)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. t-SNE Plots - Same Dataset (rrc05) Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for same_rrc05 only\n",
    "tsne_same = {k: v for k, v in all_pngs.get('tsne', {}).items() \n",
    "             if 'same_rrc05' in k or 'generated' in k or '_rrc05' not in k and '_rrc04' not in k}\n",
    "\n",
    "if tsne_same:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"t-SNE PLOTS - SAME DATASET (rrc05)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        tsne_same,\n",
    "        't-SNE Overlay - Same Dataset (rrc05)',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_tsne_same_rrc05.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No t-SNE plots for same dataset (rrc05).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. t-SNE Plots - Different Dataset (rrc04) Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for diff_rrc04 only\n",
    "tsne_diff = {k: v for k, v in all_pngs.get('tsne', {}).items() if 'diff_rrc04' in k}\n",
    "\n",
    "if tsne_diff:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"t-SNE PLOTS - DIFFERENT DATASET (rrc04)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    create_combined_plot_grid(\n",
    "        tsne_diff,\n",
    "        't-SNE Overlay - Different Dataset (rrc04)',\n",
    "        os.path.join(OUTPUT_DIR, 'combined_tsne_diff_rrc04.png'),\n",
    "        ncols=3,\n",
    "        figsize_per_image=(7, 6)\n",
    "    )\n",
    "else:\n",
    "    print(\"No t-SNE plots for different dataset (rrc04).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Metric Visualizations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Overall Score Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Score Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "if not scores_same.empty:\n",
    "    # Same dataset\n",
    "    colors_same = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores_same)))\n",
    "    axes[0].barh(range(len(scores_same)), scores_same, color=colors_same)\n",
    "    axes[0].set_yticks(range(len(scores_same)))\n",
    "    axes[0].set_yticklabels(scores_same.index)\n",
    "    axes[0].set_xlabel('Overall Score')\n",
    "    axes[0].set_title('Same Dataset (rrc05) - Overall Score', fontsize=14)\n",
    "    for i, (idx, v) in enumerate(scores_same.items()):\n",
    "        axes[0].text(v, i, f' {v:.1f}', va='center', fontsize=9)\n",
    "\n",
    "if not scores_diff.empty:\n",
    "    # Different dataset\n",
    "    colors_diff = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores_diff)))\n",
    "    axes[1].barh(range(len(scores_diff)), scores_diff, color=colors_diff)\n",
    "    axes[1].set_yticks(range(len(scores_diff)))\n",
    "    axes[1].set_yticklabels(scores_diff.index)\n",
    "    axes[1].set_xlabel('Overall Score')\n",
    "    axes[1].set_title('Different Dataset (rrc04) - Overall Score', fontsize=14)\n",
    "    for i, (idx, v) in enumerate(scores_diff.items()):\n",
    "        axes[1].text(v, i, f' {v:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'overall_comparison.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Performance Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap - Same Dataset\n",
    "key_metric_names = list(KEY_METRICS.keys())\n",
    "available_heatmap = [m for m in key_metric_names if m in df_same.columns]\n",
    "\n",
    "if len(available_heatmap) >= 2 and not df_same.empty:\n",
    "    plot_data = df_same[available_heatmap].copy()\n",
    "    \n",
    "    # Normalize each column\n",
    "    for col in plot_data.columns:\n",
    "        config = KEY_METRICS.get(col, {'direction': 'higher'})\n",
    "        values = plot_data[col]\n",
    "        min_val, max_val = values.min(), values.max()\n",
    "        if max_val > min_val:\n",
    "            normalized = (values - min_val) / (max_val - min_val)\n",
    "            if config['direction'] == 'lower':\n",
    "                normalized = 1 - normalized\n",
    "            plot_data[col] = normalized\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.heatmap(plot_data, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score (higher=better)'})\n",
    "    ax.set_title('Method Performance Heatmap (Same Dataset - rrc05)\\n(Normalized: 1=best, 0=worst)', fontsize=14)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'heatmap_same_rrc05.png'), dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for heatmap (same dataset).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap - Different Dataset\n",
    "available_heatmap_diff = [m for m in key_metric_names if m in df_diff.columns]\n",
    "\n",
    "if len(available_heatmap_diff) >= 2 and not df_diff.empty:\n",
    "    plot_data = df_diff[available_heatmap_diff].copy()\n",
    "    \n",
    "    for col in plot_data.columns:\n",
    "        config = KEY_METRICS.get(col, {'direction': 'higher'})\n",
    "        values = plot_data[col]\n",
    "        min_val, max_val = values.min(), values.max()\n",
    "        if max_val > min_val:\n",
    "            normalized = (values - min_val) / (max_val - min_val)\n",
    "            if config['direction'] == 'lower':\n",
    "                normalized = 1 - normalized\n",
    "            plot_data[col] = normalized\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.heatmap(plot_data, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score (higher=better)'})\n",
    "    ax.set_title('Method Performance Heatmap (Different Dataset - rrc04)\\n(Normalized: 1=best, 0=worst)', fontsize=14)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'heatmap_diff_rrc04.png'), dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data for heatmap (different dataset).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 23. Key Metric Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Metric Bar Charts\n",
    "key_metrics_to_plot = [\n",
    "    ('Mean KS Statistic', 'lower'),\n",
    "    ('Correlation Similarity (Pearson)', 'higher'),\n",
    "    ('Correlation Score', 'higher'),\n",
    "    ('Distribution Score (Weighted)', 'higher')\n",
    "]\n",
    "\n",
    "available_to_plot = [(m, d) for m, d in key_metrics_to_plot if m in df_same.columns]\n",
    "\n",
    "if available_to_plot:\n",
    "    n_plots = len(available_to_plot)\n",
    "    ncols = min(2, n_plots)\n",
    "    nrows = (n_plots + ncols - 1) // ncols\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(8 * ncols, 6 * nrows))\n",
    "    if n_plots == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    for idx, (metric, direction) in enumerate(available_to_plot):\n",
    "        values = df_same[metric].dropna().sort_values(ascending=(direction == 'lower'))\n",
    "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(values)))\n",
    "        if direction == 'lower':\n",
    "            colors = colors[::-1]\n",
    "        \n",
    "        axes[idx].barh(range(len(values)), values, color=colors)\n",
    "        axes[idx].set_yticks(range(len(values)))\n",
    "        axes[idx].set_yticklabels(values.index)\n",
    "        axes[idx].set_xlabel(metric)\n",
    "        axes[idx].set_title(f'{metric} (Same Dataset - rrc05)')\n",
    "        \n",
    "        for i, (method, v) in enumerate(values.items()):\n",
    "            axes[idx].text(v, i, f' {v:.4f}', va='center', fontsize=8)\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for idx in range(len(available_to_plot), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'key_metrics_comparison.png'), dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No key metrics available to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Ranking Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display ranking tables\n",
    "if not df_same.empty:\n",
    "    rankings_same = create_ranking_table(df_same)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RANKING TABLE - Same Dataset (rrc05)\")\n",
    "    print(\"(Lower rank = better performance)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(rankings_same.round(2).style.background_gradient(cmap='RdYlGn_r', axis=0))\n",
    "else:\n",
    "    rankings_same = pd.DataFrame()\n",
    "    print(\"No data for ranking table (same dataset).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_diff.empty:\n",
    "    rankings_diff = create_ranking_table(df_diff)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RANKING TABLE - Different Dataset (rrc04)\")\n",
    "    print(\"(Lower rank = better performance)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(rankings_diff.round(2).style.background_gradient(cmap='RdYlGn_r', axis=0))\n",
    "else:\n",
    "    rankings_diff = pd.DataFrame()\n",
    "    print(\"No data for ranking table (different dataset).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 25. Best Methods by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'Distribution Similarity': ['Mean KS Statistic', 'Mean Wasserstein Distance'],\n",
    "    'Correlation Preservation': ['Correlation Similarity (Pearson)', 'Correlation Similarity (Spearman)'],\n",
    "    'Overall Scores': ['Distribution Score (Weighted)', 'Correlation Score', 'Effect Size Score (Weighted)']\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST METHODS BY METRIC CATEGORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for eval_type, df in [('Same Dataset (rrc05)', df_same), ('Different Dataset (rrc04)', df_diff)]:\n",
    "    if df.empty:\n",
    "        continue\n",
    "    print(f\"\\n{eval_type}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for category, metrics in categories.items():\n",
    "        print(f\"\\n  {category}:\")\n",
    "        for metric in metrics:\n",
    "            if metric in df.columns:\n",
    "                config = KEY_METRICS.get(metric, {'direction': 'higher'})\n",
    "                if config['direction'] == 'higher':\n",
    "                    best = df[metric].idxmax()\n",
    "                    value = df[metric].max()\n",
    "                else:\n",
    "                    best = df[metric].idxmin()\n",
    "                    value = df[metric].min()\n",
    "                print(f\"    {metric}: {best} ({value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 26. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "if not df_same.empty:\n",
    "    df_same.to_csv(os.path.join(OUTPUT_DIR, 'comparison_same_rrc05.csv'))\n",
    "if not df_diff.empty:\n",
    "    df_diff.to_csv(os.path.join(OUTPUT_DIR, 'comparison_diff_rrc04.csv'))\n",
    "if not rankings_same.empty:\n",
    "    rankings_same.to_csv(os.path.join(OUTPUT_DIR, 'rankings_same_rrc05.csv'))\n",
    "if not rankings_diff.empty:\n",
    "    rankings_diff.to_csv(os.path.join(OUTPUT_DIR, 'rankings_diff_rrc04.csv'))\n",
    "\n",
    "# Save overall scores\n",
    "if not scores_same.empty:\n",
    "    pd.DataFrame({'Method': scores_same.index, 'Overall Score': scores_same.values}).to_csv(\n",
    "        os.path.join(OUTPUT_DIR, 'overall_scores_same_rrc05.csv'), index=False)\n",
    "if not scores_diff.empty:\n",
    "    pd.DataFrame({'Method': scores_diff.index, 'Overall Score': scores_diff.values}).to_csv(\n",
    "        os.path.join(OUTPUT_DIR, 'overall_scores_diff_rrc04.csv'), index=False)\n",
    "\n",
    "print(f\"\\nAll results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal methods configured: {len(SYNTHETIC_DATASETS)}\")\n",
    "print(f\"Methods with same dataset (rrc05) results: {len(df_same)}\")\n",
    "print(f\"Methods with different dataset (rrc04) results: {len(df_diff)}\")\n",
    "\n",
    "print(\"\\nPNG plots collected:\")\n",
    "for plot_type, images in all_pngs.items():\n",
    "    print(f\"  {plot_type}: {len(images)} images\")\n",
    "\n",
    "if not scores_same.empty:\n",
    "    print(\"\\nTop 3 Methods Overall:\")\n",
    "    print(\"\\n  Same Dataset (rrc05):\")\n",
    "    for rank, (method, score) in enumerate(scores_same.head(3).items(), 1):\n",
    "        print(f\"    {rank}. {method}: {score:.2f}\")\n",
    "\n",
    "if not scores_diff.empty:\n",
    "    print(\"\\n  Different Dataset (rrc04):\")\n",
    "    for rank, (method, score) in enumerate(scores_diff.head(3).items(), 1):\n",
    "        print(f\"    {rank}. {method}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
