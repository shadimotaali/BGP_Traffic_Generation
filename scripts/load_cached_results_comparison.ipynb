{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Cached Results Comparison\n",
    "\n",
    "This notebook loads pre-computed evaluation results from `enhanced_v3_summary.csv` files\n",
    "and creates systematic comparisons without recalculating metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Analysis timestamp: {TIMESTAMP}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Define paths to all synthetic dataset results directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these paths to match your data\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR = f'./cached_results_comparison_{TIMESTAMP}'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SYNTHETIC_DATASETS = {\n",
    "    # SCAPY (direct generation - only one version)\n",
    "    'SCAPY': {\n",
    "        'generated': '/home/smotaali/BGP_Traffic_Generation/results_zend/Scapy_enhanced_1215_v3'\n",
    "    },\n",
    "\n",
    "    # GAN Default Values\n",
    "    'GAN_LSTM_default': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_LSTM_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_LSTM_rrc04'\n",
    "    },\n",
    "    'GAN_TimeGAN_default': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_TIME_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_TIME_rrc04'\n",
    "    },\n",
    "    'GAN_DoppelGanger_default': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_Doppelganger_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs/Correlation_GAN_Doppelganger_rrc04'\n",
    "    },\n",
    "\n",
    "    # GAN Enhanced/Tuned Parameters\n",
    "    'GAN_LSTM_enhanced': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_LSTM_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_LSTM_rrc04'\n",
    "    },\n",
    "    'GAN_TimeGAN_enhanced': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_TIME_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_TIME_rrc04'\n",
    "    },\n",
    "    'GAN_DoppelGanger_enhanced': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_Doppelganger_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/gan_outputs_improved/Correlation_GAN_Doppelganger_rrc04'\n",
    "    },\n",
    "\n",
    "    # SMOTE Variants\n",
    "    'SMOTE_normal': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/normal_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/normal_rrc04'\n",
    "    },\n",
    "    'SMOTE_borderline': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/borderline_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/borderline_rrc04'\n",
    "    },\n",
    "    'SMOTE_kmeans': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/kmeans_v3_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/kmeans_v3_rrc04'\n",
    "    },\n",
    "    'SMOTE_adasyn': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/adasyn_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/SMOTE_enhanced/adasyn_rrc04'\n",
    "    },\n",
    "\n",
    "    # Hybrid (SMOTE + GAN)\n",
    "    'Hybrid_SMOTE_GAN': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/synthetic_hybrid/compare_hybrid_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_huarie/results/synthetic_hybrid/compare_hybrid_rrc04'\n",
    "    },\n",
    "\n",
    "    # Copula\n",
    "    'Copula': {\n",
    "        'same_rrc05': '/home/smotaali/BGP_Traffic_Generation/results_zend/copula_rrc05',\n",
    "        'diff_rrc04': '/home/smotaali/BGP_Traffic_Generation/results_zend/copula_rrc04'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Key metrics configuration\n",
    "KEY_METRICS = {\n",
    "    'Mean KS Statistic': {'direction': 'lower', 'weight': 1.5},\n",
    "    'Mean Wasserstein Distance': {'direction': 'lower', 'weight': 1.5},\n",
    "    'Weighted Wasserstein Distance': {'direction': 'lower', 'weight': 1.5},\n",
    "    'PCA Centroid Distance': {'direction': 'lower', 'weight': 1.0},\n",
    "    'Correlation Similarity (Pearson)': {'direction': 'higher', 'weight': 2.0},\n",
    "    'Correlation Similarity (Spearman)': {'direction': 'higher', 'weight': 2.0},\n",
    "    'Distribution Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Distribution Score (Unweighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Correlation Score': {'direction': 'higher', 'weight': 1.5},\n",
    "    'Effect Size Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Effect Size Score (Unweighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Wasserstein Score (Weighted)': {'direction': 'higher', 'weight': 1.0},\n",
    "    'KS Excellent Features': {'direction': 'higher', 'weight': 1.0},\n",
    "    'KS Good or Better Features': {'direction': 'higher', 'weight': 1.0},\n",
    "    'Negligible Effect Features': {'direction': 'higher', 'weight': 1.0},\n",
    "}\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Number of methods configured: {len(SYNTHETIC_DATASETS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_summary_csv(directory: str) -> Optional[str]:\n",
    "    \"\"\"Find the enhanced_v3_summary.csv file in a directory.\"\"\"\n",
    "    summary_file = os.path.join(directory, 'enhanced_v3_summary.csv')\n",
    "    if os.path.exists(summary_file):\n",
    "        return summary_file\n",
    "    \n",
    "    # Try to find any summary.csv file\n",
    "    for f in os.listdir(directory) if os.path.exists(directory) else []:\n",
    "        if 'summary' in f.lower() and f.endswith('.csv'):\n",
    "            return os.path.join(directory, f)\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_summary_csv(filepath: str) -> Optional[Dict]:\n",
    "    \"\"\"Load a summary CSV and convert to dictionary.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        if 'Metric' in df.columns and 'Value' in df.columns:\n",
    "            result = {}\n",
    "            for _, row in df.iterrows():\n",
    "                metric = row['Metric']\n",
    "                value = row['Value']\n",
    "                # Handle strings like \"100.0/100\"\n",
    "                if isinstance(value, str) and '/' in value:\n",
    "                    try:\n",
    "                        value = float(value.split('/')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                result[metric] = value\n",
    "            return result\n",
    "        else:\n",
    "            # Try first two columns\n",
    "            result = {}\n",
    "            cols = df.columns.tolist()\n",
    "            for _, row in df.iterrows():\n",
    "                metric = row[cols[0]]\n",
    "                value = row[cols[1]]\n",
    "                if isinstance(value, str) and '/' in value:\n",
    "                    try:\n",
    "                        value = float(value.split('/')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "                else:\n",
    "                    try:\n",
    "                        value = float(value)\n",
    "                    except:\n",
    "                        pass\n",
    "                result[metric] = value\n",
    "            return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filepath}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_results(datasets: Dict) -> Dict:\n",
    "    \"\"\"Load all cached results from the dataset directories.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for method_name, variants in datasets.items():\n",
    "        results[method_name] = {}\n",
    "        for variant_name, path in variants.items():\n",
    "            summary_file = find_summary_csv(path)\n",
    "            if summary_file:\n",
    "                data = load_summary_csv(summary_file)\n",
    "                if data:\n",
    "                    results[method_name][variant_name] = data\n",
    "                    print(f\"✓ Loaded: {method_name} - {variant_name}\")\n",
    "                else:\n",
    "                    print(f\"✗ Failed to parse: {method_name} - {variant_name}\")\n",
    "            else:\n",
    "                print(f\"✗ Not found: {method_name} - {variant_name} ({path})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_comparison_dataframe(results: Dict, evaluation_type: str = 'same_rrc05') -> pd.DataFrame:\n",
    "    \"\"\"Create a comparison DataFrame for a specific evaluation type.\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for method_name, variants in results.items():\n",
    "        # Handle special cases like SCAPY which only has 'generated'\n",
    "        if evaluation_type in variants:\n",
    "            data = variants[evaluation_type]\n",
    "        elif 'generated' in variants:\n",
    "            data = variants['generated']\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        row = {'Method': method_name}\n",
    "        row.update(data)\n",
    "        rows.append(row)\n",
    "    \n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(rows)\n",
    "    df.set_index('Method', inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_overall_score(row: pd.Series, metrics: Dict = KEY_METRICS) -> float:\n",
    "    \"\"\"Calculate an overall weighted score for a method.\"\"\"\n",
    "    score = 0\n",
    "    total_weight = 0\n",
    "    \n",
    "    for metric, config in metrics.items():\n",
    "        if metric in row and pd.notna(row[metric]):\n",
    "            value = row[metric]\n",
    "            weight = config['weight']\n",
    "            direction = config['direction']\n",
    "            \n",
    "            if direction == 'higher':\n",
    "                if 'Score' in metric:\n",
    "                    normalized = value / 100\n",
    "                elif 'Correlation' in metric:\n",
    "                    normalized = value\n",
    "                else:\n",
    "                    normalized = min(value / 100, 1.0)\n",
    "            else:\n",
    "                if 'KS' in metric:\n",
    "                    normalized = max(0, 1 - value)\n",
    "                elif 'Wasserstein' in metric or 'Distance' in metric:\n",
    "                    normalized = max(0, 1 - value / 2)\n",
    "                else:\n",
    "                    normalized = max(0, 1 - value)\n",
    "            \n",
    "            score += normalized * weight\n",
    "            total_weight += weight\n",
    "    \n",
    "    return (score / total_weight * 100) if total_weight > 0 else 0\n",
    "\n",
    "\n",
    "def create_ranking_table(df: pd.DataFrame, metrics: List[str] = None) -> pd.DataFrame:\n",
    "    \"\"\"Create a ranking table for methods across metrics.\"\"\"\n",
    "    if metrics is None:\n",
    "        metrics = list(KEY_METRICS.keys())\n",
    "    \n",
    "    available_metrics = [m for m in metrics if m in df.columns]\n",
    "    rankings = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for metric in available_metrics:\n",
    "        if metric in df.columns:\n",
    "            config = KEY_METRICS.get(metric, {'direction': 'higher'})\n",
    "            ascending = config['direction'] == 'lower'\n",
    "            rankings[metric] = df[metric].rank(ascending=ascending, na_option='bottom')\n",
    "    \n",
    "    rankings['Average Rank'] = rankings.mean(axis=1)\n",
    "    rankings = rankings.sort_values('Average Rank')\n",
    "    \n",
    "    return rankings\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load All Cached Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading results from enhanced_v3_summary.csv files...\")\n",
    "print(\"=\" * 60)\n",
    "results = load_all_results(SYNTHETIC_DATASETS)\n",
    "print(\"\\nLoading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Comparison DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrames\n",
    "df_same = create_comparison_dataframe(results, 'same_rrc05')\n",
    "df_diff = create_comparison_dataframe(results, 'diff_rrc04')\n",
    "\n",
    "# For SCAPY which only has 'generated', include it in both\n",
    "for method, variants in results.items():\n",
    "    if 'generated' in variants and method not in df_same.index:\n",
    "        row = variants['generated']\n",
    "        row_df = pd.DataFrame([row], index=[method])\n",
    "        df_same = pd.concat([df_same, row_df])\n",
    "        df_diff = pd.concat([df_diff, row_df])\n",
    "\n",
    "print(f\"Same dataset (rrc05): {len(df_same)} methods\")\n",
    "print(f\"Different dataset (rrc04): {len(df_diff)} methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select key metrics to display\n",
    "display_metrics = [\n",
    "    'Mean KS Statistic',\n",
    "    'Mean Wasserstein Distance',\n",
    "    'Correlation Similarity (Pearson)',\n",
    "    'Correlation Similarity (Spearman)',\n",
    "    'PCA Centroid Distance',\n",
    "    'Distribution Score (Weighted)',\n",
    "    'Correlation Score',\n",
    "    'Effect Size Score (Weighted)',\n",
    "    'KS Excellent Features',\n",
    "    'KS Good or Better Features',\n",
    "    'Negligible Effect Features'\n",
    "]\n",
    "\n",
    "available_display = [m for m in display_metrics if m in df_same.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAME DATASET (rrc05) COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "if not df_same.empty and available_display:\n",
    "    display(df_same[available_display].round(4).style.background_gradient(cmap='RdYlGn', axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_display_diff = [m for m in display_metrics if m in df_diff.columns]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DIFFERENT DATASET (rrc04) COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "if not df_diff.empty and available_display_diff:\n",
    "    display(df_diff[available_display_diff].round(4).style.background_gradient(cmap='RdYlGn', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Overall Rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall scores\n",
    "scores_same = df_same.apply(calculate_overall_score, axis=1).sort_values(ascending=False)\n",
    "scores_diff = df_diff.apply(calculate_overall_score, axis=1).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL RANKINGS - Same Dataset (rrc05)\")\n",
    "print(\"=\" * 60)\n",
    "for rank, (method, score) in enumerate(scores_same.items(), 1):\n",
    "    print(f\"{rank:2d}. {method:30s}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OVERALL RANKINGS - Different Dataset (rrc04)\")\n",
    "print(\"=\" * 60)\n",
    "for rank, (method, score) in enumerate(scores_diff.items(), 1):\n",
    "    print(f\"{rank:2d}. {method:30s}: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall Score Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Same dataset\n",
    "colors_same = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores_same)))\n",
    "axes[0].barh(range(len(scores_same)), scores_same, color=colors_same)\n",
    "axes[0].set_yticks(range(len(scores_same)))\n",
    "axes[0].set_yticklabels(scores_same.index)\n",
    "axes[0].set_xlabel('Overall Score')\n",
    "axes[0].set_title('Same Dataset (rrc05) - Overall Score', fontsize=14)\n",
    "for i, (idx, v) in enumerate(scores_same.items()):\n",
    "    axes[0].text(v, i, f' {v:.1f}', va='center', fontsize=9)\n",
    "\n",
    "# Different dataset\n",
    "colors_diff = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(scores_diff)))\n",
    "axes[1].barh(range(len(scores_diff)), scores_diff, color=colors_diff)\n",
    "axes[1].set_yticks(range(len(scores_diff)))\n",
    "axes[1].set_yticklabels(scores_diff.index)\n",
    "axes[1].set_xlabel('Overall Score')\n",
    "axes[1].set_title('Different Dataset (rrc04) - Overall Score', fontsize=14)\n",
    "for i, (idx, v) in enumerate(scores_diff.items()):\n",
    "    axes[1].text(v, i, f' {v:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'overall_comparison.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap - Same Dataset\n",
    "key_metric_names = list(KEY_METRICS.keys())\n",
    "available_heatmap = [m for m in key_metric_names if m in df_same.columns]\n",
    "\n",
    "if len(available_heatmap) >= 2:\n",
    "    plot_data = df_same[available_heatmap].copy()\n",
    "    \n",
    "    # Normalize each column\n",
    "    for col in plot_data.columns:\n",
    "        config = KEY_METRICS.get(col, {'direction': 'higher'})\n",
    "        values = plot_data[col]\n",
    "        min_val, max_val = values.min(), values.max()\n",
    "        if max_val > min_val:\n",
    "            normalized = (values - min_val) / (max_val - min_val)\n",
    "            if config['direction'] == 'lower':\n",
    "                normalized = 1 - normalized\n",
    "            plot_data[col] = normalized\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.heatmap(plot_data, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score (higher=better)'})\n",
    "    ax.set_title('Method Performance Heatmap (Same Dataset - rrc05)\\n(Normalized: 1=best, 0=worst)', fontsize=14)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'heatmap_same_rrc05.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap - Different Dataset\n",
    "available_heatmap_diff = [m for m in key_metric_names if m in df_diff.columns]\n",
    "\n",
    "if len(available_heatmap_diff) >= 2:\n",
    "    plot_data = df_diff[available_heatmap_diff].copy()\n",
    "    \n",
    "    for col in plot_data.columns:\n",
    "        config = KEY_METRICS.get(col, {'direction': 'higher'})\n",
    "        values = plot_data[col]\n",
    "        min_val, max_val = values.min(), values.max()\n",
    "        if max_val > min_val:\n",
    "            normalized = (values - min_val) / (max_val - min_val)\n",
    "            if config['direction'] == 'lower':\n",
    "                normalized = 1 - normalized\n",
    "            plot_data[col] = normalized\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    sns.heatmap(plot_data, annot=True, fmt='.2f', cmap='RdYlGn',\n",
    "                ax=ax, vmin=0, vmax=1, cbar_kws={'label': 'Normalized Score (higher=better)'})\n",
    "    ax.set_title('Method Performance Heatmap (Different Dataset - rrc04)\\n(Normalized: 1=best, 0=worst)', fontsize=14)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'heatmap_diff_rrc04.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Metric Bar Charts\n",
    "key_metrics_to_plot = [\n",
    "    ('Mean KS Statistic', 'lower'),\n",
    "    ('Correlation Similarity (Pearson)', 'higher'),\n",
    "    ('Correlation Score', 'higher'),\n",
    "    ('Distribution Score (Weighted)', 'higher')\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (metric, direction) in enumerate(key_metrics_to_plot):\n",
    "    if metric in df_same.columns:\n",
    "        values = df_same[metric].dropna().sort_values(ascending=(direction == 'lower'))\n",
    "        colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(values)))\n",
    "        if direction == 'lower':\n",
    "            colors = colors[::-1]\n",
    "        \n",
    "        axes[idx].barh(range(len(values)), values, color=colors)\n",
    "        axes[idx].set_yticks(range(len(values)))\n",
    "        axes[idx].set_yticklabels(values.index)\n",
    "        axes[idx].set_xlabel(metric)\n",
    "        axes[idx].set_title(f'{metric} (Same Dataset - rrc05)')\n",
    "        \n",
    "        for i, (method, v) in enumerate(values.items()):\n",
    "            axes[idx].text(v, i, f' {v:.4f}', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'key_metrics_comparison.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Ranking Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display ranking tables\n",
    "rankings_same = create_ranking_table(df_same)\n",
    "rankings_diff = create_ranking_table(df_diff)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANKING TABLE - Same Dataset (rrc05)\")\n",
    "print(\"(Lower rank = better performance)\")\n",
    "print(\"=\" * 80)\n",
    "display(rankings_same.round(2).style.background_gradient(cmap='RdYlGn_r', axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANKING TABLE - Different Dataset (rrc04)\")\n",
    "print(\"(Lower rank = better performance)\")\n",
    "print(\"=\" * 80)\n",
    "display(rankings_diff.round(2).style.background_gradient(cmap='RdYlGn_r', axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Methods by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {\n",
    "    'Distribution Similarity': ['Mean KS Statistic', 'Mean Wasserstein Distance'],\n",
    "    'Correlation Preservation': ['Correlation Similarity (Pearson)', 'Correlation Similarity (Spearman)'],\n",
    "    'Overall Scores': ['Distribution Score (Weighted)', 'Correlation Score', 'Effect Size Score (Weighted)']\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST METHODS BY METRIC CATEGORY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for eval_type, df in [('Same Dataset (rrc05)', df_same), ('Different Dataset (rrc04)', df_diff)]:\n",
    "    print(f\"\\n{eval_type}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for category, metrics in categories.items():\n",
    "        print(f\"\\n  {category}:\")\n",
    "        for metric in metrics:\n",
    "            if metric in df.columns:\n",
    "                config = KEY_METRICS.get(metric, {'direction': 'higher'})\n",
    "                if config['direction'] == 'higher':\n",
    "                    best = df[metric].idxmax()\n",
    "                    value = df[metric].max()\n",
    "                else:\n",
    "                    best = df[metric].idxmin()\n",
    "                    value = df[metric].min()\n",
    "                print(f\"    {metric}: {best} ({value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "df_same.to_csv(os.path.join(OUTPUT_DIR, 'comparison_same_rrc05.csv'))\n",
    "df_diff.to_csv(os.path.join(OUTPUT_DIR, 'comparison_diff_rrc04.csv'))\n",
    "rankings_same.to_csv(os.path.join(OUTPUT_DIR, 'rankings_same_rrc05.csv'))\n",
    "rankings_diff.to_csv(os.path.join(OUTPUT_DIR, 'rankings_diff_rrc04.csv'))\n",
    "\n",
    "# Save overall scores\n",
    "pd.DataFrame({'Method': scores_same.index, 'Overall Score': scores_same.values}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR, 'overall_scores_same_rrc05.csv'), index=False)\n",
    "pd.DataFrame({'Method': scores_diff.index, 'Overall Score': scores_diff.values}).to_csv(\n",
    "    os.path.join(OUTPUT_DIR, 'overall_scores_diff_rrc04.csv'), index=False)\n",
    "\n",
    "print(f\"\\nAll results saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nSaved files:\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    print(f\"  - {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal methods compared: {len(results)}\")\n",
    "print(f\"Methods with same dataset (rrc05) results: {len(df_same)}\")\n",
    "print(f\"Methods with different dataset (rrc04) results: {len(df_diff)}\")\n",
    "\n",
    "print(\"\\nTop 3 Methods Overall:\")\n",
    "print(\"\\n  Same Dataset (rrc05):\")\n",
    "for rank, (method, score) in enumerate(scores_same.head(3).items(), 1):\n",
    "    print(f\"    {rank}. {method}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n  Different Dataset (rrc04):\")\n",
    "for rank, (method, score) in enumerate(scores_diff.head(3).items(), 1):\n",
    "    print(f\"    {rank}. {method}: {score:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
