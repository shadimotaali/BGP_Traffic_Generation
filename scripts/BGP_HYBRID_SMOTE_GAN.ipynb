{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Hybrid SMOTE-GAN Synthetic Data Generation\n",
    "\n",
    "## Approach: Best of Both Worlds\n",
    "\n",
    "This notebook implements a hybrid approach that combines:\n",
    "- **SMOTE-KMeans**: Fast, excellent correlation preservation (89.5%)\n",
    "- **DoppelGANger**: Complex temporal patterns, novel generation\n",
    "- **Conditional Sampling**: Domain-constrained feature generation\n",
    "- **Mixture Models**: Sparse event-driven features\n",
    "\n",
    "### Rationale\n",
    "- SMOTE-KMeans achieved 34.0/100 with best correlation (89.5%)\n",
    "- DoppelGANger achieved 34.9/100 but slower training\n",
    "- Both struggle with same features: unique_as_path_max, edit_distance_*, flaps\n",
    "- Hybrid approach targets weaknesses with specialized generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE ASSIGNMENT STRATEGY\n",
    "# =============================================================================\n",
    "\n",
    "FEATURE_ASSIGNMENT = {\n",
    "    # SMOTE-KMeans: Features with good correlation preservation needed\n",
    "    # These are \"static\" features that don't need complex temporal modeling\n",
    "    'smote_kmeans': [\n",
    "        'announcements',        # Base volume - SMOTE handles well\n",
    "        'withdrawals',          # Base volume\n",
    "        'nlri_ann',             # Correlated with announcements\n",
    "        'dups',                 # Simple count\n",
    "        'origin_0',             # Categorical-like\n",
    "        'origin_2',             # Categorical-like\n",
    "        'origin_changes',       # Low complexity\n",
    "        'as_path_max',          # SMOTE preserves distribution\n",
    "        'imp_wd_spath',         # Correlated feature\n",
    "        'imp_wd_dpath',         # Correlated feature\n",
    "    ],\n",
    "    \n",
    "    # Empirical Sampling: Heavy-tailed features - sample from KDE\n",
    "    'empirical_kde': [\n",
    "        'unique_as_path_max',   # Worst feature - use empirical\n",
    "        'edit_distance_max',    # Heavy-tailed\n",
    "        'edit_distance_avg',    # Continuous\n",
    "        'rare_ases_avg',        # Heavy-tailed, Zipf-like\n",
    "    ],\n",
    "    \n",
    "    # Conditional Generation: Derived from other features\n",
    "    'conditional': [\n",
    "        'edit_distance_dict_0', \n",
    "        'edit_distance_dict_1',\n",
    "        'edit_distance_dict_2', \n",
    "        'edit_distance_dict_3',\n",
    "        'edit_distance_dict_4',\n",
    "        'edit_distance_dict_5',\n",
    "        'edit_distance_dict_6',\n",
    "        'edit_distance_unique_dict_0',\n",
    "        'edit_distance_unique_dict_1',\n",
    "    ],\n",
    "    \n",
    "    # Zero-Inflated Mixture: Sparse event-driven features\n",
    "    'mixture_model': [\n",
    "        'flaps',                # Sparse, event-driven\n",
    "        'nadas',                # Sparse, event-driven  \n",
    "        'imp_wd',               # Can be sparse\n",
    "        'number_rare_ases',     # Integer count, sparse\n",
    "    ]\n",
    "}\n",
    "\n",
    "# All 27 features\n",
    "ALL_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_avg', 'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'rare_ases_avg',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Integer features (must be rounded)\n",
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Generation settings\n",
    "N_SYNTHETIC = 20000  # Number of samples to generate\n",
    "N_CLUSTERS = 15      # For KMeans-SMOTE\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"\\nFeature assignment:\")\n",
    "for strategy, features in FEATURE_ASSIGNMENT.items():\n",
    "    print(f\"  {strategy}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your real BGP data\n",
    "# Adjust path as needed\n",
    "DATA_PATH = '../data/likely_normal_traffic.csv'  # or your data path\n",
    "\n",
    "try:\n",
    "    df_real = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded {len(df_real)} samples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH to point to your real BGP data\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"\\nCreating dummy data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    df_real = pd.DataFrame({\n",
    "        col: np.random.exponential(scale=10, size=10000) \n",
    "        for col in ALL_FEATURES\n",
    "    })\n",
    "\n",
    "# Filter to only the features we need\n",
    "available_features = [f for f in ALL_FEATURES if f in df_real.columns]\n",
    "X_real = df_real[available_features].copy()\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} features\")\n",
    "print(f\"Real data shape: {X_real.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SMOTE-KMeans Generator (for correlated features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_smote_kmeans(X, features, n_samples, n_clusters=15, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate samples using KMeans clustering + SMOTE.\n",
    "    Best for features requiring correlation preservation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame - Real data\n",
    "    features : list - Features to generate\n",
    "    n_samples : int - Number of samples to generate\n",
    "    n_clusters : int - Number of KMeans clusters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with generated features\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Filter to available features\n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    X_subset = X[available].values\n",
    "    \n",
    "    # Apply log1p transform for stability\n",
    "    X_log = np.log1p(np.clip(X_subset, 0, None))\n",
    "    \n",
    "    # Scale for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_log)\n",
    "    \n",
    "    # KMeans clustering\n",
    "    n_clusters_actual = min(n_clusters, len(X_subset) // 10)\n",
    "    kmeans = KMeans(n_clusters=n_clusters_actual, random_state=random_state, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Allocate samples per cluster proportionally\n",
    "    cluster_sizes = np.bincount(cluster_labels)\n",
    "    samples_per_cluster = (cluster_sizes / cluster_sizes.sum() * n_samples).astype(int)\n",
    "    samples_per_cluster = np.maximum(samples_per_cluster, 1)\n",
    "    \n",
    "    synthetic_all = []\n",
    "    \n",
    "    for cluster_id in range(n_clusters_actual):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        X_cluster = X_log[cluster_mask]\n",
    "        \n",
    "        if len(X_cluster) < 3:\n",
    "            continue\n",
    "        \n",
    "        n_to_generate = samples_per_cluster[cluster_id]\n",
    "        minority_size = max(2, int(len(X_cluster) * 0.1))\n",
    "        safe_k = min(3, minority_size - 1)\n",
    "        \n",
    "        if safe_k < 1:\n",
    "            continue\n",
    "        \n",
    "        # Create artificial minority class for SMOTE\n",
    "        minority_idx = np.random.choice(len(X_cluster), minority_size, replace=False)\n",
    "        y_cluster = np.zeros(len(X_cluster))\n",
    "        y_cluster[minority_idx] = 1\n",
    "        \n",
    "        try:\n",
    "            smote = SMOTE(\n",
    "                sampling_strategy={1: n_to_generate + minority_size},\n",
    "                k_neighbors=safe_k,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            X_res, y_res = smote.fit_resample(X_cluster, y_cluster)\n",
    "            synthetic = X_res[y_res == 1][minority_size:]\n",
    "            synthetic_all.append(synthetic)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if synthetic_all:\n",
    "        result = np.vstack(synthetic_all)\n",
    "        # Inverse log1p transform\n",
    "        result = np.expm1(result)\n",
    "        result = np.clip(result, 0, None)\n",
    "        return pd.DataFrame(result[:n_samples], columns=available)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=available)\n",
    "\n",
    "print(\"SMOTE-KMeans generator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Empirical KDE Generator (for heavy-tailed features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_empirical_kde(X, features, n_samples, bandwidth_factor=0.5, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate samples using Kernel Density Estimation on real data.\n",
    "    Best for heavy-tailed features that GANs struggle with.\n",
    "    \n",
    "    Uses adaptive bandwidth based on feature variance.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for feature in available:\n",
    "        real_values = X[feature].values\n",
    "        real_values = real_values[~np.isnan(real_values)]\n",
    "        \n",
    "        if len(real_values) < 10:\n",
    "            result[feature] = np.zeros(n_samples)\n",
    "            continue\n",
    "        \n",
    "        # Log transform for heavy-tailed\n",
    "        log_values = np.log1p(np.clip(real_values, 0, None))\n",
    "        \n",
    "        try:\n",
    "            # Fit KDE with Scott's rule bandwidth * factor\n",
    "            kde = gaussian_kde(log_values, bw_method='scott')\n",
    "            kde.set_bandwidth(kde.factor * bandwidth_factor)\n",
    "            \n",
    "            # Sample from KDE\n",
    "            synthetic_log = kde.resample(n_samples).flatten()\n",
    "            \n",
    "            # Inverse transform\n",
    "            synthetic = np.expm1(synthetic_log)\n",
    "            synthetic = np.clip(synthetic, 0, np.percentile(real_values, 99.9))\n",
    "            \n",
    "            result[feature] = synthetic\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Fallback to bootstrap sampling\n",
    "            result[feature] = np.random.choice(real_values, n_samples, replace=True)\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "print(\"Empirical KDE generator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Zero-Inflated Mixture Generator (for sparse features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_mixture_model(X, features, n_samples, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate sparse features using zero-inflated mixture model.\n",
    "    \n",
    "    Model: P(x) = p_zero * I(x=0) + (1-p_zero) * f(x|x>0)\n",
    "    \n",
    "    Best for features like 'flaps', 'nadas' that are often zero.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for feature in available:\n",
    "        real_values = X[feature].values\n",
    "        real_values = real_values[~np.isnan(real_values)]\n",
    "        \n",
    "        # Calculate zero probability\n",
    "        p_zero = (real_values == 0).mean()\n",
    "        \n",
    "        # Get non-zero values\n",
    "        non_zero = real_values[real_values > 0]\n",
    "        \n",
    "        synthetic = np.zeros(n_samples)\n",
    "        \n",
    "        # Determine which samples are non-zero\n",
    "        non_zero_mask = np.random.random(n_samples) > p_zero\n",
    "        n_non_zero = non_zero_mask.sum()\n",
    "        \n",
    "        if n_non_zero > 0 and len(non_zero) > 0:\n",
    "            # Sample from non-zero distribution with small noise\n",
    "            sampled = np.random.choice(non_zero, n_non_zero, replace=True)\n",
    "            # Add small perturbation\n",
    "            noise = np.random.normal(0, non_zero.std() * 0.1, n_non_zero)\n",
    "            synthetic[non_zero_mask] = np.maximum(1, sampled + noise)\n",
    "        \n",
    "        result[feature] = synthetic\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "print(\"Mixture model generator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Conditional Generator (for edit_distance_dict features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_conditional(X, features, n_samples, synthetic_base, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate edit_distance_dict features conditioned on:\n",
    "    - announcements (volume)\n",
    "    - edit_distance_max (upper bound)\n",
    "    \n",
    "    The dict values should follow a distribution where:\n",
    "    - Lower edit distances (0-2) are more common\n",
    "    - Higher edit distances (3-6) are rare\n",
    "    - Sum relates to announcement volume\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Learn the conditional distribution from real data\n",
    "    # P(edit_distance_dict_i | announcements, edit_distance_max)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Get conditioning variables from synthetic_base if available\n",
    "    if 'announcements' in synthetic_base.columns:\n",
    "        syn_announcements = synthetic_base['announcements'].values\n",
    "    else:\n",
    "        syn_announcements = np.random.choice(X['announcements'].values, n_samples)\n",
    "    \n",
    "    if 'edit_distance_max' in synthetic_base.columns:\n",
    "        syn_ed_max = synthetic_base['edit_distance_max'].values.astype(int)\n",
    "    else:\n",
    "        syn_ed_max = np.random.choice(X['edit_distance_max'].values, n_samples).astype(int)\n",
    "    \n",
    "    # Learn typical distribution shape from real data\n",
    "    ed_dict_cols = [f'edit_distance_dict_{i}' for i in range(7) if f'edit_distance_dict_{i}' in X.columns]\n",
    "    \n",
    "    if ed_dict_cols:\n",
    "        # Calculate average proportions\n",
    "        real_ed_dict = X[ed_dict_cols].values\n",
    "        row_sums = real_ed_dict.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        proportions = (real_ed_dict / row_sums).mean(axis=0)\n",
    "        \n",
    "        # Generate for each sample\n",
    "        for i, col in enumerate(ed_dict_cols):\n",
    "            # Scale by announcements (more announcements = more edit events)\n",
    "            scale_factor = np.log1p(syn_announcements) / np.log1p(X['announcements'].mean())\n",
    "            base_value = X[col].mean() * scale_factor\n",
    "            \n",
    "            # Add noise\n",
    "            noise = np.random.normal(0, X[col].std() * 0.3, n_samples)\n",
    "            synthetic_col = np.maximum(0, base_value + noise)\n",
    "            \n",
    "            # Zero out values beyond edit_distance_max\n",
    "            if i > 0:  # dict_1 and above\n",
    "                synthetic_col[syn_ed_max < i] = 0\n",
    "            \n",
    "            result[col] = synthetic_col\n",
    "    \n",
    "    # Handle edit_distance_unique_dict features similarly\n",
    "    ed_unique_cols = [f'edit_distance_unique_dict_{i}' for i in range(2) \n",
    "                     if f'edit_distance_unique_dict_{i}' in X.columns]\n",
    "    \n",
    "    for col in ed_unique_cols:\n",
    "        real_values = X[col].values\n",
    "        scale_factor = np.log1p(syn_announcements) / np.log1p(X['announcements'].mean())\n",
    "        base_value = real_values.mean() * scale_factor\n",
    "        noise = np.random.normal(0, real_values.std() * 0.3, n_samples)\n",
    "        result[col] = np.maximum(0, base_value + noise)\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "print(\"Conditional generator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Correlation Alignment via Cholesky Decomposition\n",
    "# =============================================================================\n",
    "\n",
    "def align_correlations(synthetic, real, features_to_align=None):\n",
    "    \"\"\"\n",
    "    Adjust synthetic data to match real correlation structure.\n",
    "    \n",
    "    Uses Cholesky decomposition to impose correlation structure:\n",
    "    1. Decorrelate synthetic data\n",
    "    2. Re-correlate with real data's correlation matrix\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    synthetic : DataFrame - Generated data\n",
    "    real : DataFrame - Real data\n",
    "    features_to_align : list - Features to align (default: all common)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with aligned correlations\n",
    "    \"\"\"\n",
    "    if features_to_align is None:\n",
    "        features_to_align = [c for c in synthetic.columns if c in real.columns]\n",
    "    \n",
    "    if len(features_to_align) < 2:\n",
    "        return synthetic\n",
    "    \n",
    "    try:\n",
    "        # Get correlation matrices\n",
    "        real_subset = real[features_to_align]\n",
    "        syn_subset = synthetic[features_to_align].copy()\n",
    "        \n",
    "        real_corr = real_subset.corr().values\n",
    "        syn_corr = syn_subset.corr().values\n",
    "        \n",
    "        # Add small regularization for numerical stability\n",
    "        eps = 1e-6\n",
    "        real_corr = real_corr + eps * np.eye(len(features_to_align))\n",
    "        syn_corr = syn_corr + eps * np.eye(len(features_to_align))\n",
    "        \n",
    "        # Cholesky decomposition\n",
    "        L_real = np.linalg.cholesky(real_corr)\n",
    "        L_syn = np.linalg.cholesky(syn_corr)\n",
    "        \n",
    "        # Standardize synthetic data\n",
    "        syn_mean = syn_subset.mean().values\n",
    "        syn_std = syn_subset.std().values\n",
    "        syn_std[syn_std == 0] = 1  # Avoid division by zero\n",
    "        \n",
    "        syn_standardized = (syn_subset.values - syn_mean) / syn_std\n",
    "        \n",
    "        # Decorrelate then re-correlate\n",
    "        syn_decorr = syn_standardized @ np.linalg.inv(L_syn.T)\n",
    "        syn_recorr = syn_decorr @ L_real.T\n",
    "        \n",
    "        # Rescale to match real data's scale\n",
    "        real_mean = real_subset.mean().values\n",
    "        real_std = real_subset.std().values\n",
    "        \n",
    "        aligned = syn_recorr * real_std + real_mean\n",
    "        \n",
    "        # Update synthetic DataFrame\n",
    "        result = synthetic.copy()\n",
    "        for i, col in enumerate(features_to_align):\n",
    "            result[col] = aligned[:, i]\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Correlation alignment failed: {e}\")\n",
    "        return synthetic\n",
    "\n",
    "print(\"Correlation alignment function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-Processing & Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Post-Processing: Enforce BGP Domain Constraints\n",
    "# =============================================================================\n",
    "\n",
    "def enforce_bgp_constraints(synthetic, real):\n",
    "    \"\"\"\n",
    "    Apply BGP domain-specific constraints to synthetic data.\n",
    "    \n",
    "    Constraints:\n",
    "    1. All features non-negative\n",
    "    2. Integer features are integers\n",
    "    3. origin_0 + origin_2 <= announcements\n",
    "    4. edit_distance_max >= max(dict indices with non-zero values)\n",
    "    5. Values within realistic bounds (based on real data percentiles)\n",
    "    \"\"\"\n",
    "    result = synthetic.copy()\n",
    "    \n",
    "    # 1. Non-negative\n",
    "    for col in result.columns:\n",
    "        result[col] = np.maximum(0, result[col])\n",
    "    \n",
    "    # 2. Integer features\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = np.round(result[col]).astype(int)\n",
    "    \n",
    "    # 3. Origin constraint\n",
    "    if all(c in result.columns for c in ['origin_0', 'origin_2', 'announcements']):\n",
    "        origin_sum = result['origin_0'] + result['origin_2']\n",
    "        excess = origin_sum > result['announcements']\n",
    "        if excess.any():\n",
    "            scale = result.loc[excess, 'announcements'] / origin_sum[excess]\n",
    "            result.loc[excess, 'origin_0'] = (result.loc[excess, 'origin_0'] * scale).astype(int)\n",
    "            result.loc[excess, 'origin_2'] = (result.loc[excess, 'origin_2'] * scale).astype(int)\n",
    "    \n",
    "    # 4. Edit distance max constraint\n",
    "    ed_dict_cols = [f'edit_distance_dict_{i}' for i in range(7) if f'edit_distance_dict_{i}' in result.columns]\n",
    "    if 'edit_distance_max' in result.columns and ed_dict_cols:\n",
    "        for idx in result.index:\n",
    "            ed_max = int(result.loc[idx, 'edit_distance_max'])\n",
    "            for i, col in enumerate(ed_dict_cols):\n",
    "                if i > ed_max:\n",
    "                    result.loc[idx, col] = 0\n",
    "    \n",
    "    # 5. Realistic bounds (clip to 99.5th percentile of real data)\n",
    "    for col in result.columns:\n",
    "        if col in real.columns:\n",
    "            upper_bound = np.percentile(real[col], 99.5)\n",
    "            result[col] = np.clip(result[col], 0, upper_bound * 1.1)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"BGP constraints function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Hybrid Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYBRID GENERATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def generate_hybrid(X_real, n_samples, feature_assignment, random_state=42):\n",
    "    \"\"\"\n",
    "    Main hybrid generation pipeline combining multiple strategies.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. SMOTE-KMeans for correlated features\n",
    "    2. Empirical KDE for heavy-tailed features\n",
    "    3. Mixture model for sparse features\n",
    "    4. Conditional generation for dependent features\n",
    "    5. Correlation alignment\n",
    "    6. BGP constraint enforcement\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"HYBRID SMOTE-GAN GENERATION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    synthetic_parts = {}\n",
    "    \n",
    "    # Step 1: SMOTE-KMeans for base features\n",
    "    print(\"\\n[1/6] Generating SMOTE-KMeans features...\")\n",
    "    smote_features = feature_assignment.get('smote_kmeans', [])\n",
    "    if smote_features:\n",
    "        synthetic_parts['smote'] = generate_smote_kmeans(\n",
    "            X_real, smote_features, n_samples, \n",
    "            n_clusters=N_CLUSTERS, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['smote'].columns)} features via SMOTE-KMeans\")\n",
    "    \n",
    "    # Step 2: Empirical KDE for heavy-tailed features\n",
    "    print(\"\\n[2/6] Generating Empirical KDE features...\")\n",
    "    kde_features = feature_assignment.get('empirical_kde', [])\n",
    "    if kde_features:\n",
    "        synthetic_parts['kde'] = generate_empirical_kde(\n",
    "            X_real, kde_features, n_samples, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['kde'].columns)} features via KDE\")\n",
    "    \n",
    "    # Step 3: Mixture model for sparse features\n",
    "    print(\"\\n[3/6] Generating Mixture Model features...\")\n",
    "    mixture_features = feature_assignment.get('mixture_model', [])\n",
    "    if mixture_features:\n",
    "        synthetic_parts['mixture'] = generate_mixture_model(\n",
    "            X_real, mixture_features, n_samples, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['mixture'].columns)} features via Mixture\")\n",
    "    \n",
    "    # Combine parts so far for conditioning\n",
    "    synthetic_base = pd.concat(synthetic_parts.values(), axis=1)\n",
    "    \n",
    "    # Step 4: Conditional generation\n",
    "    print(\"\\n[4/6] Generating Conditional features...\")\n",
    "    conditional_features = feature_assignment.get('conditional', [])\n",
    "    if conditional_features:\n",
    "        synthetic_parts['conditional'] = generate_conditional(\n",
    "            X_real, conditional_features, n_samples, \n",
    "            synthetic_base, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['conditional'].columns)} features conditionally\")\n",
    "    \n",
    "    # Combine all parts\n",
    "    print(\"\\n[5/6] Combining and aligning correlations...\")\n",
    "    synthetic_combined = pd.concat(synthetic_parts.values(), axis=1)\n",
    "    \n",
    "    # Ensure we have all expected columns\n",
    "    for col in X_real.columns:\n",
    "        if col not in synthetic_combined.columns:\n",
    "            # Fallback: bootstrap sample from real data\n",
    "            synthetic_combined[col] = np.random.choice(\n",
    "                X_real[col].values, n_samples, replace=True\n",
    "            )\n",
    "    \n",
    "    # Reorder columns to match real data\n",
    "    synthetic_combined = synthetic_combined[[c for c in X_real.columns if c in synthetic_combined.columns]]\n",
    "    \n",
    "    # Correlation alignment\n",
    "    # Focus on the most important correlated feature groups\n",
    "    corr_groups = [\n",
    "        ['announcements', 'withdrawals', 'nlri_ann', 'origin_0', 'origin_2'],\n",
    "        ['as_path_max', 'unique_as_path_max', 'edit_distance_max', 'edit_distance_avg'],\n",
    "        ['imp_wd', 'imp_wd_spath', 'imp_wd_dpath'],\n",
    "    ]\n",
    "    \n",
    "    for group in corr_groups:\n",
    "        available_group = [c for c in group if c in synthetic_combined.columns and c in X_real.columns]\n",
    "        if len(available_group) >= 2:\n",
    "            synthetic_combined = align_correlations(synthetic_combined, X_real, available_group)\n",
    "    \n",
    "    print(f\"    Aligned correlations for {sum(len(g) for g in corr_groups)} features\")\n",
    "    \n",
    "    # Step 6: Enforce BGP constraints\n",
    "    print(\"\\n[6/6] Enforcing BGP domain constraints...\")\n",
    "    synthetic_final = enforce_bgp_constraints(synthetic_combined, X_real)\n",
    "    print(f\"    Constraints enforced on {len(synthetic_final.columns)} features\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"GENERATION COMPLETE: {len(synthetic_final)} samples, {len(synthetic_final.columns)} features\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return synthetic_final\n",
    "\n",
    "print(\"Hybrid generation pipeline defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data using hybrid approach\n",
    "synthetic_hybrid = generate_hybrid(\n",
    "    X_real, \n",
    "    N_SYNTHETIC, \n",
    "    FEATURE_ASSIGNMENT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated shape: {synthetic_hybrid.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "synthetic_hybrid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_quality(real, synthetic, feature_importance=None):\n",
    "    \"\"\"\n",
    "    Comprehensive quality evaluation.\n",
    "    \n",
    "    Returns dict with:\n",
    "    - Per-feature KS statistics\n",
    "    - Cohen's d effect sizes\n",
    "    - Correlation matrix similarity\n",
    "    - Overall weighted score\n",
    "    \"\"\"\n",
    "    common_cols = [c for c in real.columns if c in synthetic.columns]\n",
    "    \n",
    "    results = {\n",
    "        'ks_stats': {},\n",
    "        'cohens_d': {},\n",
    "        'wasserstein': {}\n",
    "    }\n",
    "    \n",
    "    for col in common_cols:\n",
    "        real_vals = real[col].dropna().values\n",
    "        syn_vals = synthetic[col].dropna().values\n",
    "        \n",
    "        # KS statistic\n",
    "        ks_stat, _ = ks_2samp(real_vals, syn_vals)\n",
    "        results['ks_stats'][col] = ks_stat\n",
    "        \n",
    "        # Cohen's d\n",
    "        pooled_std = np.sqrt((real_vals.std()**2 + syn_vals.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            cohens_d = (real_vals.mean() - syn_vals.mean()) / pooled_std\n",
    "        else:\n",
    "            cohens_d = 0\n",
    "        results['cohens_d'][col] = cohens_d\n",
    "        \n",
    "        # Wasserstein distance (normalized)\n",
    "        real_norm = (real_vals - real_vals.min()) / (real_vals.max() - real_vals.min() + 1e-10)\n",
    "        syn_norm = (syn_vals - syn_vals.min()) / (syn_vals.max() - syn_vals.min() + 1e-10)\n",
    "        wasserstein = stats.wasserstein_distance(real_norm, syn_norm)\n",
    "        results['wasserstein'][col] = wasserstein\n",
    "    \n",
    "    # Correlation similarity\n",
    "    real_corr = real[common_cols].corr()\n",
    "    syn_corr = synthetic[common_cols].corr()\n",
    "    corr_similarity = np.corrcoef(real_corr.values.flatten(), syn_corr.values.flatten())[0, 1]\n",
    "    results['correlation_similarity'] = corr_similarity\n",
    "    \n",
    "    # Summary statistics\n",
    "    results['mean_ks'] = np.mean(list(results['ks_stats'].values()))\n",
    "    results['mean_cohens_d'] = np.mean(np.abs(list(results['cohens_d'].values())))\n",
    "    results['mean_wasserstein'] = np.mean(list(results['wasserstein'].values()))\n",
    "    \n",
    "    # Calculate scores (0-100 scale)\n",
    "    # KS score: 0 means identical, we want lower\n",
    "    ks_score = max(0, 100 * (1 - results['mean_ks'] * 2))  # KS < 0.5 gives positive score\n",
    "    \n",
    "    # Effect size score: Cohen's d < 0.2 is negligible\n",
    "    effect_score = max(0, 100 * (1 - results['mean_cohens_d'] / 2))\n",
    "    \n",
    "    # Correlation score\n",
    "    corr_score = max(0, results['correlation_similarity'] * 100)\n",
    "    \n",
    "    # Wasserstein score\n",
    "    wass_score = max(0, 100 * (1 - results['mean_wasserstein'] * 2))\n",
    "    \n",
    "    # Overall weighted score\n",
    "    results['component_scores'] = {\n",
    "        'distribution_ks': ks_score,\n",
    "        'effect_size': effect_score,\n",
    "        'correlation': corr_score,\n",
    "        'wasserstein': wass_score\n",
    "    }\n",
    "    \n",
    "    weights = {'distribution_ks': 0.25, 'effect_size': 0.25, 'correlation': 0.25, 'wasserstein': 0.25}\n",
    "    results['overall_score'] = sum(\n",
    "        results['component_scores'][k] * weights[k] \n",
    "        for k in weights\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_results = evaluate_quality(X_real, synthetic_hybrid)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID APPROACH EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nOVERALL SCORE: {eval_results['overall_score']:.1f}/100\")\n",
    "\n",
    "print(f\"\\nComponent Scores:\")\n",
    "for component, score in eval_results['component_scores'].items():\n",
    "    print(f\"  {component}: {score:.1f}\")\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"  Mean KS Statistic: {eval_results['mean_ks']:.4f}\")\n",
    "print(f\"  Mean |Cohen's d|: {eval_results['mean_cohens_d']:.4f}\")\n",
    "print(f\"  Correlation Similarity: {eval_results['correlation_similarity']:.4f}\")\n",
    "print(f\"  Mean Wasserstein: {eval_results['mean_wasserstein']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst features analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP-10 WORST FEATURES (by KS statistic)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ks_sorted = sorted(eval_results['ks_stats'].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (feature, ks) in enumerate(ks_sorted[:10], 1):\n",
    "    cohens_d = eval_results['cohens_d'][feature]\n",
    "    wass = eval_results['wasserstein'][feature]\n",
    "    print(f\"{i:2}. {feature:30} KS={ks:.4f}  d={cohens_d:+.3f}  W={wass:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Distribution comparison for worst features\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "worst_10 = [f for f, _ in ks_sorted[:10]]\n",
    "\n",
    "for idx, feature in enumerate(worst_10):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    real_vals = X_real[feature].values\n",
    "    syn_vals = synthetic_hybrid[feature].values\n",
    "    \n",
    "    # Use log scale for heavy-tailed\n",
    "    if real_vals.max() > 100:\n",
    "        real_vals = np.log1p(real_vals)\n",
    "        syn_vals = np.log1p(syn_vals)\n",
    "        ax.set_xlabel('log1p(value)')\n",
    "    \n",
    "    ax.hist(real_vals, bins=50, alpha=0.5, label='Real', density=True)\n",
    "    ax.hist(syn_vals, bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "    ax.set_title(f'{feature}\\nKS={eval_results[\"ks_stats\"][feature]:.3f}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution Comparison: Top-10 Worst Features', y=1.02, fontsize=14)\n",
    "plt.savefig('hybrid_worst_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "common_cols = [c for c in X_real.columns if c in synthetic_hybrid.columns]\n",
    "\n",
    "real_corr = X_real[common_cols].corr()\n",
    "syn_corr = synthetic_hybrid[common_cols].corr()\n",
    "diff_corr = real_corr - syn_corr\n",
    "\n",
    "sns.heatmap(real_corr, ax=axes[0], cmap='coolwarm', center=0, \n",
    "            xticklabels=False, yticklabels=False)\n",
    "axes[0].set_title('Real Data Correlations')\n",
    "\n",
    "sns.heatmap(syn_corr, ax=axes[1], cmap='coolwarm', center=0,\n",
    "            xticklabels=False, yticklabels=False)\n",
    "axes[1].set_title('Synthetic Data Correlations')\n",
    "\n",
    "sns.heatmap(diff_corr, ax=axes[2], cmap='RdBu', center=0,\n",
    "            xticklabels=False, yticklabels=False, vmin=-0.5, vmax=0.5)\n",
    "axes[2].set_title(f'Difference (Similarity: {eval_results[\"correlation_similarity\"]:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synthetic data\n",
    "output_path = '../data/synthetic_hybrid_normal.csv'\n",
    "synthetic_hybrid.to_csv(output_path, index=False)\n",
    "print(f\"Saved synthetic data to {output_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "import json\n",
    "\n",
    "eval_output = {\n",
    "    'overall_score': eval_results['overall_score'],\n",
    "    'component_scores': eval_results['component_scores'],\n",
    "    'mean_ks': eval_results['mean_ks'],\n",
    "    'mean_cohens_d': eval_results['mean_cohens_d'],\n",
    "    'correlation_similarity': eval_results['correlation_similarity'],\n",
    "    'per_feature_ks': eval_results['ks_stats'],\n",
    "    'generation_config': {\n",
    "        'n_samples': N_SYNTHETIC,\n",
    "        'n_clusters': N_CLUSTERS,\n",
    "        'feature_assignment': {k: v for k, v in FEATURE_ASSIGNMENT.items()}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../data/hybrid_evaluation_results.json', 'w') as f:\n",
    "    json.dump(eval_output, f, indent=2)\n",
    "\n",
    "print(\"Evaluation results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous results\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON WITH PREVIOUS APPROACHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "previous_results = {\n",
    "    'Scapy (Packet-level)': 19.8,\n",
    "    'TimeGAN (Default)': 29.8,\n",
    "    'SMOTE-KMeans': 34.0,\n",
    "    'DoppelGANger (Enhanced)': 34.9,\n",
    "    'HYBRID (This approach)': eval_results['overall_score']\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<30} {'Score':>10}\")\n",
    "print(\"-\"*42)\n",
    "for method, score in sorted(previous_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    marker = ' <-- NEW' if 'HYBRID' in method else ''\n",
    "    print(f\"{method:<30} {score:>10.1f}{marker}\")\n",
    "\n",
    "best_previous = max(v for k, v in previous_results.items() if 'HYBRID' not in k)\n",
    "improvement = eval_results['overall_score'] - best_previous\n",
    "print(f\"\\nImprovement over best previous: {improvement:+.1f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Next Steps & Tuning\n",
    "\n",
    "### If results are not satisfactory, try:\n",
    "\n",
    "1. **Adjust feature assignment**: Move problematic features between strategies\n",
    "2. **Tune KDE bandwidth**: Lower bandwidth_factor (0.3-0.5) for tighter distributions\n",
    "3. **Increase clusters**: More clusters in SMOTE-KMeans for heterogeneous data\n",
    "4. **Add DoppelGANger**: For temporal sequences, generate with DoppelGANger first, then post-process\n",
    "\n",
    "### Alternative: DoppelGANger + Post-Processing Pipeline\n",
    "\n",
    "```python\n",
    "# 1. Generate base with DoppelGANger\n",
    "synthetic_gan = doppelganger_generate(X_real, n_samples)\n",
    "\n",
    "# 2. Replace worst features with SMOTE/KDE\n",
    "for feature in worst_features:\n",
    "    synthetic_gan[feature] = generate_empirical_kde(X_real, [feature], n_samples)[feature]\n",
    "\n",
    "# 3. Re-align correlations\n",
    "synthetic_final = align_correlations(synthetic_gan, X_real)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
