{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Hybrid SMOTE-GAN Synthetic Data Generation\n",
    "\n",
    "## Approach: Best of Both Worlds\n",
    "\n",
    "This notebook implements a hybrid approach that combines:\n",
    "- **SMOTE-KMeans**: Fast, excellent correlation preservation (89.5%)\n",
    "- **DoppelGANger**: Complex temporal patterns, novel generation\n",
    "- **Conditional Sampling**: Domain-constrained feature generation\n",
    "- **Mixture Models**: Sparse event-driven features\n",
    "\n",
    "### Rationale\n",
    "- SMOTE-KMeans achieved 34.0/100 with best correlation (89.5%)\n",
    "- DoppelGANger achieved 34.9/100 but slower training\n",
    "- Both struggle with same features: unique_as_path_max, edit_distance_*, flaps\n",
    "- Hybrid approach targets weaknesses with specialized generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, gaussian_kde\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# FEATURE ASSIGNMENT STRATEGY\n# =============================================================================\n\nFEATURE_ASSIGNMENT = {\n    # SMOTE-KMeans: Features with good correlation preservation needed\n    # These are \"static\" features that don't need complex temporal modeling\n    'smote_kmeans': [\n        'announcements',        # Base volume - SMOTE handles well\n        'withdrawals',          # Base volume\n        'nlri_ann',             # Correlated with announcements\n        'dups',                 # Simple count\n        'origin_0',             # Categorical-like\n        'origin_2',             # Categorical-like\n        'origin_changes',       # Low complexity\n        'as_path_max',          # SMOTE preserves distribution\n        'imp_wd_spath',         # Correlated feature\n        'imp_wd_dpath',         # Correlated feature\n    ],\n    \n    # Empirical Sampling: Heavy-tailed features - sample from KDE\n    'empirical_kde': [\n        'unique_as_path_max',   # Worst feature - use empirical\n        'edit_distance_max',    # Heavy-tailed\n        'edit_distance_avg',    # Continuous\n        'rare_ases_avg',        # Heavy-tailed, Zipf-like\n    ],\n    \n    # Conditional Generation: Derived from other features\n    'conditional': [\n        'edit_distance_dict_0', \n        'edit_distance_dict_1',\n        'edit_distance_dict_2', \n        'edit_distance_dict_3',\n        'edit_distance_dict_4',\n        'edit_distance_dict_5',\n        'edit_distance_dict_6',\n        'edit_distance_unique_dict_0',\n        'edit_distance_unique_dict_1',\n    ],\n    \n    # Zero-Inflated Mixture: Sparse event-driven features\n    'mixture_model': [\n        'flaps',                # Sparse, event-driven\n        'nadas',                # Sparse, event-driven  \n        'imp_wd',               # Can be sparse\n        'number_rare_ases',     # Integer count, sparse\n    ]\n}\n\n# All 27 features (using underscore convention: origin_0, nlri_ann)\nALL_FEATURES = [\n    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n    'origin_0', 'origin_2', 'origin_changes',\n    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n    'as_path_max', 'unique_as_path_max',\n    'edit_distance_avg', 'edit_distance_max',\n    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n    'edit_distance_dict_6',\n    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n    'number_rare_ases', 'rare_ases_avg',\n    'nadas', 'flaps'\n]\n\n# Integer features (must be rounded) - CRITICAL: must be enforced AFTER all processing\nINTEGER_FEATURES = [\n    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n    'origin_0', 'origin_2', 'origin_changes',\n    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n    'as_path_max', 'unique_as_path_max',\n    'edit_distance_max',\n    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n    'edit_distance_dict_6',\n    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n    'number_rare_ases', 'nadas', 'flaps'\n]\n\n# =============================================================================\n# FEATURE NAME MAPPING (for GAN compatibility)\n# =============================================================================\n# GAN notebooks may use different naming conventions (no underscores)\n# This mapping allows seamless integration\n\nFEATURE_NAME_MAPPING = {\n    # GAN format (lowercase, no underscores) -> Standard format (with underscores)\n    'nlriann': 'nlri_ann',\n    'origin0': 'origin_0',\n    'origin2': 'origin_2',\n    'impwd': 'imp_wd',\n    'impwdspath': 'imp_wd_spath',\n    'impwddpath': 'imp_wd_dpath',\n    'aspathmax': 'as_path_max',\n    'uniqueaspathmax': 'unique_as_path_max',\n    'editdistanceavg': 'edit_distance_avg',\n    'editdistancemax': 'edit_distance_max',\n    'editdistancedict0': 'edit_distance_dict_0',\n    'editdistancedict1': 'edit_distance_dict_1',\n    'editdistancedict2': 'edit_distance_dict_2',\n    'editdistancedict3': 'edit_distance_dict_3',\n    'editdistancedict4': 'edit_distance_dict_4',\n    'editdistancedict5': 'edit_distance_dict_5',\n    'editdistancedict6': 'edit_distance_dict_6',\n    'editdistanceuniquedict0': 'edit_distance_unique_dict_0',\n    'editdistanceuniquedict1': 'edit_distance_unique_dict_1',\n    'numberrareases': 'number_rare_ases',\n    'rareasesavg': 'rare_ases_avg',  # Fixed typo: was 'rareasesvg'\n}\n\n# Reverse mapping (Standard -> GAN format)\nFEATURE_NAME_MAPPING_REVERSE = {v: k for k, v in FEATURE_NAME_MAPPING.items()}\n\ndef standardize_column_names(df):\n    \"\"\"Convert GAN-style column names to standard underscore format.\"\"\"\n    rename_dict = {}\n    for col in df.columns:\n        col_lower = col.lower().replace('_', '')\n        if col_lower in FEATURE_NAME_MAPPING:\n            rename_dict[col] = FEATURE_NAME_MAPPING[col_lower]\n        elif col.lower() in FEATURE_NAME_MAPPING:\n            rename_dict[col] = FEATURE_NAME_MAPPING[col.lower()]\n    \n    if rename_dict:\n        print(f\"Standardized {len(rename_dict)} column names: {list(rename_dict.keys())}\")\n        df = df.rename(columns=rename_dict)\n    return df\n\n# Generation settings\nN_SYNTHETIC = 20000  # Number of samples to generate\nN_CLUSTERS = 15      # For KMeans-SMOTE\nRANDOM_STATE = 42\n\nprint(\"Configuration loaded!\")\nprint(f\"\\nFeature assignment:\")\nfor strategy, features in FEATURE_ASSIGNMENT.items():\n    print(f\"  {strategy}: {len(features)} features\")\nprint(f\"\\nFeature name mapping defined for {len(FEATURE_NAME_MAPPING)} GAN-style names\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your real BGP data\n",
    "# Adjust path as needed\n",
    "DATA_PATH = '../data/likely_normal_traffic.csv'  # or your data path\n",
    "\n",
    "try:\n",
    "    df_real = pd.read_csv(DATA_PATH)\n",
    "    print(f\"Loaded {len(df_real)} samples\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH to point to your real BGP data\")\n",
    "    # Create dummy data for demonstration\n",
    "    print(\"\\nCreating dummy data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    df_real = pd.DataFrame({\n",
    "        col: np.random.exponential(scale=10, size=10000) \n",
    "        for col in ALL_FEATURES\n",
    "    })\n",
    "\n",
    "# Filter to only the features we need\n",
    "available_features = [f for f in ALL_FEATURES if f in df_real.columns]\n",
    "X_real = df_real[available_features].copy()\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} features\")\n",
    "print(f\"Real data shape: {X_real.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generator Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SMOTE-KMeans Generator (for correlated features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_smote_kmeans(X, features, n_samples, n_clusters=15, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate samples using KMeans clustering + SMOTE.\n",
    "    Best for features requiring correlation preservation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame - Real data\n",
    "    features : list - Features to generate\n",
    "    n_samples : int - Number of samples to generate\n",
    "    n_clusters : int - Number of KMeans clusters\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with generated features\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Filter to available features\n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    X_subset = X[available].values\n",
    "    \n",
    "    # Apply log1p transform for stability\n",
    "    X_log = np.log1p(np.clip(X_subset, 0, None))\n",
    "    \n",
    "    # Scale for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_log)\n",
    "    \n",
    "    # KMeans clustering\n",
    "    n_clusters_actual = min(n_clusters, len(X_subset) // 10)\n",
    "    kmeans = KMeans(n_clusters=n_clusters_actual, random_state=random_state, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Allocate samples per cluster proportionally\n",
    "    cluster_sizes = np.bincount(cluster_labels)\n",
    "    samples_per_cluster = (cluster_sizes / cluster_sizes.sum() * n_samples).astype(int)\n",
    "    samples_per_cluster = np.maximum(samples_per_cluster, 1)\n",
    "    \n",
    "    synthetic_all = []\n",
    "    \n",
    "    for cluster_id in range(n_clusters_actual):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        X_cluster = X_log[cluster_mask]\n",
    "        \n",
    "        if len(X_cluster) < 3:\n",
    "            continue\n",
    "        \n",
    "        n_to_generate = samples_per_cluster[cluster_id]\n",
    "        minority_size = max(2, int(len(X_cluster) * 0.1))\n",
    "        safe_k = min(3, minority_size - 1)\n",
    "        \n",
    "        if safe_k < 1:\n",
    "            continue\n",
    "        \n",
    "        # Create artificial minority class for SMOTE\n",
    "        minority_idx = np.random.choice(len(X_cluster), minority_size, replace=False)\n",
    "        y_cluster = np.zeros(len(X_cluster))\n",
    "        y_cluster[minority_idx] = 1\n",
    "        \n",
    "        try:\n",
    "            smote = SMOTE(\n",
    "                sampling_strategy={1: n_to_generate + minority_size},\n",
    "                k_neighbors=safe_k,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            X_res, y_res = smote.fit_resample(X_cluster, y_cluster)\n",
    "            synthetic = X_res[y_res == 1][minority_size:]\n",
    "            synthetic_all.append(synthetic)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    if synthetic_all:\n",
    "        result = np.vstack(synthetic_all)\n",
    "        # Inverse log1p transform\n",
    "        result = np.expm1(result)\n",
    "        result = np.clip(result, 0, None)\n",
    "        return pd.DataFrame(result[:n_samples], columns=available)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=available)\n",
    "\n",
    "print(\"SMOTE-KMeans generator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Empirical KDE Generator (for heavy-tailed features)\n# =============================================================================\n\ndef generate_empirical_kde(X, features, n_samples, bandwidth_factor=0.5, random_state=42):\n    \"\"\"\n    Generate samples using Kernel Density Estimation on real data.\n    Best for heavy-tailed features that GANs struggle with.\n    \n    Uses adaptive bandwidth based on feature variance.\n    \n    Edge cases handled:\n    - Constant features (std=0): bootstrap sample from unique value\n    - Very low variance: bootstrap sampling fallback\n    - KDE failures: bootstrap sampling fallback\n    \"\"\"\n    np.random.seed(random_state)\n    \n    available = [f for f in features if f in X.columns]\n    if not available:\n        return pd.DataFrame()\n    \n    result = {}\n    fallback_features = []  # Track features using fallback\n    \n    for feature in available:\n        real_values = X[feature].values\n        real_values = real_values[~np.isnan(real_values)]\n        \n        # Edge case: too few values\n        if len(real_values) < 10:\n            result[feature] = np.zeros(n_samples)\n            fallback_features.append((feature, 'too_few_values'))\n            continue\n        \n        # Edge case: constant or near-constant feature (std â‰ˆ 0)\n        feature_std = np.std(real_values)\n        if feature_std < 1e-10:\n            # Bootstrap sample from the constant value(s)\n            result[feature] = np.random.choice(real_values, n_samples, replace=True)\n            fallback_features.append((feature, 'constant'))\n            continue\n        \n        # Edge case: very low variance relative to mean (coefficient of variation)\n        cv = feature_std / (np.abs(np.mean(real_values)) + 1e-10)\n        if cv < 0.01:  # Very low relative variance\n            result[feature] = np.random.choice(real_values, n_samples, replace=True)\n            fallback_features.append((feature, 'low_variance'))\n            continue\n        \n        # Log transform for heavy-tailed distributions\n        log_values = np.log1p(np.clip(real_values, 0, None))\n        \n        # Check if log-transformed values have sufficient variance\n        if np.std(log_values) < 1e-10:\n            result[feature] = np.random.choice(real_values, n_samples, replace=True)\n            fallback_features.append((feature, 'low_log_variance'))\n            continue\n        \n        try:\n            # Fit KDE with Scott's rule bandwidth * factor\n            kde = gaussian_kde(log_values, bw_method='scott')\n            kde.set_bandwidth(kde.factor * bandwidth_factor)\n            \n            # Sample from KDE\n            synthetic_log = kde.resample(n_samples).flatten()\n            \n            # Inverse transform\n            synthetic = np.expm1(synthetic_log)\n            synthetic = np.clip(synthetic, 0, np.percentile(real_values, 99.9))\n            \n            result[feature] = synthetic\n            \n        except Exception as e:\n            # Fallback to bootstrap sampling\n            result[feature] = np.random.choice(real_values, n_samples, replace=True)\n            fallback_features.append((feature, f'kde_error: {str(e)[:30]}'))\n    \n    # Report fallback usage\n    if fallback_features:\n        print(f\"  KDE fallback to bootstrap for {len(fallback_features)} features:\")\n        for feat, reason in fallback_features:\n            print(f\"    - {feat}: {reason}\")\n    \n    return pd.DataFrame(result)\n\nprint(\"Empirical KDE generator defined (with edge case handling)!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Zero-Inflated Mixture Generator (for sparse features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_mixture_model(X, features, n_samples, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate sparse features using zero-inflated mixture model.\n",
    "    \n",
    "    Model: P(x) = p_zero * I(x=0) + (1-p_zero) * f(x|x>0)\n",
    "    \n",
    "    Best for features like 'flaps', 'nadas' that are often zero.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for feature in available:\n",
    "        real_values = X[feature].values\n",
    "        real_values = real_values[~np.isnan(real_values)]\n",
    "        \n",
    "        # Calculate zero probability\n",
    "        p_zero = (real_values == 0).mean()\n",
    "        \n",
    "        # Get non-zero values\n",
    "        non_zero = real_values[real_values > 0]\n",
    "        \n",
    "        synthetic = np.zeros(n_samples)\n",
    "        \n",
    "        # Determine which samples are non-zero\n",
    "        non_zero_mask = np.random.random(n_samples) > p_zero\n",
    "        n_non_zero = non_zero_mask.sum()\n",
    "        \n",
    "        if n_non_zero > 0 and len(non_zero) > 0:\n",
    "            # Sample from non-zero distribution with small noise\n",
    "            sampled = np.random.choice(non_zero, n_non_zero, replace=True)\n",
    "            # Add small perturbation\n",
    "            noise = np.random.normal(0, non_zero.std() * 0.1, n_non_zero)\n",
    "            synthetic[non_zero_mask] = np.maximum(1, sampled + noise)\n",
    "        \n",
    "        result[feature] = synthetic\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "print(\"Mixture model generator defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Conditional Generator (for edit_distance_dict features)\n",
    "# =============================================================================\n",
    "\n",
    "def generate_conditional(X, features, n_samples, synthetic_base, random_state=42):\n",
    "    \"\"\"\n",
    "    Generate edit_distance_dict features conditioned on:\n",
    "    - announcements (volume)\n",
    "    - edit_distance_max (upper bound)\n",
    "    \n",
    "    The dict values should follow a distribution where:\n",
    "    - Lower edit distances (0-2) are more common\n",
    "    - Higher edit distances (3-6) are rare\n",
    "    - Sum relates to announcement volume\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    available = [f for f in features if f in X.columns]\n",
    "    if not available:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Learn the conditional distribution from real data\n",
    "    # P(edit_distance_dict_i | announcements, edit_distance_max)\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Get conditioning variables from synthetic_base if available\n",
    "    if 'announcements' in synthetic_base.columns:\n",
    "        syn_announcements = synthetic_base['announcements'].values\n",
    "    else:\n",
    "        syn_announcements = np.random.choice(X['announcements'].values, n_samples)\n",
    "    \n",
    "    if 'edit_distance_max' in synthetic_base.columns:\n",
    "        syn_ed_max = synthetic_base['edit_distance_max'].values.astype(int)\n",
    "    else:\n",
    "        syn_ed_max = np.random.choice(X['edit_distance_max'].values, n_samples).astype(int)\n",
    "    \n",
    "    # Learn typical distribution shape from real data\n",
    "    ed_dict_cols = [f'edit_distance_dict_{i}' for i in range(7) if f'edit_distance_dict_{i}' in X.columns]\n",
    "    \n",
    "    if ed_dict_cols:\n",
    "        # Calculate average proportions\n",
    "        real_ed_dict = X[ed_dict_cols].values\n",
    "        row_sums = real_ed_dict.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "        proportions = (real_ed_dict / row_sums).mean(axis=0)\n",
    "        \n",
    "        # Generate for each sample\n",
    "        for i, col in enumerate(ed_dict_cols):\n",
    "            # Scale by announcements (more announcements = more edit events)\n",
    "            scale_factor = np.log1p(syn_announcements) / np.log1p(X['announcements'].mean())\n",
    "            base_value = X[col].mean() * scale_factor\n",
    "            \n",
    "            # Add noise\n",
    "            noise = np.random.normal(0, X[col].std() * 0.3, n_samples)\n",
    "            synthetic_col = np.maximum(0, base_value + noise)\n",
    "            \n",
    "            # Zero out values beyond edit_distance_max\n",
    "            if i > 0:  # dict_1 and above\n",
    "                synthetic_col[syn_ed_max < i] = 0\n",
    "            \n",
    "            result[col] = synthetic_col\n",
    "    \n",
    "    # Handle edit_distance_unique_dict features similarly\n",
    "    ed_unique_cols = [f'edit_distance_unique_dict_{i}' for i in range(2) \n",
    "                     if f'edit_distance_unique_dict_{i}' in X.columns]\n",
    "    \n",
    "    for col in ed_unique_cols:\n",
    "        real_values = X[col].values\n",
    "        scale_factor = np.log1p(syn_announcements) / np.log1p(X['announcements'].mean())\n",
    "        base_value = real_values.mean() * scale_factor\n",
    "        noise = np.random.normal(0, real_values.std() * 0.3, n_samples)\n",
    "        result[col] = np.maximum(0, base_value + noise)\n",
    "    \n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "print(\"Conditional generator defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlation Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Correlation Alignment via Cholesky Decomposition\n",
    "# =============================================================================\n",
    "\n",
    "def align_correlations(synthetic, real, features_to_align=None):\n",
    "    \"\"\"\n",
    "    Adjust synthetic data to match real correlation structure.\n",
    "    \n",
    "    Uses Cholesky decomposition to impose correlation structure:\n",
    "    1. Decorrelate synthetic data\n",
    "    2. Re-correlate with real data's correlation matrix\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    synthetic : DataFrame - Generated data\n",
    "    real : DataFrame - Real data\n",
    "    features_to_align : list - Features to align (default: all common)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with aligned correlations\n",
    "    \"\"\"\n",
    "    if features_to_align is None:\n",
    "        features_to_align = [c for c in synthetic.columns if c in real.columns]\n",
    "    \n",
    "    if len(features_to_align) < 2:\n",
    "        return synthetic\n",
    "    \n",
    "    try:\n",
    "        # Get correlation matrices\n",
    "        real_subset = real[features_to_align]\n",
    "        syn_subset = synthetic[features_to_align].copy()\n",
    "        \n",
    "        real_corr = real_subset.corr().values\n",
    "        syn_corr = syn_subset.corr().values\n",
    "        \n",
    "        # Add small regularization for numerical stability\n",
    "        eps = 1e-6\n",
    "        real_corr = real_corr + eps * np.eye(len(features_to_align))\n",
    "        syn_corr = syn_corr + eps * np.eye(len(features_to_align))\n",
    "        \n",
    "        # Cholesky decomposition\n",
    "        L_real = np.linalg.cholesky(real_corr)\n",
    "        L_syn = np.linalg.cholesky(syn_corr)\n",
    "        \n",
    "        # Standardize synthetic data\n",
    "        syn_mean = syn_subset.mean().values\n",
    "        syn_std = syn_subset.std().values\n",
    "        syn_std[syn_std == 0] = 1  # Avoid division by zero\n",
    "        \n",
    "        syn_standardized = (syn_subset.values - syn_mean) / syn_std\n",
    "        \n",
    "        # Decorrelate then re-correlate\n",
    "        syn_decorr = syn_standardized @ np.linalg.inv(L_syn.T)\n",
    "        syn_recorr = syn_decorr @ L_real.T\n",
    "        \n",
    "        # Rescale to match real data's scale\n",
    "        real_mean = real_subset.mean().values\n",
    "        real_std = real_subset.std().values\n",
    "        \n",
    "        aligned = syn_recorr * real_std + real_mean\n",
    "        \n",
    "        # Update synthetic DataFrame\n",
    "        result = synthetic.copy()\n",
    "        for i, col in enumerate(features_to_align):\n",
    "            result[col] = aligned[:, i]\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Correlation alignment failed: {e}\")\n",
    "        return synthetic\n",
    "\n",
    "print(\"Correlation alignment function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-Processing & Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# Post-Processing: Enforce BGP Domain Constraints\n# =============================================================================\n\ndef enforce_bgp_constraints(synthetic, real, verbose=True):\n    \"\"\"\n    Apply BGP domain-specific constraints to synthetic data.\n    \n    IMPORTANT: This function should be called as the FINAL step after all\n    generation and correlation alignment, as it ensures:\n    1. All features are non-negative\n    2. All INTEGER_FEATURES are properly rounded to integers\n    3. Domain constraints are satisfied\n    \n    Constraints:\n    1. All features non-negative (clipped to 0)\n    2. Integer features rounded to nearest integer\n    3. origin_0 + origin_2 <= announcements\n    4. edit_distance_dict_i = 0 when i > edit_distance_max\n    5. Values within realistic bounds (99.5th percentile)\n    6. FINAL: Re-clip and re-round all integer features\n    \"\"\"\n    result = synthetic.copy()\n    \n    # Track constraint violations for reporting\n    violations = {\n        'negative_values': 0,\n        'origin_constraint': 0,\n        'edit_distance_constraint': 0,\n        'bound_violations': 0\n    }\n    \n    # 1. Non-negative: Clip ALL features to >= 0\n    for col in result.columns:\n        neg_count = (result[col] < 0).sum()\n        if neg_count > 0:\n            violations['negative_values'] += neg_count\n            result[col] = np.maximum(0, result[col])\n    \n    # 2. Integer features: First pass rounding\n    for col in INTEGER_FEATURES:\n        if col in result.columns:\n            result[col] = np.round(result[col]).astype(int)\n    \n    # 3. Origin constraint: origin_0 + origin_2 <= announcements\n    origin_cols = ['origin_0', 'origin_2', 'announcements']\n    if all(c in result.columns for c in origin_cols):\n        origin_sum = result['origin_0'] + result['origin_2']\n        excess_mask = origin_sum > result['announcements']\n        violations['origin_constraint'] = excess_mask.sum()\n        \n        if excess_mask.any():\n            # Scale down origin values to fit within announcements\n            scale = result.loc[excess_mask, 'announcements'] / (origin_sum[excess_mask] + 1e-10)\n            result.loc[excess_mask, 'origin_0'] = np.floor(result.loc[excess_mask, 'origin_0'] * scale).astype(int)\n            result.loc[excess_mask, 'origin_2'] = np.floor(result.loc[excess_mask, 'origin_2'] * scale).astype(int)\n    \n    # 4. Edit distance max constraint: dict_i = 0 when i > edit_distance_max\n    ed_dict_cols = [f'edit_distance_dict_{i}' for i in range(7) if f'edit_distance_dict_{i}' in result.columns]\n    if 'edit_distance_max' in result.columns and ed_dict_cols:\n        for idx in result.index:\n            ed_max = int(result.loc[idx, 'edit_distance_max'])\n            for i, col in enumerate(ed_dict_cols):\n                if i > ed_max and result.loc[idx, col] != 0:\n                    violations['edit_distance_constraint'] += 1\n                    result.loc[idx, col] = 0\n    \n    # 5. Realistic bounds: Clip to 99.5th percentile of real data\n    for col in result.columns:\n        if col in real.columns:\n            upper_bound = np.percentile(real[col], 99.5)\n            bound_violations = (result[col] > upper_bound * 1.1).sum()\n            if bound_violations > 0:\n                violations['bound_violations'] += bound_violations\n                result[col] = np.clip(result[col], 0, upper_bound * 1.1)\n    \n    # 6. FINAL STEP: Re-ensure all integer features are properly typed\n    # This is critical after all other operations that might introduce floats\n    for col in INTEGER_FEATURES:\n        if col in result.columns:\n            # Clip to non-negative first\n            result[col] = np.maximum(0, result[col])\n            # Round to nearest integer\n            result[col] = np.round(result[col]).astype(int)\n    \n    # Report violations if verbose\n    if verbose:\n        total_violations = sum(violations.values())\n        if total_violations > 0:\n            print(f\"  Constraint violations fixed:\")\n            for constraint, count in violations.items():\n                if count > 0:\n                    print(f\"    - {constraint}: {count}\")\n        else:\n            print(\"  No constraint violations detected\")\n    \n    return result\n\n# Standalone function for final integer enforcement (can be called separately)\ndef final_integer_enforcement(df):\n    \"\"\"\n    Ensure all INTEGER_FEATURES are non-negative integers.\n    Call this as the absolute final step before saving/returning data.\n    \"\"\"\n    result = df.copy()\n    for col in INTEGER_FEATURES:\n        if col in result.columns:\n            result[col] = np.maximum(0, result[col])\n            result[col] = np.round(result[col]).astype(int)\n    return result\n\nprint(\"BGP constraints function defined (with final integer enforcement)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Main Hybrid Generation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYBRID GENERATION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def generate_hybrid(X_real, n_samples, feature_assignment, random_state=42):\n",
    "    \"\"\"\n",
    "    Main hybrid generation pipeline combining multiple strategies.\n",
    "    \n",
    "    Pipeline:\n",
    "    1. SMOTE-KMeans for correlated features\n",
    "    2. Empirical KDE for heavy-tailed features\n",
    "    3. Mixture model for sparse features\n",
    "    4. Conditional generation for dependent features\n",
    "    5. Correlation alignment\n",
    "    6. BGP constraint enforcement\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"HYBRID SMOTE-GAN GENERATION PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    synthetic_parts = {}\n",
    "    \n",
    "    # Step 1: SMOTE-KMeans for base features\n",
    "    print(\"\\n[1/6] Generating SMOTE-KMeans features...\")\n",
    "    smote_features = feature_assignment.get('smote_kmeans', [])\n",
    "    if smote_features:\n",
    "        synthetic_parts['smote'] = generate_smote_kmeans(\n",
    "            X_real, smote_features, n_samples, \n",
    "            n_clusters=N_CLUSTERS, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['smote'].columns)} features via SMOTE-KMeans\")\n",
    "    \n",
    "    # Step 2: Empirical KDE for heavy-tailed features\n",
    "    print(\"\\n[2/6] Generating Empirical KDE features...\")\n",
    "    kde_features = feature_assignment.get('empirical_kde', [])\n",
    "    if kde_features:\n",
    "        synthetic_parts['kde'] = generate_empirical_kde(\n",
    "            X_real, kde_features, n_samples, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['kde'].columns)} features via KDE\")\n",
    "    \n",
    "    # Step 3: Mixture model for sparse features\n",
    "    print(\"\\n[3/6] Generating Mixture Model features...\")\n",
    "    mixture_features = feature_assignment.get('mixture_model', [])\n",
    "    if mixture_features:\n",
    "        synthetic_parts['mixture'] = generate_mixture_model(\n",
    "            X_real, mixture_features, n_samples, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['mixture'].columns)} features via Mixture\")\n",
    "    \n",
    "    # Combine parts so far for conditioning\n",
    "    synthetic_base = pd.concat(synthetic_parts.values(), axis=1)\n",
    "    \n",
    "    # Step 4: Conditional generation\n",
    "    print(\"\\n[4/6] Generating Conditional features...\")\n",
    "    conditional_features = feature_assignment.get('conditional', [])\n",
    "    if conditional_features:\n",
    "        synthetic_parts['conditional'] = generate_conditional(\n",
    "            X_real, conditional_features, n_samples, \n",
    "            synthetic_base, random_state=random_state\n",
    "        )\n",
    "        print(f\"    Generated {len(synthetic_parts['conditional'].columns)} features conditionally\")\n",
    "    \n",
    "    # Combine all parts\n",
    "    print(\"\\n[5/6] Combining and aligning correlations...\")\n",
    "    synthetic_combined = pd.concat(synthetic_parts.values(), axis=1)\n",
    "    \n",
    "    # Ensure we have all expected columns\n",
    "    for col in X_real.columns:\n",
    "        if col not in synthetic_combined.columns:\n",
    "            # Fallback: bootstrap sample from real data\n",
    "            synthetic_combined[col] = np.random.choice(\n",
    "                X_real[col].values, n_samples, replace=True\n",
    "            )\n",
    "    \n",
    "    # Reorder columns to match real data\n",
    "    synthetic_combined = synthetic_combined[[c for c in X_real.columns if c in synthetic_combined.columns]]\n",
    "    \n",
    "    # Correlation alignment\n",
    "    # Focus on the most important correlated feature groups\n",
    "    corr_groups = [\n",
    "        ['announcements', 'withdrawals', 'nlri_ann', 'origin_0', 'origin_2'],\n",
    "        ['as_path_max', 'unique_as_path_max', 'edit_distance_max', 'edit_distance_avg'],\n",
    "        ['imp_wd', 'imp_wd_spath', 'imp_wd_dpath'],\n",
    "    ]\n",
    "    \n",
    "    for group in corr_groups:\n",
    "        available_group = [c for c in group if c in synthetic_combined.columns and c in X_real.columns]\n",
    "        if len(available_group) >= 2:\n",
    "            synthetic_combined = align_correlations(synthetic_combined, X_real, available_group)\n",
    "    \n",
    "    print(f\"    Aligned correlations for {sum(len(g) for g in corr_groups)} features\")\n",
    "    \n",
    "    # Step 6: Enforce BGP constraints\n",
    "    print(\"\\n[6/6] Enforcing BGP domain constraints...\")\n",
    "    synthetic_final = enforce_bgp_constraints(synthetic_combined, X_real)\n",
    "    print(f\"    Constraints enforced on {len(synthetic_final.columns)} features\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"GENERATION COMPLETE: {len(synthetic_final)} samples, {len(synthetic_final.columns)} features\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return synthetic_final\n",
    "\n",
    "print(\"Hybrid generation pipeline defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data using hybrid approach\n",
    "synthetic_hybrid = generate_hybrid(\n",
    "    X_real, \n",
    "    N_SYNTHETIC, \n",
    "    FEATURE_ASSIGNMENT,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated shape: {synthetic_hybrid.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "synthetic_hybrid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# EVALUATION FUNCTIONS (Optimized)\n# =============================================================================\n\ndef evaluate_quality(real, synthetic, feature_importance=None, precomputed_real_corr=None):\n    \"\"\"\n    Comprehensive quality evaluation.\n    \n    Optimizations:\n    - Precompute real correlation matrix outside per-feature loop\n    - Accept precomputed_real_corr to avoid recomputation across multiple calls\n    \n    Returns dict with:\n    - Per-feature KS statistics\n    - Cohen's d effect sizes\n    - Correlation matrix similarity\n    - Overall weighted score\n    \"\"\"\n    common_cols = [c for c in real.columns if c in synthetic.columns]\n    \n    if not common_cols:\n        print(\"WARNING: No common columns between real and synthetic data!\")\n        print(f\"  Real columns: {list(real.columns)[:5]}...\")\n        print(f\"  Synthetic columns: {list(synthetic.columns)[:5]}...\")\n        return {'overall_score': 0, 'ks_stats': {}, 'cohens_d': {}, 'wasserstein': {}}\n    \n    results = {\n        'ks_stats': {},\n        'cohens_d': {},\n        'wasserstein': {},\n        'generation_method': {}  # Track which method generated each feature\n    }\n    \n    # Per-feature evaluation\n    for col in common_cols:\n        real_vals = real[col].dropna().values\n        syn_vals = synthetic[col].dropna().values\n        \n        if len(real_vals) == 0 or len(syn_vals) == 0:\n            results['ks_stats'][col] = 1.0\n            results['cohens_d'][col] = float('inf')\n            results['wasserstein'][col] = 1.0\n            continue\n        \n        # KS statistic\n        ks_stat, _ = ks_2samp(real_vals, syn_vals)\n        results['ks_stats'][col] = ks_stat\n        \n        # Cohen's d\n        pooled_std = np.sqrt((real_vals.std()**2 + syn_vals.std()**2) / 2)\n        if pooled_std > 0:\n            cohens_d = (real_vals.mean() - syn_vals.mean()) / pooled_std\n        else:\n            cohens_d = 0\n        results['cohens_d'][col] = cohens_d\n        \n        # Wasserstein distance (normalized)\n        real_range = real_vals.max() - real_vals.min()\n        syn_range = syn_vals.max() - syn_vals.min()\n        \n        if real_range > 0:\n            real_norm = (real_vals - real_vals.min()) / (real_range + 1e-10)\n        else:\n            real_norm = np.zeros_like(real_vals)\n            \n        if syn_range > 0:\n            syn_norm = (syn_vals - syn_vals.min()) / (syn_range + 1e-10)\n        else:\n            syn_norm = np.zeros_like(syn_vals)\n            \n        wasserstein = stats.wasserstein_distance(real_norm, syn_norm)\n        results['wasserstein'][col] = wasserstein\n    \n    # Correlation similarity (OPTIMIZED: compute once)\n    if precomputed_real_corr is not None:\n        real_corr = precomputed_real_corr\n    else:\n        real_corr = real[common_cols].corr()\n    \n    syn_corr = synthetic[common_cols].corr()\n    \n    # Handle NaN correlations\n    real_corr_flat = real_corr.values.flatten()\n    syn_corr_flat = syn_corr.values.flatten()\n    \n    # Remove NaN pairs\n    valid_mask = ~(np.isnan(real_corr_flat) | np.isnan(syn_corr_flat))\n    if valid_mask.sum() > 1:\n        corr_similarity = np.corrcoef(\n            real_corr_flat[valid_mask], \n            syn_corr_flat[valid_mask]\n        )[0, 1]\n    else:\n        corr_similarity = 0\n    \n    results['correlation_similarity'] = corr_similarity\n    results['real_correlation_matrix'] = real_corr  # Cache for reuse\n    \n    # Summary statistics\n    ks_values = list(results['ks_stats'].values())\n    cohens_values = [abs(v) for v in results['cohens_d'].values() if np.isfinite(v)]\n    wass_values = list(results['wasserstein'].values())\n    \n    results['mean_ks'] = np.mean(ks_values) if ks_values else 1.0\n    results['mean_cohens_d'] = np.mean(cohens_values) if cohens_values else float('inf')\n    results['mean_wasserstein'] = np.mean(wass_values) if wass_values else 1.0\n    \n    # Calculate scores (0-100 scale)\n    ks_score = max(0, 100 * (1 - results['mean_ks'] * 2))\n    effect_score = max(0, 100 * (1 - results['mean_cohens_d'] / 2))\n    corr_score = max(0, results['correlation_similarity'] * 100) if np.isfinite(results['correlation_similarity']) else 0\n    wass_score = max(0, 100 * (1 - results['mean_wasserstein'] * 2))\n    \n    results['component_scores'] = {\n        'distribution_ks': ks_score,\n        'effect_size': effect_score,\n        'correlation': corr_score,\n        'wasserstein': wass_score\n    }\n    \n    weights = {'distribution_ks': 0.25, 'effect_size': 0.25, 'correlation': 0.25, 'wasserstein': 0.25}\n    results['overall_score'] = sum(\n        results['component_scores'][k] * weights[k] \n        for k in weights\n    )\n    \n    # Report bootstrap/fallback features\n    results['n_common_features'] = len(common_cols)\n    results['n_total_real_features'] = len(real.columns)\n    results['n_total_synthetic_features'] = len(synthetic.columns)\n    \n    return results\n\nprint(\"Evaluation functions defined (optimized with correlation caching)!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_results = evaluate_quality(X_real, synthetic_hybrid)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HYBRID APPROACH EVALUATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nOVERALL SCORE: {eval_results['overall_score']:.1f}/100\")\n",
    "\n",
    "print(f\"\\nComponent Scores:\")\n",
    "for component, score in eval_results['component_scores'].items():\n",
    "    print(f\"  {component}: {score:.1f}\")\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"  Mean KS Statistic: {eval_results['mean_ks']:.4f}\")\n",
    "print(f\"  Mean |Cohen's d|: {eval_results['mean_cohens_d']:.4f}\")\n",
    "print(f\"  Correlation Similarity: {eval_results['correlation_similarity']:.4f}\")\n",
    "print(f\"  Mean Wasserstein: {eval_results['mean_wasserstein']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worst features analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TOP-10 WORST FEATURES (by KS statistic)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ks_sorted = sorted(eval_results['ks_stats'].items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (feature, ks) in enumerate(ks_sorted[:10], 1):\n",
    "    cohens_d = eval_results['cohens_d'][feature]\n",
    "    wass = eval_results['wasserstein'][feature]\n",
    "    print(f\"{i:2}. {feature:30} KS={ks:.4f}  d={cohens_d:+.3f}  W={wass:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Distribution comparison for worst features\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "worst_10 = [f for f, _ in ks_sorted[:10]]\n",
    "\n",
    "for idx, feature in enumerate(worst_10):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    real_vals = X_real[feature].values\n",
    "    syn_vals = synthetic_hybrid[feature].values\n",
    "    \n",
    "    # Use log scale for heavy-tailed\n",
    "    if real_vals.max() > 100:\n",
    "        real_vals = np.log1p(real_vals)\n",
    "        syn_vals = np.log1p(syn_vals)\n",
    "        ax.set_xlabel('log1p(value)')\n",
    "    \n",
    "    ax.hist(real_vals, bins=50, alpha=0.5, label='Real', density=True)\n",
    "    ax.hist(syn_vals, bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "    ax.set_title(f'{feature}\\nKS={eval_results[\"ks_stats\"][feature]:.3f}')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution Comparison: Top-10 Worst Features', y=1.02, fontsize=14)\n",
    "plt.savefig('hybrid_worst_features.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "common_cols = [c for c in X_real.columns if c in synthetic_hybrid.columns]\n",
    "\n",
    "real_corr = X_real[common_cols].corr()\n",
    "syn_corr = synthetic_hybrid[common_cols].corr()\n",
    "diff_corr = real_corr - syn_corr\n",
    "\n",
    "sns.heatmap(real_corr, ax=axes[0], cmap='coolwarm', center=0, \n",
    "            xticklabels=False, yticklabels=False)\n",
    "axes[0].set_title('Real Data Correlations')\n",
    "\n",
    "sns.heatmap(syn_corr, ax=axes[1], cmap='coolwarm', center=0,\n",
    "            xticklabels=False, yticklabels=False)\n",
    "axes[1].set_title('Synthetic Data Correlations')\n",
    "\n",
    "sns.heatmap(diff_corr, ax=axes[2], cmap='RdBu', center=0,\n",
    "            xticklabels=False, yticklabels=False, vmin=-0.5, vmax=0.5)\n",
    "axes[2].set_title(f'Difference (Similarity: {eval_results[\"correlation_similarity\"]:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synthetic data\n",
    "output_path = '../data/synthetic_hybrid_normal.csv'\n",
    "synthetic_hybrid.to_csv(output_path, index=False)\n",
    "print(f\"Saved synthetic data to {output_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "import json\n",
    "\n",
    "eval_output = {\n",
    "    'overall_score': eval_results['overall_score'],\n",
    "    'component_scores': eval_results['component_scores'],\n",
    "    'mean_ks': eval_results['mean_ks'],\n",
    "    'mean_cohens_d': eval_results['mean_cohens_d'],\n",
    "    'correlation_similarity': eval_results['correlation_similarity'],\n",
    "    'per_feature_ks': eval_results['ks_stats'],\n",
    "    'generation_config': {\n",
    "        'n_samples': N_SYNTHETIC,\n",
    "        'n_clusters': N_CLUSTERS,\n",
    "        'feature_assignment': {k: v for k, v in FEATURE_ASSIGNMENT.items()}\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('../data/hybrid_evaluation_results.json', 'w') as f:\n",
    "    json.dump(eval_output, f, indent=2)\n",
    "\n",
    "print(\"Evaluation results saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with previous results\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON WITH PREVIOUS APPROACHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "previous_results = {\n",
    "    'Scapy (Packet-level)': 19.8,\n",
    "    'TimeGAN (Default)': 29.8,\n",
    "    'SMOTE-KMeans': 34.0,\n",
    "    'DoppelGANger (Enhanced)': 34.9,\n",
    "    'HYBRID (This approach)': eval_results['overall_score']\n",
    "}\n",
    "\n",
    "print(f\"\\n{'Method':<30} {'Score':>10}\")\n",
    "print(\"-\"*42)\n",
    "for method, score in sorted(previous_results.items(), key=lambda x: x[1], reverse=True):\n",
    "    marker = ' <-- NEW' if 'HYBRID' in method else ''\n",
    "    print(f\"{method:<30} {score:>10.1f}{marker}\")\n",
    "\n",
    "best_previous = max(v for k, v in previous_results.items() if 'HYBRID' not in k)\n",
    "improvement = eval_results['overall_score'] - best_previous\n",
    "print(f\"\\nImprovement over best previous: {improvement:+.1f} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# =============================================================================\n# PART 2: DOPPELGANGER INTEGRATION\n# =============================================================================\n# This section integrates pre-trained DoppelGANger with post-processing\n# Use this when you want GAN's novelty but with fixed problematic features"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# DoppelGANger + Post-Processing Pipeline\n# =============================================================================\n\nclass DoppelGANgerHybrid:\n    \"\"\"\n    Hybrid pipeline that uses DoppelGANger as base generator\n    and applies targeted post-processing to fix problematic features.\n    \n    IMPORTANT: POST-PROCESSED GAN OUTPUT REQUIRED\n    =============================================\n    The GAN CSV input MUST be the post-processed DoppelGANger output, meaning:\n    - Inverse scaling applied (undo MinMaxScaler/StandardScaler)\n    - Inverse log transformation (np.expm1 for log1p features)\n    - Clipping to valid ranges (non-negative for count features)\n    - Rounding for integer features\n    \n    In your BGP_GAN_FIXED_v2.ipynb, this is: synthetic_processed['DoppelGANger']\n    NOT the raw GAN output, which is in scaled/transformed space.\n    \n    HOW TO USE WITH YOUR ACTUAL GAN OUTPUT:\n    ========================================\n    \n    Option 1: From CSV file (RECOMMENDED)\n    -------------------------------------\n    First, in your GAN notebook (BGP_GAN_FIXED_v2.ipynb), export the \n    post-processed data:\n    \n        >>> synthetic_processed['DoppelGANger'].to_csv(\n        ...     '/path/to/synthetic_doppelganger_postprocessed.csv', \n        ...     index=False\n        ... )\n    \n    Then use it here:\n    \n        >>> hybrid = DoppelGANgerHybrid(X_real, ks_threshold=0.4)\n        >>> synthetic = hybrid.generate(\n        ...     gan_output_path='/path/to/synthetic_doppelganger_postprocessed.csv'\n        ... )\n    \n    Option 2: From DataFrame (if running in same session)\n    ----------------------------------------------------\n        >>> # After running GAN notebook and post-processing:\n        >>> hybrid = DoppelGANgerHybrid(X_real, ks_threshold=0.4)\n        >>> synthetic = hybrid.generate(gan_output=synthetic_processed['DoppelGANger'])\n    \n    FEATURE NAME HANDLING:\n    - Automatically converts GAN-style names (nlriann) to standard (nlri_ann)\n    - Works with both naming conventions\n    \n    Strategy:\n    1. Load post-processed DoppelGANger output\n    2. Standardize column names for compatibility\n    3. Identify worst features (KS > threshold)\n    4. Replace worst features with SMOTE/KDE/Mixture alternatives\n    5. Re-align correlations\n    6. Enforce BGP constraints (including final integer rounding)\n    \"\"\"\n    \n    def __init__(self, real_data, ks_threshold=0.4, random_state=42):\n        \"\"\"\n        Parameters:\n        -----------\n        real_data : DataFrame - Real BGP data (used as reference)\n        ks_threshold : float - Features with KS > threshold get replaced\n        random_state : int - Random seed for reproducibility\n        \"\"\"\n        self.real_data = real_data\n        self.ks_threshold = ks_threshold\n        self.random_state = random_state\n        self.worst_features = []\n        self.feature_ks = {}\n        self.replaced_features = {}  # Track what was replaced and how\n        \n    def load_gan_output(self, gan_output_path):\n        \"\"\"\n        Load pre-generated DoppelGANger output from CSV.\n        \n        IMPORTANT: The CSV should contain POST-PROCESSED data:\n        - After inverse scaling\n        - After inverse log transform\n        - After clipping to valid ranges\n        - Values should be in the same domain as real data\n        \n        Automatically standardizes column names.\n        \"\"\"\n        self.gan_synthetic = pd.read_csv(gan_output_path)\n        self.gan_synthetic = standardize_column_names(self.gan_synthetic)\n        print(f\"Loaded GAN output: {self.gan_synthetic.shape}\")\n        print(f\"Columns: {list(self.gan_synthetic.columns)}\")\n        \n        # Verify data is in reasonable range (sanity check for post-processing)\n        for col in self.gan_synthetic.columns:\n            if col in self.real_data.columns:\n                real_max = self.real_data[col].max()\n                syn_max = self.gan_synthetic[col].max()\n                if syn_max > real_max * 100:\n                    print(f\"  WARNING: {col} has very high max ({syn_max:.1f} vs real {real_max:.1f})\")\n                    print(f\"           GAN output may not be post-processed correctly!\")\n        \n        return self\n    \n    def set_gan_output(self, gan_df):\n        \"\"\"\n        Set GAN output directly from DataFrame.\n        Automatically standardizes column names.\n        \"\"\"\n        self.gan_synthetic = gan_df.copy()\n        self.gan_synthetic = standardize_column_names(self.gan_synthetic)\n        return self\n        \n    def identify_worst_features(self):\n        \"\"\"Identify features with KS > threshold.\"\"\"\n        self.worst_features = []\n        self.feature_ks = {}\n        \n        for col in self.gan_synthetic.columns:\n            if col not in self.real_data.columns:\n                continue\n            \n            real_vals = self.real_data[col].dropna().values\n            syn_vals = self.gan_synthetic[col].dropna().values\n            \n            if len(real_vals) == 0 or len(syn_vals) == 0:\n                continue\n            \n            ks_stat, _ = ks_2samp(real_vals, syn_vals)\n            self.feature_ks[col] = ks_stat\n            \n            if ks_stat > self.ks_threshold:\n                self.worst_features.append(col)\n        \n        print(f\"\\nFeatures needing replacement (KS > {self.ks_threshold}):\")\n        for feat in sorted(self.worst_features, key=lambda x: self.feature_ks[x], reverse=True):\n            print(f\"  {feat}: KS={self.feature_ks[feat]:.4f}\")\n        \n        return self.worst_features\n    \n    def replace_worst_features(self, strategy='auto'):\n        \"\"\"\n        Replace worst features with better alternatives.\n        \n        strategy: 'auto', 'kde', 'smote', or 'mixture'\n        \n        NOTE: Features not covered by any generator strategy will use\n        empirical bootstrap sampling (random sampling with replacement\n        from real data). This is documented in replaced_features dict.\n        \"\"\"\n        n_samples = len(self.gan_synthetic)\n        \n        # Feature categorization for auto strategy\n        heavy_tailed = ['unique_as_path_max', 'edit_distance_max', 'rare_ases_avg', \n                        'edit_distance_avg', 'as_path_max']\n        sparse_features = ['flaps', 'nadas', 'imp_wd', 'number_rare_ases']\n        correlated_features = ['announcements', 'withdrawals', 'nlri_ann', \n                               'origin_0', 'origin_2', 'imp_wd_spath', 'imp_wd_dpath']\n        \n        self.replaced_features = {}\n        \n        for feature in self.worst_features:\n            if feature not in self.real_data.columns:\n                continue\n                \n            # Determine best strategy\n            if strategy == 'auto':\n                if feature in sparse_features:\n                    method = 'mixture'\n                elif feature in heavy_tailed:\n                    method = 'kde'\n                elif feature in correlated_features:\n                    method = 'smote'\n                else:\n                    method = 'kde'  # default for unknown features\n            else:\n                method = strategy\n            \n            # Generate replacement\n            try:\n                if method == 'kde':\n                    replacement_df = generate_empirical_kde(\n                        self.real_data, [feature], n_samples, \n                        random_state=self.random_state\n                    )\n                    if feature in replacement_df.columns:\n                        replacement = replacement_df[feature].values\n                    else:\n                        raise ValueError(\"KDE failed\")\n                        \n                elif method == 'mixture':\n                    replacement_df = generate_mixture_model(\n                        self.real_data, [feature], n_samples,\n                        random_state=self.random_state\n                    )\n                    if feature in replacement_df.columns:\n                        replacement = replacement_df[feature].values\n                    else:\n                        raise ValueError(\"Mixture failed\")\n                        \n                elif method == 'smote':\n                    replacement_df = generate_smote_kmeans(\n                        self.real_data, [feature], n_samples,\n                        random_state=self.random_state\n                    )\n                    if feature in replacement_df.columns:\n                        replacement = replacement_df[feature].values\n                    else:\n                        raise ValueError(\"SMOTE failed\")\n                \n                self.gan_synthetic[feature] = replacement\n                self.replaced_features[feature] = method\n                print(f\"  Replaced {feature} using {method}\")\n                \n            except Exception as e:\n                # Fallback to bootstrap sampling\n                replacement = np.random.choice(\n                    self.real_data[feature].values, n_samples, replace=True\n                )\n                self.gan_synthetic[feature] = replacement\n                self.replaced_features[feature] = 'bootstrap_fallback'\n                print(f\"  Replaced {feature} using bootstrap_fallback (reason: {str(e)[:30]})\")\n        \n        return self\n    \n    def align_correlations(self):\n        \"\"\"Re-align correlations after feature replacement.\"\"\"\n        corr_groups = [\n            ['announcements', 'withdrawals', 'nlri_ann', 'origin_0', 'origin_2'],\n            ['as_path_max', 'unique_as_path_max', 'edit_distance_max', 'edit_distance_avg'],\n            ['imp_wd', 'imp_wd_spath', 'imp_wd_dpath'],\n        ]\n        \n        for group in corr_groups:\n            available = [c for c in group \n                        if c in self.gan_synthetic.columns and c in self.real_data.columns]\n            if len(available) >= 2:\n                self.gan_synthetic = align_correlations(\n                    self.gan_synthetic, self.real_data, available\n                )\n        \n        print(\"  Correlations re-aligned\")\n        return self\n    \n    def enforce_constraints(self):\n        \"\"\"Apply BGP domain constraints including final integer enforcement.\"\"\"\n        self.gan_synthetic = enforce_bgp_constraints(\n            self.gan_synthetic, self.real_data, verbose=True\n        )\n        # Extra safety: ensure integers are integers\n        self.gan_synthetic = final_integer_enforcement(self.gan_synthetic)\n        print(\"  BGP constraints enforced (including integer rounding)\")\n        return self\n    \n    def generate(self, gan_output=None, gan_output_path=None):\n        \"\"\"\n        Full pipeline execution.\n        \n        Parameters:\n        -----------\n        gan_output : DataFrame - Pre-generated GAN output (direct)\n        gan_output_path : str - Path to CSV with GAN output\n        \n        IMPORTANT: Input must be POST-PROCESSED GAN output!\n        \n        Returns:\n        --------\n        DataFrame with post-processed synthetic data\n        \"\"\"\n        print(\"=\"*70)\n        print(\"DOPPELGANGER + POST-PROCESSING PIPELINE\")\n        print(\"=\"*70)\n        \n        # Load GAN output\n        if gan_output is not None:\n            self.set_gan_output(gan_output)\n            print(f\"Loaded GAN output from DataFrame: {self.gan_synthetic.shape}\")\n        elif gan_output_path is not None:\n            self.load_gan_output(gan_output_path)\n        else:\n            raise ValueError(\"Must provide gan_output (DataFrame) or gan_output_path (str)\")\n        \n        # Check column overlap\n        common_cols = set(self.gan_synthetic.columns) & set(self.real_data.columns)\n        print(f\"\\nColumn overlap: {len(common_cols)}/{len(self.real_data.columns)} features\")\n        \n        missing_in_gan = set(self.real_data.columns) - set(self.gan_synthetic.columns)\n        if missing_in_gan:\n            print(f\"WARNING: Missing in GAN output: {missing_in_gan}\")\n        \n        # Identify and replace worst features\n        print(\"\\n[1/4] Identifying worst features...\")\n        self.identify_worst_features()\n        \n        print(\"\\n[2/4] Replacing worst features...\")\n        self.replace_worst_features(strategy='auto')\n        \n        print(\"\\n[3/4] Re-aligning correlations...\")\n        self.align_correlations()\n        \n        print(\"\\n[4/4] Enforcing constraints...\")\n        self.enforce_constraints()\n        \n        print(\"\\n\" + \"=\"*70)\n        print(f\"COMPLETE: {len(self.gan_synthetic)} samples processed\")\n        print(f\"Features replaced: {len(self.replaced_features)}\")\n        print(\"=\"*70)\n        \n        return self.gan_synthetic\n    \n    def get_replacement_report(self):\n        \"\"\"Get a summary of which features were replaced and how.\"\"\"\n        return {\n            'worst_features': self.worst_features,\n            'feature_ks_scores': self.feature_ks,\n            'replaced_features': self.replaced_features,\n            'ks_threshold': self.ks_threshold\n        }\n\nprint(\"DoppelGANger Hybrid class defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Using DoppelGANger Hybrid Pipeline with Option 1 (CSV file)\n# =============================================================================\n# \n# IMPORTANT: The GAN CSV must contain POST-PROCESSED data!\n# In BGP_GAN_FIXED_v2.ipynb, this is synthetic_processed['DoppelGANger']\n# after: inverse scaling, inverse log transform, clipping, and rounding.\n#\n# If your current GAN output at the path below is RAW (scaled/transformed),\n# you need to first export the post-processed version from your GAN notebook:\n#\n#     synthetic_processed['DoppelGANger'].to_csv(\n#         '/home/smotaali/GAN_Traffic_Producer/results/gan_outputs_improved/synthetic_doppelganger_postprocessed.csv',\n#         index=False\n#     )\n# =============================================================================\n\n# Path to your post-processed DoppelGANger output\nGAN_OUTPUT_PATH = '/home/smotaali/GAN_Traffic_Producer/results/gan_outputs_improved/synthetic_doppelganger_improved.csv'\n\nprint(\"=\"*70)\nprint(\"OPTION 1: Loading DoppelGANger output from CSV file\")\nprint(\"=\"*70)\nprint(f\"\\nGAN output path: {GAN_OUTPUT_PATH}\")\n\n# Check if the file exists\nimport os\nif os.path.exists(GAN_OUTPUT_PATH):\n    print(f\"File found!\")\n    \n    # Create hybrid pipeline\n    hybrid_pipeline = DoppelGANgerHybrid(\n        X_real, \n        ks_threshold=0.4,  # Replace features with KS > 0.4\n        random_state=RANDOM_STATE\n    )\n    \n    # Run the full pipeline with Option 1\n    synthetic_gan_fixed = hybrid_pipeline.generate(\n        gan_output_path=GAN_OUTPUT_PATH\n    )\n    \n    # Get replacement report\n    report = hybrid_pipeline.get_replacement_report()\n    print(f\"\\nReplacement summary:\")\n    print(f\"  Total features evaluated: {len(report['feature_ks_scores'])}\")\n    print(f\"  Features replaced: {len(report['replaced_features'])}\")\n    \n    # Evaluate the result\n    eval_gan_fixed = evaluate_quality(X_real, synthetic_gan_fixed)\n    print(f\"\\n{'='*70}\")\n    print(f\"EVALUATION AFTER HYBRID POST-PROCESSING\")\n    print(f\"{'='*70}\")\n    print(f\"Overall Score: {eval_gan_fixed['overall_score']:.1f}/100\")\n    print(f\"Correlation Similarity: {eval_gan_fixed['correlation_similarity']:.4f}\")\n    print(f\"Mean KS: {eval_gan_fixed['mean_ks']:.4f}\")\n    \nelse:\n    print(f\"\\nFile NOT found at: {GAN_OUTPUT_PATH}\")\n    print(\"\\nTo use Option 1, you need to:\")\n    print(\"1. Run your GAN notebook (BGP_GAN_FIXED_v2.ipynb)\")\n    print(\"2. Export the POST-PROCESSED output to a CSV file:\")\n    print(\"   >>> synthetic_processed['DoppelGANger'].to_csv(\")\n    print(f\"   ...     '{GAN_OUTPUT_PATH}',\")\n    print(\"   ...     index=False\")\n    print(\"   ... )\")\n    print(\"\\n3. Re-run this cell\")\n    print(\"\\nAlternatively, update GAN_OUTPUT_PATH to point to your actual GAN output.\")\n    \n    # Create mock data for demonstration purposes\n    print(\"\\n--- Running with mock GAN data for demonstration ---\")\n    np.random.seed(42)\n    mock_gan_output = pd.DataFrame()\n    for col in X_real.columns:\n        real_vals = X_real[col].values\n        # Simulate GAN output with typical problems\n        mock_gan_output[col] = np.random.normal(\n            real_vals.mean(), \n            real_vals.std() * 0.7,  # GANs often underestimate variance\n            size=N_SYNTHETIC\n        )\n        mock_gan_output[col] = np.maximum(0, mock_gan_output[col])\n    \n    hybrid_pipeline = DoppelGANgerHybrid(X_real, ks_threshold=0.35)\n    synthetic_gan_fixed = hybrid_pipeline.generate(gan_output=mock_gan_output)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# =============================================================================\n# PART 3: ADVANCED GENERATION STRATEGIES\n# =============================================================================\n\n## Additional techniques for further improvement:\n1. **Copula-based Generation** - Better dependency structure preservation\n2. **Rejection Sampling** - Filter unrealistic samples\n3. **Ensemble Generation** - Combine multiple methods\n4. **Quantile Mapping** - Post-hoc distribution correction",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Strategy 1: Gaussian Copula Generation\n# =============================================================================\n# Copulas separate marginal distributions from dependency structure\n# This allows us to: 1) Sample exact marginals, 2) Impose real correlations\n\ndef generate_gaussian_copula(X_real, n_samples, random_state=42):\n    \"\"\"\n    Generate synthetic data using Gaussian Copula.\n    \n    Process:\n    1. Transform each feature to uniform via empirical CDF\n    2. Transform uniform to normal (Gaussian copula)\n    3. Estimate correlation matrix of Gaussian space\n    4. Generate correlated Gaussians\n    5. Transform back via inverse CDF\n    \n    This preserves:\n    - Exact marginal distributions (via inverse CDF)\n    - Correlation structure (via Gaussian copula)\n    \"\"\"\n    np.random.seed(random_state)\n    \n    n_features = X_real.shape[1]\n    feature_names = X_real.columns.tolist()\n    \n    # Step 1: Transform to uniform using empirical CDF (probability integral transform)\n    uniform_data = np.zeros_like(X_real.values, dtype=float)\n    \n    for i, col in enumerate(feature_names):\n        vals = X_real[col].values\n        # Empirical CDF: rank / n\n        ranks = stats.rankdata(vals, method='average')\n        uniform_data[:, i] = ranks / (len(vals) + 1)  # +1 to avoid 0 and 1\n    \n    # Step 2: Transform uniform to normal (inverse normal CDF)\n    normal_data = stats.norm.ppf(uniform_data)\n    normal_data = np.clip(normal_data, -6, 6)  # Clip extremes\n    \n    # Step 3: Estimate Gaussian correlation matrix\n    corr_matrix = np.corrcoef(normal_data.T)\n    \n    # Ensure positive definite\n    eigvals, eigvecs = np.linalg.eigh(corr_matrix)\n    eigvals = np.maximum(eigvals, 1e-6)\n    corr_matrix = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    \n    # Step 4: Generate correlated Gaussians\n    L = np.linalg.cholesky(corr_matrix)\n    standard_normals = np.random.randn(n_samples, n_features)\n    correlated_normals = standard_normals @ L.T\n    \n    # Step 5: Transform back via inverse empirical CDF\n    # Transform Gaussian to Uniform\n    uniform_synthetic = stats.norm.cdf(correlated_normals)\n    \n    # Transform Uniform to original scale via inverse empirical CDF (quantile function)\n    synthetic_data = np.zeros((n_samples, n_features))\n    \n    for i, col in enumerate(feature_names):\n        real_vals = np.sort(X_real[col].values)\n        # Quantile mapping\n        indices = (uniform_synthetic[:, i] * len(real_vals)).astype(int)\n        indices = np.clip(indices, 0, len(real_vals) - 1)\n        synthetic_data[:, i] = real_vals[indices]\n        \n        # Add small noise to avoid exact duplicates\n        noise = np.random.normal(0, real_vals.std() * 0.01, n_samples)\n        synthetic_data[:, i] = np.maximum(0, synthetic_data[:, i] + noise)\n    \n    return pd.DataFrame(synthetic_data, columns=feature_names)\n\nprint(\"Gaussian Copula generator defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Strategy 2: Rejection Sampling with Quality Filter\n# =============================================================================\n# Generate more samples than needed, then filter to keep best ones\n\ndef rejection_sampling(synthetic, real, n_final, quality_threshold=0.8):\n    \"\"\"\n    Filter synthetic samples to keep only high-quality ones.\n    \n    Quality is measured by:\n    1. Per-sample likelihood under real data distribution\n    2. Feature-wise percentile bounds\n    3. Constraint satisfaction\n    \n    Parameters:\n    -----------\n    synthetic : DataFrame - Generated samples (should be > n_final)\n    real : DataFrame - Real data\n    n_final : int - Number of samples to keep\n    quality_threshold : float - Percentile threshold (0.8 = keep top 80%)\n    \"\"\"\n    n_synthetic = len(synthetic)\n    \n    # Calculate quality scores\n    quality_scores = np.zeros(n_synthetic)\n    \n    for col in synthetic.columns:\n        if col not in real.columns:\n            continue\n            \n        real_vals = real[col].values\n        syn_vals = synthetic[col].values\n        \n        # Score: how well does each sample fit real distribution?\n        # Use percentile position as proxy for likelihood\n        p_low, p_high = np.percentile(real_vals, [2.5, 97.5])\n        \n        # Samples within typical range get high score\n        in_range = (syn_vals >= p_low) & (syn_vals <= p_high)\n        quality_scores += in_range.astype(float)\n        \n        # Penalize extreme outliers\n        extreme_low = syn_vals < np.percentile(real_vals, 0.5)\n        extreme_high = syn_vals > np.percentile(real_vals, 99.5)\n        quality_scores -= (extreme_low | extreme_high).astype(float) * 2\n    \n    # Normalize scores\n    quality_scores = (quality_scores - quality_scores.min()) / \\\n                     (quality_scores.max() - quality_scores.min() + 1e-10)\n    \n    # Select top samples\n    threshold = np.percentile(quality_scores, 100 * (1 - quality_threshold))\n    good_mask = quality_scores >= threshold\n    \n    good_samples = synthetic[good_mask].head(n_final)\n    \n    print(f\"Rejection sampling: kept {len(good_samples)}/{n_synthetic} samples\")\n    print(f\"Quality threshold: {threshold:.3f}\")\n    \n    return good_samples.reset_index(drop=True)\n\nprint(\"Rejection sampling defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Strategy 3: Ensemble Generation\n# =============================================================================\n# Generate from multiple methods, select best features from each\n\ndef ensemble_generation(X_real, n_samples, random_state=42):\n    \"\"\"\n    Ensemble approach: generate from multiple methods, \n    select best performing features from each.\n    \n    Methods:\n    1. SMOTE-KMeans\n    2. Gaussian Copula\n    3. Empirical KDE\n    \n    For each feature, use the method with best KS statistic.\n    \"\"\"\n    print(\"=\"*70)\n    print(\"ENSEMBLE GENERATION\")\n    print(\"=\"*70)\n    \n    # Generate from each method\n    print(\"\\n[1/3] Generating SMOTE-KMeans samples...\")\n    synthetic_smote = generate_smote_kmeans(\n        X_real, X_real.columns.tolist(), n_samples, \n        n_clusters=N_CLUSTERS, random_state=random_state\n    )\n    \n    print(\"[2/3] Generating Gaussian Copula samples...\")\n    synthetic_copula = generate_gaussian_copula(X_real, n_samples, random_state)\n    \n    print(\"[3/3] Generating KDE samples...\")\n    synthetic_kde = generate_empirical_kde(\n        X_real, X_real.columns.tolist(), n_samples, random_state=random_state\n    )\n    \n    # For each feature, select best method\n    print(\"\\n[4/4] Selecting best method per feature...\")\n    \n    ensemble_result = pd.DataFrame(index=range(n_samples))\n    method_selection = {}\n    \n    for col in X_real.columns:\n        real_vals = X_real[col].dropna().values\n        \n        candidates = {}\n        \n        # Evaluate each method\n        if col in synthetic_smote.columns:\n            ks_smote, _ = ks_2samp(real_vals, synthetic_smote[col].values)\n            candidates['smote'] = (ks_smote, synthetic_smote[col].values)\n        \n        if col in synthetic_copula.columns:\n            ks_copula, _ = ks_2samp(real_vals, synthetic_copula[col].values)\n            candidates['copula'] = (ks_copula, synthetic_copula[col].values)\n        \n        if col in synthetic_kde.columns:\n            ks_kde, _ = ks_2samp(real_vals, synthetic_kde[col].values)\n            candidates['kde'] = (ks_kde, synthetic_kde[col].values)\n        \n        # Select best (lowest KS)\n        if candidates:\n            best_method = min(candidates.keys(), key=lambda x: candidates[x][0])\n            ensemble_result[col] = candidates[best_method][1]\n            method_selection[col] = (best_method, candidates[best_method][0])\n        else:\n            # Fallback\n            ensemble_result[col] = np.random.choice(real_vals, n_samples, replace=True)\n            method_selection[col] = ('fallback', 1.0)\n    \n    # Print selection summary\n    print(\"\\nMethod selection per feature:\")\n    for method in ['smote', 'copula', 'kde', 'fallback']:\n        features = [f for f, (m, _) in method_selection.items() if m == method]\n        if features:\n            avg_ks = np.mean([method_selection[f][1] for f in features])\n            print(f\"  {method}: {len(features)} features (avg KS: {avg_ks:.4f})\")\n    \n    # Re-align correlations after ensemble\n    ensemble_result = align_correlations(ensemble_result, X_real)\n    ensemble_result = enforce_bgp_constraints(ensemble_result, X_real)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"ENSEMBLE COMPLETE: {len(ensemble_result)} samples\")\n    print(\"=\"*70)\n    \n    return ensemble_result, method_selection\n\nprint(\"Ensemble generation defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Strategy 4: Quantile Mapping (Distribution Correction)\n# =============================================================================\n# Post-hoc correction: map synthetic quantiles to real quantiles\n\ndef quantile_mapping(synthetic, real):\n    \"\"\"\n    Apply quantile mapping to correct synthetic distributions.\n    \n    This is a post-processing step that:\n    1. For each synthetic value, find its quantile\n    2. Replace with the real value at that quantile\n    \n    Pros: Guarantees matching marginal distributions\n    Cons: May break some correlations (use sparingly)\n    \"\"\"\n    result = synthetic.copy()\n    \n    for col in synthetic.columns:\n        if col not in real.columns:\n            continue\n            \n        syn_vals = synthetic[col].values\n        real_vals = real[col].values\n        \n        # Calculate quantiles of synthetic values\n        # (rank / n gives empirical CDF)\n        ranks = stats.rankdata(syn_vals, method='average')\n        quantiles = ranks / (len(syn_vals) + 1)\n        \n        # Map to real distribution quantiles\n        result[col] = np.percentile(real_vals, quantiles * 100)\n    \n    return result\n\n# =============================================================================\n# Selective Quantile Mapping (for worst features only)\n# =============================================================================\n\ndef selective_quantile_mapping(synthetic, real, features_to_map=None, ks_threshold=0.3):\n    \"\"\"\n    Apply quantile mapping only to features that need it.\n    \n    If features_to_map is None, automatically select features with KS > threshold.\n    \"\"\"\n    result = synthetic.copy()\n    \n    if features_to_map is None:\n        features_to_map = []\n        for col in synthetic.columns:\n            if col not in real.columns:\n                continue\n            ks_stat, _ = ks_2samp(real[col].dropna(), synthetic[col].dropna())\n            if ks_stat > ks_threshold:\n                features_to_map.append(col)\n    \n    print(f\"Applying quantile mapping to {len(features_to_map)} features\")\n    \n    for col in features_to_map:\n        if col not in real.columns:\n            continue\n            \n        syn_vals = synthetic[col].values\n        real_vals = real[col].values\n        \n        ranks = stats.rankdata(syn_vals, method='average')\n        quantiles = ranks / (len(syn_vals) + 1)\n        \n        result[col] = np.percentile(real_vals, quantiles * 100)\n        \n        # Check improvement\n        ks_before, _ = ks_2samp(real_vals, syn_vals)\n        ks_after, _ = ks_2samp(real_vals, result[col].values)\n        print(f\"  {col}: KS {ks_before:.4f} -> {ks_after:.4f}\")\n    \n    return result\n\nprint(\"Quantile mapping defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Run All Approaches and Compare\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"RUNNING ALL GENERATION APPROACHES FOR COMPARISON\")\nprint(\"=\"*70)\n\nall_results = {}\n\n# 1. Basic Hybrid (SMOTE-based)\nprint(\"\\n[1/4] Hybrid SMOTE approach...\")\nall_results['hybrid_smote'] = synthetic_hybrid\n\n# 2. Gaussian Copula\nprint(\"\\n[2/4] Gaussian Copula approach...\")\nsynthetic_copula = generate_gaussian_copula(X_real, N_SYNTHETIC, RANDOM_STATE)\nsynthetic_copula = enforce_bgp_constraints(synthetic_copula, X_real)\nall_results['copula'] = synthetic_copula\n\n# 3. Ensemble\nprint(\"\\n[3/4] Ensemble approach...\")\nsynthetic_ensemble, method_sel = ensemble_generation(X_real, N_SYNTHETIC, RANDOM_STATE)\nall_results['ensemble'] = synthetic_ensemble\n\n# 4. Copula + Rejection Sampling\nprint(\"\\n[4/4] Copula + Rejection Sampling...\")\n# Generate 50% more samples, then filter\nsynthetic_copula_extra = generate_gaussian_copula(X_real, int(N_SYNTHETIC * 1.5), RANDOM_STATE)\nsynthetic_copula_filtered = rejection_sampling(synthetic_copula_extra, X_real, N_SYNTHETIC)\nsynthetic_copula_filtered = enforce_bgp_constraints(synthetic_copula_filtered, X_real)\nall_results['copula_rejection'] = synthetic_copula_filtered\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ALL APPROACHES COMPLETE\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# Comprehensive Comparison\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"COMPREHENSIVE COMPARISON OF ALL APPROACHES\")\nprint(\"=\"*70)\n\ncomparison_results = []\n\nfor name, synthetic in all_results.items():\n    eval_res = evaluate_quality(X_real, synthetic)\n    comparison_results.append({\n        'Method': name,\n        'Overall Score': eval_res['overall_score'],\n        'KS Score': eval_res['component_scores']['distribution_ks'],\n        'Effect Score': eval_res['component_scores']['effect_size'],\n        'Correlation': eval_res['component_scores']['correlation'],\n        'Wasserstein': eval_res['component_scores']['wasserstein'],\n        'Mean KS': eval_res['mean_ks'],\n        'Corr Similarity': eval_res['correlation_similarity']\n    })\n\n# Add previous results for comparison\nprevious_methods = {\n    'Scapy (baseline)': {'Overall Score': 19.8, 'Correlation': 74.2},\n    'TimeGAN': {'Overall Score': 29.8, 'Correlation': 83.3},\n    'DoppelGANger': {'Overall Score': 34.9, 'Correlation': 86.9},\n    'SMOTE-KMeans (prev)': {'Overall Score': 34.0, 'Correlation': 89.5},\n}\n\ncomparison_df = pd.DataFrame(comparison_results)\ncomparison_df = comparison_df.sort_values('Overall Score', ascending=False)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"RESULTS COMPARISON TABLE\")\nprint(\"=\"*70)\nprint(comparison_df.to_string(index=False))\n\n# Find best method\nbest_method = comparison_df.iloc[0]['Method']\nbest_score = comparison_df.iloc[0]['Overall Score']\n\nprint(f\"\\n{'='*70}\")\nprint(f\"BEST NEW METHOD: {best_method}\")\nprint(f\"SCORE: {best_score:.1f}/100\")\nprint(f\"{'='*70}\")\n\n# Compare with previous best\nprev_best = 34.9  # DoppelGANger\nimprovement = best_score - prev_best\nprint(f\"\\nImprovement over previous best (DoppelGANger): {improvement:+.1f} points\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# =============================================================================\n# RECOMMENDATIONS & BEST PRACTICES\n# =============================================================================\n\n## Summary of Approaches\n\n| Approach | When to Use | Pros | Cons |\n|----------|-------------|------|------|\n| **SMOTE-KMeans** | Fast generation, correlation priority | Fast, best correlation | Limited novelty |\n| **Gaussian Copula** | Exact marginals needed | Perfect marginals | May miss complex dependencies |\n| **Ensemble** | Maximum quality per-feature | Best of all methods | Slower, may break correlations |\n| **DoppelGANger + Post-process** | Temporal data, novelty needed | Novel samples, GAN benefits | Requires pre-trained GAN |\n| **Rejection Sampling** | Quality filtering | Removes outliers | Requires overgeneration |\n\n## Recommended Pipeline for BGP Data\n\n```\n1. Generate base samples with SMOTE-KMeans (fast, good correlations)\n   OR load pre-trained DoppelGANger output (if temporal novelty needed)\n\n2. Apply selective quantile mapping to worst features (KS > 0.4)\n\n3. Re-align correlations using Cholesky decomposition\n\n4. Enforce BGP domain constraints\n\n5. (Optional) Apply rejection sampling to filter low-quality samples\n```\n\n## Key Findings\n\n1. **Gaussian Copula** typically gives best marginal distributions\n2. **SMOTE-KMeans** gives best correlation preservation\n3. **Ensemble** approach balances both but is slower\n4. **Post-processing** (quantile mapping, rejection sampling) consistently improves quality",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# FINAL RECOMMENDED PIPELINE\n# =============================================================================\n\n# Default path to your post-processed DoppelGANger output\nDEFAULT_GAN_PATH = '/home/smotaali/GAN_Traffic_Producer/results/gan_outputs_improved/synthetic_doppelganger_improved.csv'\n\ndef generate_best_pipeline(X_real, n_samples, use_gan=False, gan_path=None, random_state=42):\n    \"\"\"\n    The recommended pipeline based on our experiments.\n    \n    Parameters:\n    -----------\n    X_real : DataFrame - Real BGP data\n    n_samples : int - Number of samples to generate\n    use_gan : bool - Whether to use pre-trained GAN as base\n    gan_path : str - Path to POST-PROCESSED GAN output CSV (if use_gan=True)\n                     Must be post-processed (inverse scaling, log-inverse, clipping, rounding)\n                     In BGP_GAN_FIXED_v2.ipynb: synthetic_processed['DoppelGANger']\n    \n    Returns:\n    --------\n    DataFrame with high-quality synthetic data\n    \"\"\"\n    print(\"=\"*70)\n    print(\"RECOMMENDED PIPELINE FOR BGP SYNTHETIC DATA\")\n    print(\"=\"*70)\n    \n    if use_gan:\n        # Option A: DoppelGANger + Post-processing\n        print(\"\\n[Mode: DoppelGANger + Post-processing]\")\n        \n        # Use provided path or default\n        actual_gan_path = gan_path if gan_path else DEFAULT_GAN_PATH\n        \n        if not os.path.exists(actual_gan_path):\n            print(f\"\\nERROR: GAN output file not found at: {actual_gan_path}\")\n            print(\"\\nPlease ensure the post-processed GAN output exists.\")\n            print(\"In your GAN notebook (BGP_GAN_FIXED_v2.ipynb), export it:\")\n            print(\"  synthetic_processed['DoppelGANger'].to_csv('...', index=False)\")\n            print(\"\\nFalling back to Copula-SMOTE Hybrid mode...\")\n            use_gan = False\n        else:\n            print(f\"\\nUsing GAN output from: {actual_gan_path}\")\n            pipeline = DoppelGANgerHybrid(X_real, ks_threshold=0.35, random_state=random_state)\n            synthetic = pipeline.generate(gan_output_path=actual_gan_path)\n            return synthetic\n    \n    if not use_gan:\n        # Option B: Gaussian Copula + SMOTE hybrid\n        print(\"\\n[Mode: Copula-SMOTE Hybrid]\")\n        \n        # Step 1: Generate base with Gaussian Copula (best marginals)\n        print(\"\\n[1/5] Generating Gaussian Copula base...\")\n        synthetic = generate_gaussian_copula(X_real, n_samples, random_state)\n        \n        # Step 2: For correlation-critical features, blend with SMOTE\n        print(\"\\n[2/5] Blending with SMOTE for correlation features...\")\n        corr_features = ['announcements', 'withdrawals', 'nlri_ann', \n                        'origin_0', 'origin_2', 'imp_wd_spath', 'imp_wd_dpath']\n        smote_output = generate_smote_kmeans(X_real, corr_features, n_samples, random_state=random_state)\n        \n        # Use SMOTE for correlation-critical features\n        for col in corr_features:\n            if col in smote_output.columns:\n                synthetic[col] = smote_output[col].values\n        \n        # Step 3: Re-align correlations\n        print(\"\\n[3/5] Aligning correlations...\")\n        synthetic = align_correlations(synthetic, X_real)\n        \n        # Step 4: Enforce constraints\n        print(\"\\n[4/5] Enforcing BGP constraints...\")\n        synthetic = enforce_bgp_constraints(synthetic, X_real)\n        \n        # Step 5: Rejection sampling (optional quality filter)\n        print(\"\\n[5/5] Quality filtering...\")\n        # Generate extra and filter\n        extra = generate_gaussian_copula(X_real, int(n_samples * 0.3), random_state + 1)\n        combined = pd.concat([synthetic, extra], ignore_index=True)\n        synthetic = rejection_sampling(combined, X_real, n_samples, quality_threshold=0.85)\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"PIPELINE COMPLETE: {len(synthetic)} samples generated\")\n    print(\"=\"*70)\n    \n    return synthetic\n\n# =============================================================================\n# Run the recommended pipeline\n# =============================================================================\n# Set use_gan=True to use your DoppelGANger output\n# Set use_gan=False to use pure Copula-SMOTE hybrid\n\nprint(\"Running recommended pipeline...\")\nprint(\"(Set use_gan=True below to use DoppelGANger output)\\n\")\n\n# Choose your mode:\nUSE_DOPPELGANGER = False  # Change to True to use your GAN output\n\nsynthetic_best = generate_best_pipeline(\n    X_real, \n    N_SYNTHETIC, \n    use_gan=USE_DOPPELGANGER,\n    gan_path=DEFAULT_GAN_PATH  # Uses your post-processed GAN output\n)\n\n# Evaluate\neval_best = evaluate_quality(X_real, synthetic_best)\nprint(f\"\\nFinal Score: {eval_best['overall_score']:.1f}/100\")\nprint(f\"Correlation Similarity: {eval_best['correlation_similarity']:.4f}\")\nprint(f\"Mean KS: {eval_best['mean_ks']:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# SAVE BEST RESULTS\n# =============================================================================\n\n# Save the best synthetic data\nbest_output_path = '../data/synthetic_best_pipeline.csv'\nsynthetic_best.to_csv(best_output_path, index=False)\nprint(f\"Saved best synthetic data to {best_output_path}\")\n\n# Save comparison results\ncomparison_df.to_csv('../data/method_comparison_results.csv', index=False)\nprint(\"Saved comparison results\")\n\n# Save comprehensive evaluation\nimport json\n\nfinal_eval = {\n    'best_method': best_method,\n    'best_score': float(best_score),\n    'comparison': comparison_df.to_dict('records'),\n    'improvement_over_doppelganger': float(improvement),\n    'configuration': {\n        'n_samples': N_SYNTHETIC,\n        'n_clusters': N_CLUSTERS,\n        'random_state': RANDOM_STATE\n    }\n}\n\nwith open('../data/final_evaluation.json', 'w') as f:\n    json.dump(final_eval, f, indent=2)\n\nprint(\"All results saved!\")\nprint(f\"\\nFinal recommendation: Use '{best_method}' approach for best results\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}