{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# BGP Anomaly Detection with Normal Baseline\n",
    "\n",
    "## The Correct Approach for Anomaly Detection\n",
    "\n",
    "**Problem with previous approach:**\n",
    "- Unsupervised methods find outliers *within* the provided data\n",
    "- If you only provide attack data, they find \"unusual attacks\" not \"attacks vs normal\"\n",
    "\n",
    "**Correct approach:**\n",
    "1. Train models on **NORMAL traffic** (learn what normal looks like)\n",
    "2. Score **NEW/UNKNOWN data** against the normal baseline\n",
    "3. High scores = anomalies (deviates from normal)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# NORMAL traffic file (for training the baseline)\n",
    "NORMAL_FILE = \"/path/to/normal_traffic_features.csv\"  # <-- YOUR NORMAL DATA\n",
    "\n",
    "# File to test/score (can be normal, attack, or unknown)\n",
    "TEST_FILE = \"/path/to/test_features.csv\"  # <-- DATA TO CLASSIFY\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = None  # Uses test file directory if None\n",
    "\n",
    "# Label column (if exists in test file)\n",
    "LABEL_COL = 'label'\n",
    "\n",
    "# Threshold for anomaly (percentile of normal data scores)\n",
    "# 95 means: flag as anomaly if score is worse than 95% of normal data\n",
    "ANOMALY_THRESHOLD_PERCENTILE = 95\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Save plots\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"  Normal file: {NORMAL_FILE}\")\n",
    "print(f\"  Test file: {TEST_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-load",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normal data (for training)\n",
    "print(\"Loading NORMAL data (for baseline training)...\")\n",
    "df_normal = pd.read_csv(NORMAL_FILE)\n",
    "print(f\"  Loaded {len(df_normal)} normal samples\")\n",
    "\n",
    "# Load test data (to classify)\n",
    "print(\"\\nLoading TEST data (to classify)...\")\n",
    "df_test = pd.read_csv(TEST_FILE)\n",
    "print(f\"  Loaded {len(df_test)} test samples\")\n",
    "\n",
    "# Show test data label distribution if available\n",
    "if LABEL_COL and LABEL_COL in df_test.columns:\n",
    "    print(f\"\\nTest data labels (ground truth):\")\n",
    "    for label, count in df_test[LABEL_COL].value_counts().items():\n",
    "        print(f\"  {label}: {count} ({count/len(df_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features",
   "metadata": {},
   "outputs": [],
   "source": "# Identify feature columns - exclude metadata and previous analysis columns\nMETA_COLS = {\n    # Metadata columns\n    'incident', 'window_start', 'window_end', 'timestamp', 'time',\n    'label', 'label_rule', 'label_refined', 'source', 'collector',\n    # Previous analysis columns (from BGP_Label_Validation_Discovery.ipynb)\n    'iso_forest_score', 'lof_score', 'statistical_score', 'elliptic_score',\n    'cluster', 'anomaly_votes', 'consensus_score', 'discovered_label',\n    # Any other score columns\n    'hdbscan_score', 'predicted_label'\n}\n\ndef get_feature_cols(df):\n    \"\"\"Get only the actual BGP feature columns, excluding metadata and analysis columns.\"\"\"\n    candidates = [c for c in df.columns if c.lower() not in {m.lower() for m in META_COLS}]\n    return df[candidates].select_dtypes(include=[np.number]).columns.tolist()\n\n# Get feature columns from BOTH datasets and use intersection\nnormal_features = set(get_feature_cols(df_normal))\ntest_features = set(get_feature_cols(df_test))\n\n# Use only features present in BOTH datasets\nfeature_cols = sorted(list(normal_features & test_features))\n\nprint(f\"Normal dataset has {len(normal_features)} features\")\nprint(f\"Test dataset has {len(test_features)} features\")\nprint(f\"Using {len(feature_cols)} common features\")\nprint(f\"\\nFeatures: {feature_cols}\")\n\n# Prepare normal data\nX_normal = df_normal[feature_cols].values\nvalid_normal = ~np.isnan(X_normal).any(axis=1)\nX_normal = X_normal[valid_normal]\nprint(f\"\\nNormal samples after removing NaN: {len(X_normal)}\")\n\n# Prepare test data\nX_test = df_test[feature_cols].values\nvalid_test = ~np.isnan(X_test).any(axis=1)\nX_test_clean = X_test[valid_test]\nprint(f\"Test samples after removing NaN: {len(X_test_clean)}\")\n\n# Fit scaler on NORMAL data only\nscaler = RobustScaler()\nX_normal_scaled = scaler.fit_transform(X_normal)\nX_test_scaled = scaler.transform(X_test_clean)  # Transform test with normal's scaler\n\nprint(\"\\nScaler fitted on NORMAL data and applied to test data\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-train",
   "metadata": {},
   "source": [
    "## 2. Train Models on NORMAL Data\n",
    "\n",
    "This is the key difference: we train **only on normal data** to learn what normal looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training anomaly detection models on NORMAL data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "normal_scores = {}\n",
    "\n",
    "# 1. Isolation Forest\n",
    "print(\"\\n[1/4] Training Isolation Forest...\")\n",
    "models['IsolationForest'] = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.01,  # Expect very few anomalies in normal data\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "models['IsolationForest'].fit(X_normal_scaled)\n",
    "normal_scores['IsolationForest'] = models['IsolationForest'].decision_function(X_normal_scaled)\n",
    "print(f\"  Normal score range: [{normal_scores['IsolationForest'].min():.3f}, {normal_scores['IsolationForest'].max():.3f}]\")\n",
    "\n",
    "# 2. Local Outlier Factor (novelty detection mode)\n",
    "print(\"\\n[2/4] Training Local Outlier Factor...\")\n",
    "n_neighbors = max(10, min(50, int(np.sqrt(len(X_normal_scaled)))))\n",
    "models['LOF'] = LocalOutlierFactor(\n",
    "    n_neighbors=n_neighbors,\n",
    "    contamination=0.01,\n",
    "    novelty=True  # Important: enables scoring new data\n",
    ")\n",
    "models['LOF'].fit(X_normal_scaled)\n",
    "normal_scores['LOF'] = models['LOF'].decision_function(X_normal_scaled)\n",
    "print(f\"  Normal score range: [{normal_scores['LOF'].min():.3f}, {normal_scores['LOF'].max():.3f}]\")\n",
    "\n",
    "# 3. Elliptic Envelope\n",
    "print(\"\\n[3/4] Training Elliptic Envelope...\")\n",
    "try:\n",
    "    models['EllipticEnvelope'] = EllipticEnvelope(\n",
    "        contamination=0.01,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    models['EllipticEnvelope'].fit(X_normal_scaled)\n",
    "    normal_scores['EllipticEnvelope'] = models['EllipticEnvelope'].decision_function(X_normal_scaled)\n",
    "    print(f\"  Normal score range: [{normal_scores['EllipticEnvelope'].min():.3f}, {normal_scores['EllipticEnvelope'].max():.3f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipped due to error: {e}\")\n",
    "    models['EllipticEnvelope'] = None\n",
    "\n",
    "# 4. Statistical baseline (mean and std of normal data)\n",
    "print(\"\\n[4/4] Computing Statistical baseline...\")\n",
    "models['Statistical'] = {\n",
    "    'mean': np.mean(X_normal_scaled, axis=0),\n",
    "    'std': np.std(X_normal_scaled, axis=0)\n",
    "}\n",
    "# Mahalanobis-like distance from normal mean\n",
    "normal_scores['Statistical'] = -np.mean(\n",
    "    np.abs((X_normal_scaled - models['Statistical']['mean']) / (models['Statistical']['std'] + 1e-10)), \n",
    "    axis=1\n",
    ")\n",
    "print(f\"  Normal score range: [{normal_scores['Statistical'].min():.3f}, {normal_scores['Statistical'].max():.3f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained on NORMAL data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-thresholds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute thresholds based on normal data distribution\n",
    "print(f\"Computing anomaly thresholds ({ANOMALY_THRESHOLD_PERCENTILE}th percentile of normal scores)...\")\n",
    "\n",
    "thresholds = {}\n",
    "for method, scores in normal_scores.items():\n",
    "    # For decision_function: higher = more normal, lower = more anomalous\n",
    "    # So we use a LOW percentile as threshold\n",
    "    thresholds[method] = np.percentile(scores, 100 - ANOMALY_THRESHOLD_PERCENTILE)\n",
    "    print(f\"  {method}: threshold = {thresholds[method]:.4f}\")\n",
    "\n",
    "print(\"\\nSamples scoring BELOW these thresholds will be flagged as anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-score",
   "metadata": {},
   "source": [
    "## 3. Score Test Data Against Normal Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scoring TEST data against NORMAL baseline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_scores = {}\n",
    "test_anomalies = {}\n",
    "\n",
    "# 1. Isolation Forest\n",
    "print(\"\\n[1/4] Scoring with Isolation Forest...\")\n",
    "test_scores['IsolationForest'] = models['IsolationForest'].decision_function(X_test_scaled)\n",
    "test_anomalies['IsolationForest'] = test_scores['IsolationForest'] < thresholds['IsolationForest']\n",
    "print(f\"  Anomalies: {test_anomalies['IsolationForest'].sum()} ({test_anomalies['IsolationForest'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 2. LOF\n",
    "print(\"\\n[2/4] Scoring with LOF...\")\n",
    "test_scores['LOF'] = models['LOF'].decision_function(X_test_scaled)\n",
    "test_anomalies['LOF'] = test_scores['LOF'] < thresholds['LOF']\n",
    "print(f\"  Anomalies: {test_anomalies['LOF'].sum()} ({test_anomalies['LOF'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 3. Elliptic Envelope\n",
    "if models['EllipticEnvelope'] is not None:\n",
    "    print(\"\\n[3/4] Scoring with Elliptic Envelope...\")\n",
    "    test_scores['EllipticEnvelope'] = models['EllipticEnvelope'].decision_function(X_test_scaled)\n",
    "    test_anomalies['EllipticEnvelope'] = test_scores['EllipticEnvelope'] < thresholds['EllipticEnvelope']\n",
    "    print(f\"  Anomalies: {test_anomalies['EllipticEnvelope'].sum()} ({test_anomalies['EllipticEnvelope'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 4. Statistical\n",
    "print(\"\\n[4/4] Scoring with Statistical method...\")\n",
    "test_scores['Statistical'] = -np.mean(\n",
    "    np.abs((X_test_scaled - models['Statistical']['mean']) / (models['Statistical']['std'] + 1e-10)), \n",
    "    axis=1\n",
    ")\n",
    "test_anomalies['Statistical'] = test_scores['Statistical'] < thresholds['Statistical']\n",
    "print(f\"  Anomalies: {test_anomalies['Statistical'].sum()} ({test_anomalies['Statistical'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute consensus\n",
    "print(\"Computing consensus across methods...\")\n",
    "\n",
    "n_methods = len(test_anomalies)\n",
    "anomaly_votes = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for method, anomalies in test_anomalies.items():\n",
    "    anomaly_votes += anomalies.astype(int)\n",
    "\n",
    "consensus_score = anomaly_votes / n_methods\n",
    "\n",
    "# Assign labels\n",
    "predicted_labels = np.where(\n",
    "    consensus_score >= 0.75, 'anomaly',\n",
    "    np.where(\n",
    "        consensus_score >= 0.5, 'likely_anomaly',\n",
    "        np.where(\n",
    "            consensus_score >= 0.25, 'uncertain',\n",
    "            'normal'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nPredicted Label Distribution:\")\n",
    "print(\"=\"*50)\n",
    "for label in ['normal', 'uncertain', 'likely_anomaly', 'anomaly']:\n",
    "    count = (predicted_labels == label).sum()\n",
    "    pct = count / len(predicted_labels) * 100\n",
    "    emoji = \"üü¢\" if label == 'normal' else \"üî¥\" if 'anomaly' in label else \"üü°\"\n",
    "    print(f\"  {emoji} {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-evaluate",
   "metadata": {},
   "source": [
    "## 4. Evaluate Against Ground Truth (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABEL_COL and LABEL_COL in df_test.columns:\n",
    "    print(\"Evaluating against ground truth labels...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get ground truth for valid samples\n",
    "    ground_truth = df_test.loc[valid_test, LABEL_COL].values\n",
    "    \n",
    "    # Binary ground truth: normal vs not normal\n",
    "    is_attack_gt = ~np.isin(ground_truth, ['normal', 'Normal', 'NORMAL'])\n",
    "    is_attack_pred = np.isin(predicted_labels, ['anomaly', 'likely_anomaly'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    TP = np.sum(is_attack_gt & is_attack_pred)\n",
    "    TN = np.sum(~is_attack_gt & ~is_attack_pred)\n",
    "    FP = np.sum(~is_attack_gt & is_attack_pred)\n",
    "    FN = np.sum(is_attack_gt & ~is_attack_pred)\n",
    "    \n",
    "    accuracy = (TP + TN) / len(ground_truth)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä DETECTION PERFORMANCE:\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.1f}%\")\n",
    "    print(f\"  Precision: {precision*100:.1f}%\")\n",
    "    print(f\"  Recall:    {recall*100:.1f}%\")\n",
    "    print(f\"  F1-Score:  {f1*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüìã CONFUSION MATRIX:\")\n",
    "    print(f\"  True Positives (attacks detected):  {TP}\")\n",
    "    print(f\"  True Negatives (normal correct):    {TN}\")\n",
    "    print(f\"  False Positives (false alarms):     {FP}\")\n",
    "    print(f\"  False Negatives (missed attacks):   {FN}\")\n",
    "    \n",
    "    # Per-class detection rates\n",
    "    print(f\"\\nüìà PER-CLASS DETECTION RATES:\")\n",
    "    for label in df_test[LABEL_COL].unique():\n",
    "        mask = ground_truth == label\n",
    "        if mask.sum() > 0:\n",
    "            detected = is_attack_pred[mask].sum()\n",
    "            rate = detected / mask.sum() * 100\n",
    "            emoji = \"‚úÖ\" if rate > 80 else \"‚ö†Ô∏è\" if rate > 50 else \"‚ùå\"\n",
    "            print(f\"  {emoji} {label}: {detected}/{mask.sum()} detected ({rate:.1f}%)\")\n",
    "else:\n",
    "    print(\"No ground truth labels available for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "0gb8vdudqrbu",
   "source": "# ============================================\n# DIAGNOSTIC: Compare Normal vs Attack Feature Distributions\n# ============================================\nprint(\"DIAGNOSTIC: Comparing feature distributions...\")\nprint(\"=\"*60)\n\n# Get attack data statistics\nattack_stats = pd.DataFrame(X_test_scaled, columns=feature_cols).describe()\nnormal_stats = pd.DataFrame(X_normal_scaled, columns=feature_cols).describe()\n\n# Compare means and stds\nprint(\"\\nüìä FEATURE COMPARISON (Normal vs Attack):\")\nprint(\"-\"*60)\nprint(f\"{'Feature':<30} {'Normal Mean':>12} {'Attack Mean':>12} {'Difference':>12}\")\nprint(\"-\"*60)\n\nsignificant_diffs = []\nfor col in feature_cols:\n    n_mean = normal_stats.loc['mean', col]\n    a_mean = attack_stats.loc['mean', col]\n    diff = abs(a_mean - n_mean)\n    significant_diffs.append((col, n_mean, a_mean, diff))\n\n# Sort by difference\nsignificant_diffs.sort(key=lambda x: x[3], reverse=True)\n\nfor col, n_mean, a_mean, diff in significant_diffs[:15]:\n    indicator = \"‚ö†Ô∏è\" if diff > 0.5 else \"  \"\n    print(f\"{indicator} {col:<28} {n_mean:>12.3f} {a_mean:>12.3f} {diff:>12.3f}\")\n\nprint(\"\\n‚ö†Ô∏è = Features with significant difference (>0.5 std)\")\nprint(\"\\nIf most differences are small, normal and attack data are SIMILAR!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bdh16cg6nfg",
   "source": "# ============================================\n# DIAGNOSTIC: Visualize Score Overlap\n# ============================================\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, (method, test_sc) in enumerate(test_scores.items()):\n    if i >= 4:\n        break\n    ax = axes[i]\n    \n    normal_sc = normal_scores[method]\n    threshold = thresholds[method]\n    \n    # Plot distributions\n    ax.hist(normal_sc, bins=50, alpha=0.6, label=f'Normal baseline (n={len(normal_sc)})', color='green', density=True)\n    ax.hist(test_sc, bins=50, alpha=0.6, label=f'Attack data (n={len(test_sc)})', color='red', density=True)\n    ax.axvline(threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold')\n    \n    # Calculate overlap\n    overlap_pct = (test_sc >= threshold).mean() * 100\n    ax.set_title(f'{method}\\nAttacks ABOVE threshold (looks normal): {overlap_pct:.1f}%')\n    ax.set_xlabel('Score (higher = more normal)')\n    ax.legend()\n\nplt.suptitle('PROBLEM: Attack scores overlap with Normal scores!', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('diagnostic_score_overlap.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"\\nüî¥ If red (attack) distribution overlaps with green (normal), the data is too similar!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pvmksguaf7d",
   "source": "## ‚ö†Ô∏è Diagnostic Results Interpretation\n\nIf attack detection is very low (~1%), there are **two possible explanations**:\n\n### 1. Your \"Normal\" Data Contains Anomalies\n- You mentioned the RIPE data is \"assumed normal\" without ground truth\n- If your baseline already contains attack-like patterns, the model learns them as normal\n- **Solution**: Clean the baseline or use known-good normal data\n\n### 2. The BGP Features Don't Distinguish Attacks\n- Your 27 features might not capture what makes attacks different\n- The attack traffic might have similar aggregate statistics to normal traffic\n- **Solution**: Engineer better features or use different detection approaches\n\n---\n\n## üî¨ Let's Test: Check if Normal Data is Clean\n\nRun the cell below to analyze your \"normal\" baseline for potential contamination:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0tpj4rbc7m3",
   "source": "# ============================================\n# CHECK: Is the Normal Baseline Clean?\n# ============================================\nprint(\"Analyzing NORMAL baseline for potential contamination...\")\nprint(\"=\"*60)\n\n# Check variance in normal data\nnormal_df = pd.DataFrame(X_normal_scaled, columns=feature_cols)\n\n# High variance features might indicate contamination\nvariances = normal_df.var().sort_values(ascending=False)\nprint(\"\\nüìä Top 10 highest variance features in 'normal' data:\")\nfor feat, var in variances.head(10).items():\n    warning = \"‚ö†Ô∏è HIGH\" if var > 2 else \"\"\n    print(f\"  {feat}: {var:.3f} {warning}\")\n\n# Check for outliers within normal data using IQR\nprint(\"\\nüìä Outlier check within 'normal' data (IQR method):\")\noutlier_counts = {}\nfor col in feature_cols:\n    Q1 = normal_df[col].quantile(0.25)\n    Q3 = normal_df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = ((normal_df[col] < Q1 - 1.5*IQR) | (normal_df[col] > Q3 + 1.5*IQR)).sum()\n    outlier_counts[col] = outliers\n\ntotal_outliers = sum(outlier_counts.values())\navg_outlier_pct = (total_outliers / (len(normal_df) * len(feature_cols))) * 100\nprint(f\"  Average outlier rate: {avg_outlier_pct:.2f}%\")\n\nif avg_outlier_pct > 5:\n    print(\"  ‚ö†Ô∏è HIGH outlier rate suggests normal data may be contaminated!\")\nelse:\n    print(\"  ‚úÖ Outlier rate is reasonable\")\n\n# Check skewness\nprint(\"\\nüìä Feature skewness (high skew = potential contamination):\")\nskews = normal_df.skew().abs().sort_values(ascending=False)\nfor feat, skew in skews.head(5).items():\n    warning = \"‚ö†Ô∏è\" if skew > 2 else \"\"\n    print(f\"  {feat}: {skew:.3f} {warning}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "9dtxsem6c75",
   "source": "# ============================================\n# STATISTICAL TEST: Are Normal and Attack Actually Different?\n# ============================================\nfrom scipy.stats import mannwhitneyu\n\nprint(\"Statistical Test: Are normal and attack distributions different?\")\nprint(\"=\"*60)\nprint(\"(Mann-Whitney U test, p < 0.05 = significantly different)\")\nprint()\n\ntest_results = []\nfor col in feature_cols:\n    normal_vals = X_normal_scaled[:, feature_cols.index(col)]\n    attack_vals = X_test_scaled[:, feature_cols.index(col)]\n    \n    try:\n        stat, p_value = mannwhitneyu(normal_vals, attack_vals, alternative='two-sided')\n        effect_size = abs(normal_vals.mean() - attack_vals.mean())\n        test_results.append((col, p_value, effect_size))\n    except:\n        test_results.append((col, 1.0, 0.0))\n\n# Sort by effect size\ntest_results.sort(key=lambda x: x[2], reverse=True)\n\nprint(f\"{'Feature':<30} {'p-value':>12} {'Effect Size':>12} {'Significant?':>12}\")\nprint(\"-\"*70)\n\nsignificant_count = 0\nfor col, p, effect in test_results:\n    sig = \"‚úÖ YES\" if p < 0.05 else \"‚ùå NO\"\n    if p < 0.05:\n        significant_count += 1\n    print(f\"{col:<30} {p:>12.4f} {effect:>12.3f} {sig:>12}\")\n\nprint()\nprint(f\"üìä SUMMARY: {significant_count}/{len(feature_cols)} features are significantly different\")\n\nif significant_count < len(feature_cols) * 0.3:\n    print(\"‚ö†Ô∏è PROBLEM: Most features are NOT different between normal and attack!\")\n    print(\"   This means either:\")\n    print(\"   1. Normal baseline contains attack-like patterns\")\n    print(\"   2. These features don't capture attack characteristics\")\nelif significant_count > len(feature_cols) * 0.7:\n    print(\"‚úÖ Most features ARE different - detection should work!\")\n    print(\"   If detection still fails, try lowering the threshold.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method, scores) in enumerate(test_scores.items()):\n",
    "    if i >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot normal scores distribution\n",
    "    ax.hist(normal_scores[method], bins=50, alpha=0.5, label='Normal (training)', color='green', density=True)\n",
    "    \n",
    "    # Plot test scores distribution\n",
    "    ax.hist(scores, bins=50, alpha=0.5, label='Test data', color='blue', density=True)\n",
    "    \n",
    "    # Mark threshold\n",
    "    ax.axvline(x=thresholds[method], color='red', linestyle='--', linewidth=2, label=f'Threshold')\n",
    "    \n",
    "    ax.set_xlabel('Anomaly Score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{method}\\n(Anomalies: {test_anomalies[method].mean()*100:.1f}%)')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to: score_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = df_test.loc[valid_test].copy()\n",
    "\n",
    "# Add scores\n",
    "for method, scores in test_scores.items():\n",
    "    results_df[f'{method}_score'] = scores\n",
    "    results_df[f'{method}_anomaly'] = test_anomalies[method]\n",
    "\n",
    "results_df['consensus_score'] = consensus_score\n",
    "results_df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Save\n",
    "output_path = Path(TEST_FILE).parent / f\"{Path(TEST_FILE).stem}_anomaly_detection.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements the **correct** approach for anomaly detection:\n",
    "\n",
    "1. **Train on NORMAL data** - Learn what normal traffic looks like\n",
    "2. **Compute thresholds** - Based on normal data distribution\n",
    "3. **Score test data** - Measure how different from normal\n",
    "4. **Flag anomalies** - Samples that deviate significantly from normal\n",
    "\n",
    "This approach will correctly identify attacks as anomalies because they differ from the normal baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}