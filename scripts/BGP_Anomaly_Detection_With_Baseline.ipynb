{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# BGP Anomaly Detection with Normal Baseline\n",
    "\n",
    "## The Correct Approach for Anomaly Detection\n",
    "\n",
    "**Problem with previous approach:**\n",
    "- Unsupervised methods find outliers *within* the provided data\n",
    "- If you only provide attack data, they find \"unusual attacks\" not \"attacks vs normal\"\n",
    "\n",
    "**Correct approach:**\n",
    "1. Train models on **NORMAL traffic** (learn what normal looks like)\n",
    "2. Score **NEW/UNKNOWN data** against the normal baseline\n",
    "3. High scores = anomalies (deviates from normal)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION\n",
    "# ============================================\n",
    "\n",
    "# NORMAL traffic file (for training the baseline)\n",
    "NORMAL_FILE = \"/path/to/normal_traffic_features.csv\"  # <-- YOUR NORMAL DATA\n",
    "\n",
    "# File to test/score (can be normal, attack, or unknown)\n",
    "TEST_FILE = \"/path/to/test_features.csv\"  # <-- DATA TO CLASSIFY\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = None  # Uses test file directory if None\n",
    "\n",
    "# Label column (if exists in test file)\n",
    "LABEL_COL = 'label'\n",
    "\n",
    "# Threshold for anomaly (percentile of normal data scores)\n",
    "# 95 means: flag as anomaly if score is worse than 95% of normal data\n",
    "ANOMALY_THRESHOLD_PERCENTILE = 95\n",
    "\n",
    "# Random seed\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Save plots\n",
    "SAVE_PLOTS = True\n",
    "\n",
    "print(\"Configuration set\")\n",
    "print(f\"  Normal file: {NORMAL_FILE}\")\n",
    "print(f\"  Test file: {TEST_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-load",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normal data (for training)\n",
    "print(\"Loading NORMAL data (for baseline training)...\")\n",
    "df_normal = pd.read_csv(NORMAL_FILE)\n",
    "print(f\"  Loaded {len(df_normal)} normal samples\")\n",
    "\n",
    "# Load test data (to classify)\n",
    "print(\"\\nLoading TEST data (to classify)...\")\n",
    "df_test = pd.read_csv(TEST_FILE)\n",
    "print(f\"  Loaded {len(df_test)} test samples\")\n",
    "\n",
    "# Show test data label distribution if available\n",
    "if LABEL_COL and LABEL_COL in df_test.columns:\n",
    "    print(f\"\\nTest data labels (ground truth):\")\n",
    "    for label, count in df_test[LABEL_COL].value_counts().items():\n",
    "        print(f\"  {label}: {count} ({count/len(df_test)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features",
   "metadata": {},
   "outputs": [],
   "source": "# Identify feature columns - exclude metadata and previous analysis columns\nMETA_COLS = {\n    # Metadata columns\n    'incident', 'window_start', 'window_end', 'timestamp', 'time',\n    'label', 'label_rule', 'label_refined', 'source', 'collector',\n    # Previous analysis columns (from BGP_Label_Validation_Discovery.ipynb)\n    'iso_forest_score', 'lof_score', 'statistical_score', 'elliptic_score',\n    'cluster', 'anomaly_votes', 'consensus_score', 'discovered_label',\n    # Any other score columns\n    'hdbscan_score', 'predicted_label'\n}\n\ndef get_feature_cols(df):\n    \"\"\"Get only the actual BGP feature columns, excluding metadata and analysis columns.\"\"\"\n    candidates = [c for c in df.columns if c.lower() not in {m.lower() for m in META_COLS}]\n    return df[candidates].select_dtypes(include=[np.number]).columns.tolist()\n\n# Get feature columns from BOTH datasets and use intersection\nnormal_features = set(get_feature_cols(df_normal))\ntest_features = set(get_feature_cols(df_test))\n\n# Use only features present in BOTH datasets\nfeature_cols = sorted(list(normal_features & test_features))\n\nprint(f\"Normal dataset has {len(normal_features)} features\")\nprint(f\"Test dataset has {len(test_features)} features\")\nprint(f\"Using {len(feature_cols)} common features\")\nprint(f\"\\nFeatures: {feature_cols}\")\n\n# Prepare normal data\nX_normal = df_normal[feature_cols].values\nvalid_normal = ~np.isnan(X_normal).any(axis=1)\nX_normal = X_normal[valid_normal]\nprint(f\"\\nNormal samples after removing NaN: {len(X_normal)}\")\n\n# Prepare test data\nX_test = df_test[feature_cols].values\nvalid_test = ~np.isnan(X_test).any(axis=1)\nX_test_clean = X_test[valid_test]\nprint(f\"Test samples after removing NaN: {len(X_test_clean)}\")\n\n# Fit scaler on NORMAL data only\nscaler = RobustScaler()\nX_normal_scaled = scaler.fit_transform(X_normal)\nX_test_scaled = scaler.transform(X_test_clean)  # Transform test with normal's scaler\n\nprint(\"\\nScaler fitted on NORMAL data and applied to test data\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-train",
   "metadata": {},
   "source": [
    "## 2. Train Models on NORMAL Data\n",
    "\n",
    "This is the key difference: we train **only on normal data** to learn what normal looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training anomaly detection models on NORMAL data...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "models = {}\n",
    "normal_scores = {}\n",
    "\n",
    "# 1. Isolation Forest\n",
    "print(\"\\n[1/4] Training Isolation Forest...\")\n",
    "models['IsolationForest'] = IsolationForest(\n",
    "    n_estimators=200,\n",
    "    contamination=0.01,  # Expect very few anomalies in normal data\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "models['IsolationForest'].fit(X_normal_scaled)\n",
    "normal_scores['IsolationForest'] = models['IsolationForest'].decision_function(X_normal_scaled)\n",
    "print(f\"  Normal score range: [{normal_scores['IsolationForest'].min():.3f}, {normal_scores['IsolationForest'].max():.3f}]\")\n",
    "\n",
    "# 2. Local Outlier Factor (novelty detection mode)\n",
    "print(\"\\n[2/4] Training Local Outlier Factor...\")\n",
    "n_neighbors = max(10, min(50, int(np.sqrt(len(X_normal_scaled)))))\n",
    "models['LOF'] = LocalOutlierFactor(\n",
    "    n_neighbors=n_neighbors,\n",
    "    contamination=0.01,\n",
    "    novelty=True  # Important: enables scoring new data\n",
    ")\n",
    "models['LOF'].fit(X_normal_scaled)\n",
    "normal_scores['LOF'] = models['LOF'].decision_function(X_normal_scaled)\n",
    "print(f\"  Normal score range: [{normal_scores['LOF'].min():.3f}, {normal_scores['LOF'].max():.3f}]\")\n",
    "\n",
    "# 3. Elliptic Envelope\n",
    "print(\"\\n[3/4] Training Elliptic Envelope...\")\n",
    "try:\n",
    "    models['EllipticEnvelope'] = EllipticEnvelope(\n",
    "        contamination=0.01,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    models['EllipticEnvelope'].fit(X_normal_scaled)\n",
    "    normal_scores['EllipticEnvelope'] = models['EllipticEnvelope'].decision_function(X_normal_scaled)\n",
    "    print(f\"  Normal score range: [{normal_scores['EllipticEnvelope'].min():.3f}, {normal_scores['EllipticEnvelope'].max():.3f}]\")\n",
    "except Exception as e:\n",
    "    print(f\"  Skipped due to error: {e}\")\n",
    "    models['EllipticEnvelope'] = None\n",
    "\n",
    "# 4. Statistical baseline (mean and std of normal data)\n",
    "print(\"\\n[4/4] Computing Statistical baseline...\")\n",
    "models['Statistical'] = {\n",
    "    'mean': np.mean(X_normal_scaled, axis=0),\n",
    "    'std': np.std(X_normal_scaled, axis=0)\n",
    "}\n",
    "# Mahalanobis-like distance from normal mean\n",
    "normal_scores['Statistical'] = -np.mean(\n",
    "    np.abs((X_normal_scaled - models['Statistical']['mean']) / (models['Statistical']['std'] + 1e-10)), \n",
    "    axis=1\n",
    ")\n",
    "print(f\"  Normal score range: [{normal_scores['Statistical'].min():.3f}, {normal_scores['Statistical'].max():.3f}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All models trained on NORMAL data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-thresholds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute thresholds based on normal data distribution\n",
    "print(f\"Computing anomaly thresholds ({ANOMALY_THRESHOLD_PERCENTILE}th percentile of normal scores)...\")\n",
    "\n",
    "thresholds = {}\n",
    "for method, scores in normal_scores.items():\n",
    "    # For decision_function: higher = more normal, lower = more anomalous\n",
    "    # So we use a LOW percentile as threshold\n",
    "    thresholds[method] = np.percentile(scores, 100 - ANOMALY_THRESHOLD_PERCENTILE)\n",
    "    print(f\"  {method}: threshold = {thresholds[method]:.4f}\")\n",
    "\n",
    "print(\"\\nSamples scoring BELOW these thresholds will be flagged as anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-score",
   "metadata": {},
   "source": [
    "## 3. Score Test Data Against Normal Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "score-test",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scoring TEST data against NORMAL baseline...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_scores = {}\n",
    "test_anomalies = {}\n",
    "\n",
    "# 1. Isolation Forest\n",
    "print(\"\\n[1/4] Scoring with Isolation Forest...\")\n",
    "test_scores['IsolationForest'] = models['IsolationForest'].decision_function(X_test_scaled)\n",
    "test_anomalies['IsolationForest'] = test_scores['IsolationForest'] < thresholds['IsolationForest']\n",
    "print(f\"  Anomalies: {test_anomalies['IsolationForest'].sum()} ({test_anomalies['IsolationForest'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 2. LOF\n",
    "print(\"\\n[2/4] Scoring with LOF...\")\n",
    "test_scores['LOF'] = models['LOF'].decision_function(X_test_scaled)\n",
    "test_anomalies['LOF'] = test_scores['LOF'] < thresholds['LOF']\n",
    "print(f\"  Anomalies: {test_anomalies['LOF'].sum()} ({test_anomalies['LOF'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 3. Elliptic Envelope\n",
    "if models['EllipticEnvelope'] is not None:\n",
    "    print(\"\\n[3/4] Scoring with Elliptic Envelope...\")\n",
    "    test_scores['EllipticEnvelope'] = models['EllipticEnvelope'].decision_function(X_test_scaled)\n",
    "    test_anomalies['EllipticEnvelope'] = test_scores['EllipticEnvelope'] < thresholds['EllipticEnvelope']\n",
    "    print(f\"  Anomalies: {test_anomalies['EllipticEnvelope'].sum()} ({test_anomalies['EllipticEnvelope'].mean()*100:.1f}%)\")\n",
    "\n",
    "# 4. Statistical\n",
    "print(\"\\n[4/4] Scoring with Statistical method...\")\n",
    "test_scores['Statistical'] = -np.mean(\n",
    "    np.abs((X_test_scaled - models['Statistical']['mean']) / (models['Statistical']['std'] + 1e-10)), \n",
    "    axis=1\n",
    ")\n",
    "test_anomalies['Statistical'] = test_scores['Statistical'] < thresholds['Statistical']\n",
    "print(f\"  Anomalies: {test_anomalies['Statistical'].sum()} ({test_anomalies['Statistical'].mean()*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute consensus\n",
    "print(\"Computing consensus across methods...\")\n",
    "\n",
    "n_methods = len(test_anomalies)\n",
    "anomaly_votes = np.zeros(len(X_test_scaled))\n",
    "\n",
    "for method, anomalies in test_anomalies.items():\n",
    "    anomaly_votes += anomalies.astype(int)\n",
    "\n",
    "consensus_score = anomaly_votes / n_methods\n",
    "\n",
    "# Assign labels\n",
    "predicted_labels = np.where(\n",
    "    consensus_score >= 0.75, 'anomaly',\n",
    "    np.where(\n",
    "        consensus_score >= 0.5, 'likely_anomaly',\n",
    "        np.where(\n",
    "            consensus_score >= 0.25, 'uncertain',\n",
    "            'normal'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nPredicted Label Distribution:\")\n",
    "print(\"=\"*50)\n",
    "for label in ['normal', 'uncertain', 'likely_anomaly', 'anomaly']:\n",
    "    count = (predicted_labels == label).sum()\n",
    "    pct = count / len(predicted_labels) * 100\n",
    "    emoji = \"ðŸŸ¢\" if label == 'normal' else \"ðŸ”´\" if 'anomaly' in label else \"ðŸŸ¡\"\n",
    "    print(f\"  {emoji} {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-evaluate",
   "metadata": {},
   "source": [
    "## 4. Evaluate Against Ground Truth (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABEL_COL and LABEL_COL in df_test.columns:\n",
    "    print(\"Evaluating against ground truth labels...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Get ground truth for valid samples\n",
    "    ground_truth = df_test.loc[valid_test, LABEL_COL].values\n",
    "    \n",
    "    # Binary ground truth: normal vs not normal\n",
    "    is_attack_gt = ~np.isin(ground_truth, ['normal', 'Normal', 'NORMAL'])\n",
    "    is_attack_pred = np.isin(predicted_labels, ['anomaly', 'likely_anomaly'])\n",
    "    \n",
    "    # Calculate metrics\n",
    "    TP = np.sum(is_attack_gt & is_attack_pred)\n",
    "    TN = np.sum(~is_attack_gt & ~is_attack_pred)\n",
    "    FP = np.sum(~is_attack_gt & is_attack_pred)\n",
    "    FN = np.sum(is_attack_gt & ~is_attack_pred)\n",
    "    \n",
    "    accuracy = (TP + TN) / len(ground_truth)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nðŸ“Š DETECTION PERFORMANCE:\")\n",
    "    print(f\"  Accuracy:  {accuracy*100:.1f}%\")\n",
    "    print(f\"  Precision: {precision*100:.1f}%\")\n",
    "    print(f\"  Recall:    {recall*100:.1f}%\")\n",
    "    print(f\"  F1-Score:  {f1*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ CONFUSION MATRIX:\")\n",
    "    print(f\"  True Positives (attacks detected):  {TP}\")\n",
    "    print(f\"  True Negatives (normal correct):    {TN}\")\n",
    "    print(f\"  False Positives (false alarms):     {FP}\")\n",
    "    print(f\"  False Negatives (missed attacks):   {FN}\")\n",
    "    \n",
    "    # Per-class detection rates\n",
    "    print(f\"\\nðŸ“ˆ PER-CLASS DETECTION RATES:\")\n",
    "    for label in df_test[LABEL_COL].unique():\n",
    "        mask = ground_truth == label\n",
    "        if mask.sum() > 0:\n",
    "            detected = is_attack_pred[mask].sum()\n",
    "            rate = detected / mask.sum() * 100\n",
    "            emoji = \"âœ…\" if rate > 80 else \"âš ï¸\" if rate > 50 else \"âŒ\"\n",
    "            print(f\"  {emoji} {label}: {detected}/{mask.sum()} detected ({rate:.1f}%)\")\n",
    "else:\n",
    "    print(\"No ground truth labels available for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method, scores) in enumerate(test_scores.items()):\n",
    "    if i >= 4:\n",
    "        break\n",
    "    \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot normal scores distribution\n",
    "    ax.hist(normal_scores[method], bins=50, alpha=0.5, label='Normal (training)', color='green', density=True)\n",
    "    \n",
    "    # Plot test scores distribution\n",
    "    ax.hist(scores, bins=50, alpha=0.5, label='Test data', color='blue', density=True)\n",
    "    \n",
    "    # Mark threshold\n",
    "    ax.axvline(x=thresholds[method], color='red', linestyle='--', linewidth=2, label=f'Threshold')\n",
    "    \n",
    "    ax.set_xlabel('Anomaly Score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'{method}\\n(Anomalies: {test_anomalies[method].mean()*100:.1f}%)')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plot saved to: score_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df = df_test.loc[valid_test].copy()\n",
    "\n",
    "# Add scores\n",
    "for method, scores in test_scores.items():\n",
    "    results_df[f'{method}_score'] = scores\n",
    "    results_df[f'{method}_anomaly'] = test_anomalies[method]\n",
    "\n",
    "results_df['consensus_score'] = consensus_score\n",
    "results_df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Save\n",
    "output_path = Path(TEST_FILE).parent / f\"{Path(TEST_FILE).stem}_anomaly_detection.csv\"\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements the **correct** approach for anomaly detection:\n",
    "\n",
    "1. **Train on NORMAL data** - Learn what normal traffic looks like\n",
    "2. **Compute thresholds** - Based on normal data distribution\n",
    "3. **Score test data** - Measure how different from normal\n",
    "4. **Flag anomalies** - Samples that deviate significantly from normal\n",
    "\n",
    "This approach will correctly identify attacks as anomalies because they differ from the normal baseline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}