{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header-intro",
   "metadata": {},
   "source": [
    "# BGP Label Validation & Anomaly Discovery\n",
    "\n",
    "## Purpose\n",
    "This notebook helps you **validate and discover labels** in your BGP/RIPE data when you don't have ground truth.\n",
    "\n",
    "## The Problem\n",
    "You have RIPE data that you assume is \"normal\", but you don't actually know:\n",
    "- Is it truly normal traffic?\n",
    "- Does it contain hidden anomalies or attacks?\n",
    "- How can you trust your labels?\n",
    "\n",
    "## The Solution\n",
    "We use **multiple unsupervised methods** and **consensus voting** to:\n",
    "1. Discover natural clusters in your data\n",
    "2. Find anomalies using 5 different algorithms\n",
    "3. Generate confidence-based labels\n",
    "4. Visualize the results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy import stats\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - MODIFY THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "# Path to your features CSV file\n",
    "INPUT_FILE = \"/path/to/your/features.csv\"  # <-- CHANGE THIS\n",
    "\n",
    "# Output directory (leave None to save in same directory as input)\n",
    "OUTPUT_DIR = None\n",
    "\n",
    "# Estimated anomaly rate (0.1 = 10%)\n",
    "# If you have no idea, start with 0.1\n",
    "CONTAMINATION_ESTIMATE = 0.10\n",
    "\n",
    "# Label column name (if you have existing labels)\n",
    "EXISTING_LABEL_COL = 'label'  # Set to None if no existing labels\n",
    "\n",
    "# Time column name (for temporal analysis)\n",
    "TIME_COL = 'window_start'  # Set to None if no time column\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"  Input file: {INPUT_FILE}\")\n",
    "print(f\"  Contamination estimate: {CONTAMINATION_ESTIMATE*100:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "print(f\"Loaded {len(df)} samples\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explore-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Data Overview:\")\n",
    "print(\"=\"*60)\n",
    "df.info()\n",
    "print(\"\\n\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing labels if present\n",
    "if EXISTING_LABEL_COL and EXISTING_LABEL_COL in df.columns:\n",
    "    print(f\"Existing label distribution ({EXISTING_LABEL_COL}):\")\n",
    "    print(\"=\"*40)\n",
    "    label_counts = df[EXISTING_LABEL_COL].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    label_counts.plot(kind='bar', color=sns.color_palette(\"husl\", len(label_counts)))\n",
    "    plt.title('Existing Label Distribution')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No existing labels found - this is fine!\")\n",
    "    print(\"We will discover labels from the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Prepare Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (exclude metadata)\n",
    "META_COLS = {\n",
    "    'incident', 'window_start', 'window_end', 'timestamp', 'time',\n",
    "    'label', 'label_rule', 'label_refined', 'label_discovered',\n",
    "    'cluster', 'anomaly_score', 'source', 'collector'\n",
    "}\n",
    "\n",
    "# Get numeric columns that are not metadata\n",
    "candidate_cols = [c for c in df.columns if c.lower() not in {m.lower() for m in META_COLS}]\n",
    "feature_cols = df[candidate_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Identified {len(feature_cols)} feature columns:\")\n",
    "for i, col in enumerate(feature_cols):\n",
    "    print(f\"  {i+1}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix\n",
    "X = df[feature_cols].values\n",
    "\n",
    "# Handle missing values\n",
    "valid_mask = ~np.isnan(X).any(axis=1)\n",
    "X_valid = X[valid_mask]\n",
    "\n",
    "print(f\"Valid samples: {len(X_valid)} / {len(X)}\")\n",
    "print(f\"Excluded due to NaN: {(~valid_mask).sum()}\")\n",
    "\n",
    "# Scale features (RobustScaler is robust to outliers)\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X_valid)\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": "## 4. Hyperparameter Tuning & Explanation\n\nBefore running anomaly detection, let's understand and tune the key parameters.\n\n### Parameter Reference Table\n\n| Parameter | Default | Range | How to Choose |\n|-----------|---------|-------|---------------|\n| `n_estimators` (IF) | 200 | 100-500 | More = stable but slower. 200 is usually enough |\n| `n_neighbors` (LOF) | 20 | 5-50 | ‚àön_samples or use tuning below |\n| `IQR multiplier` | 1.5 | 1.5-3.0 | 1.5=standard, 3.0=very conservative |\n| `PCA components` | 10 | 5-20 | Preserve ~95% variance |\n| `DBSCAN eps` | auto | varies | Use k-distance graph |\n| `min_samples` (DBSCAN) | 5 | 3-10 | Higher = fewer clusters |"
  },
  {
   "cell_type": "code",
   "id": "55443jbzme6",
   "source": "# ============================================\n# HYPERPARAMETER CONFIGURATION\n# ============================================\n# These are the key parameters you can tune\n\n# 1. Isolation Forest\nN_ESTIMATORS = 200  # Number of trees (100-500, more = stable but slower)\n\n# 2. Local Outlier Factor  \n# Rule of thumb: sqrt(n_samples) or between 10-50\nN_NEIGHBORS_LOF = max(10, min(50, int(np.sqrt(len(X_valid)))))\n\n# 3. Statistical Outliers\nIQR_MULTIPLIER = 1.5  # Standard=1.5, Conservative=2.0, Very Conservative=3.0\nZ_SCORE_THRESHOLD = 3  # Standard=3, Conservative=2.5\n\n# 4. PCA Components (for DBSCAN)\n# Will be set based on variance explained\n\n# 5. DBSCAN\nMIN_SAMPLES_DBSCAN = 5  # Minimum points to form a cluster\n\nprint(\"Hyperparameters configured:\")\nprint(f\"  Isolation Forest n_estimators: {N_ESTIMATORS}\")\nprint(f\"  LOF n_neighbors: {N_NEIGHBORS_LOF} (based on sqrt({len(X_valid)}))\")\nprint(f\"  IQR multiplier: {IQR_MULTIPLIER}\")\nprint(f\"  Z-score threshold: {Z_SCORE_THRESHOLD}\")\nprint(f\"  DBSCAN min_samples: {MIN_SAMPLES_DBSCAN}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1730vxiw0g2",
   "source": "# Tune LOF n_neighbors using stability analysis\nprint(\"Tuning LOF n_neighbors...\")\n\nneighbor_range = [5, 10, 15, 20, 30, 40, 50]\nneighbor_range = [n for n in neighbor_range if n < len(X_valid) // 2]  # Must be less than n_samples/2\n\nlof_stability = []\nfor n in neighbor_range:\n    lof_temp = LocalOutlierFactor(n_neighbors=n, contamination=CONTAMINATION_ESTIMATE)\n    preds = lof_temp.fit_predict(X_scaled)\n    anomaly_rate = (preds == -1).mean()\n    lof_stability.append((n, anomaly_rate))\n    print(f\"  n_neighbors={n}: {anomaly_rate*100:.1f}% anomalies\")\n\n# Plot\nplt.figure(figsize=(10, 4))\nns, rates = zip(*lof_stability)\nplt.plot(ns, [r*100 for r in rates], 'bo-', markersize=8)\nplt.axhline(y=CONTAMINATION_ESTIMATE*100, color='red', linestyle='--', \n            label=f'Expected: {CONTAMINATION_ESTIMATE*100:.0f}%')\nplt.xlabel('n_neighbors')\nplt.ylabel('Anomaly Rate (%)')\nplt.title('LOF Sensitivity to n_neighbors\\n(Choose value where curve stabilizes)')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# Find most stable value (closest to expected contamination)\nbest_n = min(lof_stability, key=lambda x: abs(x[1] - CONTAMINATION_ESTIMATE))[0]\nprint(f\"\\nRecommended n_neighbors: {best_n}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "erc7ebz2yvr",
   "source": "# Determine optimal PCA components (preserve 95% variance)\nprint(\"Analyzing PCA variance...\")\n\npca_full = PCA()\npca_full.fit(X_scaled)\n\ncumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n\n# Find number of components for 95% variance\nn_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\nn_components_90 = np.argmax(cumulative_variance >= 0.90) + 1\n\nprint(f\"  Components for 90% variance: {n_components_90}\")\nprint(f\"  Components for 95% variance: {n_components_95}\")\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 4))\n\n# Individual variance\naxes[0].bar(range(1, len(pca_full.explained_variance_ratio_)+1), \n            pca_full.explained_variance_ratio_*100)\naxes[0].set_xlabel('Principal Component')\naxes[0].set_ylabel('Variance Explained (%)')\naxes[0].set_title('Variance per Component')\n\n# Cumulative variance\naxes[1].plot(range(1, len(cumulative_variance)+1), cumulative_variance*100, 'b-', linewidth=2)\naxes[1].axhline(y=95, color='red', linestyle='--', label='95% threshold')\naxes[1].axhline(y=90, color='orange', linestyle='--', label='90% threshold')\naxes[1].axvline(x=n_components_95, color='red', linestyle=':', alpha=0.5)\naxes[1].axvline(x=n_components_90, color='orange', linestyle=':', alpha=0.5)\naxes[1].set_xlabel('Number of Components')\naxes[1].set_ylabel('Cumulative Variance (%)')\naxes[1].set_title('Cumulative Variance Explained')\naxes[1].legend()\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Set optimal PCA components\nN_PCA_COMPONENTS = n_components_95\nprint(f\"\\nUsing {N_PCA_COMPONENTS} PCA components (95% variance)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gh1l7kpjxjg",
   "source": "# HDBSCAN doesn't need eps tuning!\n# This cell is now optional - just shows the k-distance graph for reference\n\nprint(\"=\" * 60)\nprint(\"NOTE: We're using HDBSCAN instead of DBSCAN\")\nprint(\"HDBSCAN doesn't require eps tuning - it's automatic!\")\nprint(\"=\" * 60)\nprint(\"\\nSkipping eps tuning (not needed for HDBSCAN)\")\nprint(\"You can delete this cell if you want.\")\n\n# Set a placeholder for compatibility\nDBSCAN_EPS = None  # Not used with HDBSCAN",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tf8bax879ki",
   "source": "## 5. Anomaly Detection Methods (with tuned parameters)\n\nNow we run the 5 methods using the parameters we tuned above.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method-1-iforest",
   "metadata": {},
   "outputs": [],
   "source": "# Method 1: Isolation Forest (using tuned parameters)\nprint(\"[1/5] Running Isolation Forest...\")\n\niso_forest = IsolationForest(\n    n_estimators=N_ESTIMATORS,  # Using tuned parameter\n    contamination=CONTAMINATION_ESTIMATE,\n    random_state=RANDOM_STATE,\n    n_jobs=-1\n)\n\niso_predictions = iso_forest.fit_predict(X_scaled)\niso_scores = iso_forest.decision_function(X_scaled)\n\niso_anomalies = (iso_predictions == -1)\nprint(f\"  Parameters: n_estimators={N_ESTIMATORS}\")\nprint(f\"  Found {iso_anomalies.sum()} anomalies ({iso_anomalies.mean()*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method-2-lof",
   "metadata": {},
   "outputs": [],
   "source": "# Method 2: Local Outlier Factor (using tuned parameters)\nprint(\"[2/5] Running Local Outlier Factor...\")\n\nlof = LocalOutlierFactor(\n    n_neighbors=N_NEIGHBORS_LOF,  # Using tuned parameter\n    contamination=CONTAMINATION_ESTIMATE,\n    novelty=False\n)\n\nlof_predictions = lof.fit_predict(X_scaled)\nlof_scores = lof.negative_outlier_factor_\n\nlof_anomalies = (lof_predictions == -1)\nprint(f\"  Parameters: n_neighbors={N_NEIGHBORS_LOF}\")\nprint(f\"  Found {lof_anomalies.sum()} anomalies ({lof_anomalies.mean()*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method-3-statistical",
   "metadata": {},
   "outputs": [],
   "source": "# Method 3: Statistical Analysis (using tuned parameters)\nprint(\"[3/5] Running Statistical Analysis...\")\n\ndef statistical_outlier_scores(X, z_threshold=3, iqr_mult=1.5):\n    \"\"\"Calculate outlier scores using Z-score and IQR.\"\"\"\n    n_samples, n_features = X.shape\n    outlier_scores = np.zeros(n_samples)\n    \n    for i in range(n_features):\n        col = X[:, i]\n        \n        # Z-score method (tunable threshold)\n        z_scores = np.abs(stats.zscore(col, nan_policy='omit'))\n        z_outliers = z_scores > z_threshold\n        \n        # IQR method (tunable multiplier)\n        Q1, Q3 = np.percentile(col, [25, 75])\n        IQR = Q3 - Q1\n        iqr_outliers = (col < Q1 - iqr_mult * IQR) | (col > Q3 + iqr_mult * IQR)\n        \n        outlier_scores += z_outliers.astype(float) + iqr_outliers.astype(float)\n    \n    return outlier_scores / (2 * n_features)\n\n# Use tuned parameters\nstat_scores = statistical_outlier_scores(X_valid, \n                                          z_threshold=Z_SCORE_THRESHOLD,\n                                          iqr_mult=IQR_MULTIPLIER)\nstat_threshold = np.percentile(stat_scores, 100 * (1 - CONTAMINATION_ESTIMATE))\nstat_anomalies = stat_scores > stat_threshold\n\nprint(f\"  Parameters: Z-score threshold={Z_SCORE_THRESHOLD}, IQR multiplier={IQR_MULTIPLIER}\")\nprint(f\"  Found {stat_anomalies.sum()} anomalies ({stat_anomalies.mean()*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method-4-elliptic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Elliptic Envelope (Robust Covariance)\n",
    "print(\"[4/5] Running Elliptic Envelope...\")\n",
    "\n",
    "try:\n",
    "    elliptic = EllipticEnvelope(\n",
    "        contamination=CONTAMINATION_ESTIMATE,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "    elliptic_predictions = elliptic.fit_predict(X_scaled)\n",
    "    elliptic_scores = elliptic.decision_function(X_scaled)\n",
    "    elliptic_anomalies = (elliptic_predictions == -1)\n",
    "    print(f\"  Found {elliptic_anomalies.sum()} anomalies ({elliptic_anomalies.mean()*100:.1f}%)\")\n",
    "    elliptic_success = True\n",
    "except Exception as e:\n",
    "    print(f\"  Skipped due to error: {e}\")\n",
    "    elliptic_anomalies = np.zeros(len(X_valid), dtype=bool)\n",
    "    elliptic_scores = np.zeros(len(X_valid))\n",
    "    elliptic_success = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method-5-dbscan",
   "metadata": {},
   "outputs": [],
   "source": "# Method 5: HDBSCAN (better than DBSCAN - no eps tuning needed!)\nprint(\"[5/5] Running HDBSCAN Clustering...\")\n\n# Install hdbscan if not available\ntry:\n    import hdbscan\nexcept ImportError:\n    print(\"Installing hdbscan...\")\n    import subprocess\n    subprocess.check_call(['pip', 'install', 'hdbscan', '-q'])\n    import hdbscan\n\nimport gc\n\n# Use PCA with tuned components\npca = PCA(n_components=N_PCA_COMPONENTS)\nX_pca = pca.fit_transform(X_scaled)\nprint(f\"  PCA variance explained: {sum(pca.explained_variance_ratio_)*100:.1f}%\")\n\n# HDBSCAN parameters\nMIN_CLUSTER_SIZE = max(50, int(len(X_pca) * 0.005))  # At least 0.5% of data or 50\nMIN_SAMPLES = MIN_SAMPLES_DBSCAN  # Reuse the parameter we set\n\nprint(f\"  Running HDBSCAN on {len(X_pca)} samples...\")\nprint(f\"  Parameters: min_cluster_size={MIN_CLUSTER_SIZE}, min_samples={MIN_SAMPLES}\")\n\n# HDBSCAN - much better than DBSCAN!\nclusterer = hdbscan.HDBSCAN(\n    min_cluster_size=MIN_CLUSTER_SIZE,\n    min_samples=MIN_SAMPLES,\n    cluster_selection_method='eom',  # Excess of Mass (better for anomaly detection)\n    prediction_data=False,  # Save memory\n    core_dist_n_jobs=-1  # Use all cores\n)\n\nhdbscan_labels = clusterer.fit_predict(X_pca)\n\n# Get outlier scores (probability of being an outlier)\n# Higher score = more likely to be outlier\noutlier_scores = clusterer.outlier_scores_\n\n# Clean up\ngc.collect()\n\n# In HDBSCAN, -1 means noise/outlier\nhdbscan_anomalies = (hdbscan_labels == -1)\nn_clusters = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)\n\nprint(f\"  Found {n_clusters} clusters\")\nprint(f\"  Found {hdbscan_anomalies.sum()} outliers ({hdbscan_anomalies.mean()*100:.1f}%)\")\n\n# Store for later use (HDBSCAN provides better outlier scores)\ndbscan_anomalies = hdbscan_anomalies  # For compatibility with rest of notebook\ndbscan_labels = hdbscan_labels\n\n# Bonus: Show outlier score distribution\nplt.figure(figsize=(10, 4))\nplt.hist(outlier_scores, bins=50, edgecolor='black', alpha=0.7)\nplt.xlabel('HDBSCAN Outlier Score')\nplt.ylabel('Count')\nplt.title('Distribution of Outlier Scores\\\\n(Higher = More likely to be anomaly)')\nplt.axvline(x=np.percentile(outlier_scores, 90), color='red', linestyle='--', \n            label=f'90th percentile: {np.percentile(outlier_scores, 90):.2f}')\nplt.legend()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Compute Consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count votes from all methods\n",
    "n_methods = 5 if elliptic_success else 4\n",
    "\n",
    "anomaly_votes = (\n",
    "    iso_anomalies.astype(int) +\n",
    "    lof_anomalies.astype(int) +\n",
    "    stat_anomalies.astype(int) +\n",
    "    elliptic_anomalies.astype(int) +\n",
    "    dbscan_anomalies.astype(int)\n",
    ")\n",
    "\n",
    "consensus_score = anomaly_votes / n_methods\n",
    "\n",
    "# Assign discovered labels based on consensus\n",
    "discovered_labels = np.where(\n",
    "    consensus_score >= 0.8, 'high_confidence_anomaly',\n",
    "    np.where(\n",
    "        consensus_score >= 0.5, 'likely_anomaly',\n",
    "        np.where(\n",
    "            consensus_score >= 0.2, 'uncertain',\n",
    "            'likely_normal'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Consensus Results:\")\n",
    "print(\"=\"*50)\n",
    "for label in ['likely_normal', 'uncertain', 'likely_anomaly', 'high_confidence_anomaly']:\n",
    "    count = (discovered_labels == label).sum()\n",
    "    pct = count / len(discovered_labels) * 100\n",
    "    emoji = \"üü¢\" if 'normal' in label else \"üî¥\" if 'anomaly' in label else \"üü°\"\n",
    "    print(f\"  {emoji} {label}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-consensus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consensus distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of consensus scores\n",
    "axes[0].hist(consensus_score, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Consensus Score (fraction of methods agreeing)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Distribution of Consensus Scores')\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='Anomaly threshold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Pie chart of discovered labels\n",
    "label_counts = pd.Series(discovered_labels).value_counts()\n",
    "colors = {'likely_normal': '#2ecc71', 'uncertain': '#f1c40f', \n",
    "          'likely_anomaly': '#e74c3c', 'high_confidence_anomaly': '#c0392b'}\n",
    "pie_colors = [colors[l] for l in label_counts.index]\n",
    "axes[1].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', \n",
    "            colors=pie_colors, startangle=90)\n",
    "axes[1].set_title('Discovered Label Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Method Agreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "method-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agreement matrix\n",
    "methods_df = pd.DataFrame({\n",
    "    'Isolation Forest': iso_anomalies,\n",
    "    'Local Outlier Factor': lof_anomalies,\n",
    "    'Statistical': stat_anomalies,\n",
    "    'Elliptic Envelope': elliptic_anomalies,\n",
    "    'DBSCAN': dbscan_anomalies\n",
    "})\n",
    "\n",
    "# Correlation between methods\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation = methods_df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='RdYlGn_r', center=0,\n",
    "            vmin=-1, vmax=1, square=True, linewidths=0.5)\n",
    "plt.title('Agreement Between Anomaly Detection Methods')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPer-method anomaly rates:\")\n",
    "for col in methods_df.columns:\n",
    "    rate = methods_df[col].mean() * 100\n",
    "    print(f\"  {col}: {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. K-Means Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimal-clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of clusters using silhouette score\n",
    "print(\"Finding optimal number of clusters...\")\n",
    "\n",
    "max_clusters = min(10, len(X_scaled) - 1)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(2, max_clusters + 1):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append((k, score))\n",
    "    print(f\"  k={k}: silhouette score = {score:.4f}\")\n",
    "\n",
    "optimal_k = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "print(f\"\\nOptimal number of clusters: {optimal_k}\")\n",
    "\n",
    "# Plot silhouette scores\n",
    "plt.figure(figsize=(10, 5))\n",
    "ks, scores = zip(*silhouette_scores)\n",
    "plt.plot(ks, scores, 'bo-', markersize=10)\n",
    "plt.axvline(x=optimal_k, color='red', linestyle='--', label=f'Optimal k={optimal_k}')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-clustering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform final clustering with optimal k\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=RANDOM_STATE, n_init=10)\n",
    "cluster_labels = kmeans_final.fit_predict(X_scaled)\n",
    "\n",
    "print(f\"Cluster distribution:\")\n",
    "for i in range(optimal_k):\n",
    "    count = (cluster_labels == i).sum()\n",
    "    pct = count / len(cluster_labels) * 100\n",
    "    print(f\"  Cluster {i}: {count} samples ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters using PCA\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: K-Means clusters\n",
    "scatter1 = axes[0].scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, \n",
    "                           cmap='tab10', alpha=0.6, s=30)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "axes[0].set_title('K-Means Clusters')\n",
    "plt.colorbar(scatter1, ax=axes[0], label='Cluster')\n",
    "\n",
    "# Plot 2: Discovered anomalies\n",
    "colors = {'likely_normal': '#2ecc71', 'uncertain': '#f1c40f', \n",
    "          'likely_anomaly': '#e74c3c', 'high_confidence_anomaly': '#c0392b'}\n",
    "for label in ['likely_normal', 'uncertain', 'likely_anomaly', 'high_confidence_anomaly']:\n",
    "    mask = discovered_labels == label\n",
    "    axes[1].scatter(X_2d[mask, 0], X_2d[mask, 1], c=colors[label], \n",
    "                    label=label, alpha=0.6, s=30)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]*100:.1f}%)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]*100:.1f}%)')\n",
    "axes[1].set_title('Discovered Anomalies')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. Feature Analysis: What Makes Anomalies Different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feature-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare features between normal and anomalous samples\n",
    "normal_mask = discovered_labels == 'likely_normal'\n",
    "anomaly_mask = np.isin(discovered_labels, ['high_confidence_anomaly', 'likely_anomaly'])\n",
    "\n",
    "feature_analysis = []\n",
    "\n",
    "for i, col in enumerate(feature_cols):\n",
    "    normal_vals = X_valid[normal_mask, i]\n",
    "    anomaly_vals = X_valid[anomaly_mask, i]\n",
    "    \n",
    "    if len(normal_vals) > 0 and len(anomaly_vals) > 0:\n",
    "        normal_mean = np.mean(normal_vals)\n",
    "        anomaly_mean = np.mean(anomaly_vals)\n",
    "        \n",
    "        # Cohen's d effect size\n",
    "        pooled_std = np.sqrt((np.std(normal_vals)**2 + np.std(anomaly_vals)**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            effect_size = abs(anomaly_mean - normal_mean) / pooled_std\n",
    "        else:\n",
    "            effect_size = 0\n",
    "        \n",
    "        feature_analysis.append({\n",
    "            'feature': col,\n",
    "            'normal_mean': normal_mean,\n",
    "            'anomaly_mean': anomaly_mean,\n",
    "            'difference': anomaly_mean - normal_mean,\n",
    "            'effect_size': effect_size,\n",
    "            'direction': 'higher' if anomaly_mean > normal_mean else 'lower'\n",
    "        })\n",
    "\n",
    "# Sort by effect size\n",
    "feature_analysis = sorted(feature_analysis, key=lambda x: x['effect_size'], reverse=True)\n",
    "\n",
    "print(\"Top 10 Most Discriminative Features:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Feature':<30} {'Effect Size':>12} {'Direction':>10} {'Normal':>10} {'Anomaly':>10}\")\n",
    "print(\"-\"*70)\n",
    "for item in feature_analysis[:10]:\n",
    "    print(f\"{item['feature']:<30} {item['effect_size']:>12.3f} {item['direction']:>10} \"\n",
    "          f\"{item['normal_mean']:>10.2f} {item['anomaly_mean']:>10.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top features\n",
    "top_features = [f['feature'] for f in feature_analysis[:6]]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feat in enumerate(top_features):\n",
    "    feat_idx = feature_cols.index(feat)\n",
    "    \n",
    "    normal_vals = X_valid[normal_mask, feat_idx]\n",
    "    anomaly_vals = X_valid[anomaly_mask, feat_idx]\n",
    "    \n",
    "    axes[i].hist(normal_vals, bins=30, alpha=0.5, label='Normal', color='green', density=True)\n",
    "    axes[i].hist(anomaly_vals, bins=30, alpha=0.5, label='Anomaly', color='red', density=True)\n",
    "    axes[i].set_xlabel(feat)\n",
    "    axes[i].set_ylabel('Density')\n",
    "    axes[i].set_title(f'{feat}\\n(effect size: {feature_analysis[i][\"effect_size\"]:.2f})')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. Temporal Analysis (if time column exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TIME_COL and TIME_COL in df.columns:\n",
    "    # Create results dataframe for valid samples only\n",
    "    results_df = df.loc[valid_mask].copy()\n",
    "    results_df['discovered_label'] = discovered_labels\n",
    "    results_df['consensus_score'] = consensus_score\n",
    "    \n",
    "    # Parse timestamps\n",
    "    results_df[TIME_COL] = pd.to_datetime(results_df[TIME_COL], errors='coerce')\n",
    "    results_df = results_df.dropna(subset=[TIME_COL])\n",
    "    \n",
    "    if len(results_df) > 0:\n",
    "        # Group by day\n",
    "        results_df['date'] = results_df[TIME_COL].dt.date\n",
    "        \n",
    "        daily_stats = results_df.groupby('date').agg({\n",
    "            'consensus_score': ['count', 'mean'],\n",
    "            'discovered_label': lambda x: x.isin(['high_confidence_anomaly', 'likely_anomaly']).sum()\n",
    "        }).reset_index()\n",
    "        daily_stats.columns = ['date', 'total', 'avg_score', 'anomalies']\n",
    "        daily_stats['anomaly_rate'] = daily_stats['anomalies'] / daily_stats['total']\n",
    "        \n",
    "        # Plot temporal distribution\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "        \n",
    "        # Plot 1: Daily anomaly rate\n",
    "        axes[0].bar(range(len(daily_stats)), daily_stats['anomaly_rate'], color='coral')\n",
    "        axes[0].set_ylabel('Anomaly Rate')\n",
    "        axes[0].set_title('Daily Anomaly Rate')\n",
    "        axes[0].axhline(y=0.3, color='red', linestyle='--', label='Suspicious threshold (30%)')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot 2: Daily sample count\n",
    "        axes[1].bar(range(len(daily_stats)), daily_stats['total'], color='steelblue', label='Total')\n",
    "        axes[1].bar(range(len(daily_stats)), daily_stats['anomalies'], color='coral', label='Anomalies')\n",
    "        axes[1].set_xlabel('Day')\n",
    "        axes[1].set_ylabel('Count')\n",
    "        axes[1].set_title('Daily Sample Distribution')\n",
    "        axes[1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Find suspicious periods\n",
    "        suspicious = daily_stats[daily_stats['anomaly_rate'] > 0.3]\n",
    "        if len(suspicious) > 0:\n",
    "            print(\"\\n‚ö†Ô∏è SUSPICIOUS PERIODS (>30% anomaly rate):\")\n",
    "            for _, row in suspicious.iterrows():\n",
    "                print(f\"   {row['date']}: {row['anomalies']}/{row['total']} anomalies ({row['anomaly_rate']*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No suspicious time periods found.\")\n",
    "else:\n",
    "    print(\"No time column available for temporal analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. Compare with Existing Labels (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-labels",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EXISTING_LABEL_COL and EXISTING_LABEL_COL in df.columns:\n",
    "    # Get existing labels for valid samples\n",
    "    existing_labels = df.loc[valid_mask, EXISTING_LABEL_COL].values\n",
    "    \n",
    "    print(\"Comparison: Existing Labels vs Discovered Labels\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'existing': existing_labels,\n",
    "        'discovered': discovered_labels\n",
    "    })\n",
    "    \n",
    "    # Cross-tabulation\n",
    "    cross_tab = pd.crosstab(comparison_df['existing'], comparison_df['discovered'], \n",
    "                            margins=True, normalize='index')\n",
    "    \n",
    "    print(\"\\nNormalized by existing label (row-wise):\")\n",
    "    print(cross_tab.round(3))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cross_tab_counts = pd.crosstab(comparison_df['existing'], comparison_df['discovered'])\n",
    "    sns.heatmap(cross_tab_counts, annot=True, fmt='d', cmap='YlOrRd')\n",
    "    plt.title('Existing Labels vs Discovered Labels')\n",
    "    plt.xlabel('Discovered Label')\n",
    "    plt.ylabel('Existing Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\nüìã RECOMMENDATIONS:\")\n",
    "    for existing_label in df[EXISTING_LABEL_COL].unique():\n",
    "        mask = existing_labels == existing_label\n",
    "        discovered_dist = pd.Series(discovered_labels[mask]).value_counts(normalize=True)\n",
    "        \n",
    "        anomaly_rate = discovered_dist.get('high_confidence_anomaly', 0) + discovered_dist.get('likely_anomaly', 0)\n",
    "        normal_rate = discovered_dist.get('likely_normal', 0)\n",
    "        \n",
    "        if 'normal' in str(existing_label).lower() and anomaly_rate > 0.2:\n",
    "            print(f\"   ‚ö†Ô∏è '{existing_label}': {anomaly_rate*100:.1f}% look anomalous - review these samples!\")\n",
    "        elif 'attack' in str(existing_label).lower() or 'hijack' in str(existing_label).lower():\n",
    "            if normal_rate > 0.3:\n",
    "                print(f\"   ‚ö†Ô∏è '{existing_label}': {normal_rate*100:.1f}% look normal - possible mislabeling!\")\n",
    "            else:\n",
    "                print(f\"   ‚úÖ '{existing_label}': Looks correctly labeled ({anomaly_rate*100:.1f}% detected as anomalous)\")\n",
    "        else:\n",
    "            print(f\"   ‚ÑπÔ∏è '{existing_label}': {normal_rate*100:.1f}% normal, {anomaly_rate*100:.1f}% anomalous\")\n",
    "else:\n",
    "    print(\"No existing labels to compare with.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "## 11. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final results dataframe\n",
    "results_df = df.copy()\n",
    "\n",
    "# Add results for valid samples\n",
    "results_df['iso_forest_score'] = np.nan\n",
    "results_df['lof_score'] = np.nan\n",
    "results_df['statistical_score'] = np.nan\n",
    "results_df['elliptic_score'] = np.nan\n",
    "results_df['cluster'] = -1\n",
    "results_df['anomaly_votes'] = np.nan\n",
    "results_df['consensus_score'] = np.nan\n",
    "results_df['discovered_label'] = 'invalid'\n",
    "\n",
    "results_df.loc[valid_mask, 'iso_forest_score'] = iso_scores\n",
    "results_df.loc[valid_mask, 'lof_score'] = lof_scores\n",
    "results_df.loc[valid_mask, 'statistical_score'] = stat_scores\n",
    "results_df.loc[valid_mask, 'elliptic_score'] = elliptic_scores\n",
    "results_df.loc[valid_mask, 'cluster'] = cluster_labels\n",
    "results_df.loc[valid_mask, 'anomaly_votes'] = anomaly_votes\n",
    "results_df.loc[valid_mask, 'consensus_score'] = consensus_score\n",
    "results_df.loc[valid_mask, 'discovered_label'] = discovered_labels\n",
    "\n",
    "# Save to file\n",
    "input_path = Path(INPUT_FILE)\n",
    "output_dir = Path(OUTPUT_DIR) if OUTPUT_DIR else input_path.parent\n",
    "output_path = output_dir / f\"{input_path.stem}_discovered.csv\"\n",
    "\n",
    "results_df.to_csv(output_path, index=False)\n",
    "print(f\"‚úÖ Results saved to: {output_path}\")\n",
    "\n",
    "# Save anomalies separately\n",
    "anomalies_df = results_df[results_df['discovered_label'].isin(['high_confidence_anomaly', 'likely_anomaly'])]\n",
    "if len(anomalies_df) > 0:\n",
    "    anomalies_path = output_dir / f\"{input_path.stem}_anomalies.csv\"\n",
    "    anomalies_df.to_csv(anomalies_path, index=False)\n",
    "    print(f\"‚úÖ Anomalies saved to: {anomalies_path} ({len(anomalies_df)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12",
   "metadata": {},
   "source": [
    "## 12. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "likely_normal_count = (discovered_labels == 'likely_normal').sum()\n",
    "anomaly_count = np.isin(discovered_labels, ['high_confidence_anomaly', 'likely_anomaly']).sum()\n",
    "uncertain_count = (discovered_labels == 'uncertain').sum()\n",
    "\n",
    "likely_normal_pct = likely_normal_count / len(discovered_labels) * 100\n",
    "anomaly_pct = anomaly_count / len(discovered_labels) * 100\n",
    "\n",
    "print(f\"\\nüìä Data analyzed: {len(df)} samples\")\n",
    "print(f\"   Valid samples: {len(X_valid)}\")\n",
    "print(f\"   Features used: {len(feature_cols)}\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  Discovered labels:\")\n",
    "print(f\"   üü¢ Likely Normal: {likely_normal_count} ({likely_normal_pct:.1f}%)\")\n",
    "print(f\"   üü° Uncertain: {uncertain_count}\")\n",
    "print(f\"   üî¥ Anomalies: {anomaly_count} ({anomaly_pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìã LABELING STRATEGY RECOMMENDATION:\")\n",
    "\n",
    "if likely_normal_pct > 80:\n",
    "    print(\"\"\"\n",
    "    ‚úÖ Your data appears to be PREDOMINANTLY NORMAL (>{:.0f}%).\n",
    "    \n",
    "    Recommended approach:\n",
    "    1. Use 'likely_normal' samples as your NORMAL training data\n",
    "    2. Investigate 'high_confidence_anomaly' samples manually\n",
    "    3. Keep 'uncertain' samples for validation or discard them\n",
    "    4. Cross-reference anomalies with known BGP incidents\n",
    "    \"\"\".format(likely_normal_pct))\n",
    "elif anomaly_pct > 30:\n",
    "    print(\"\"\"\n",
    "    ‚ö†Ô∏è Your data has a HIGH ANOMALY RATE ({:.0f}%).\n",
    "    \n",
    "    This could indicate:\n",
    "    - Data collected during a known BGP incident\n",
    "    - Collector issues or misconfiguration\n",
    "    - Legitimate attack traffic in your dataset\n",
    "    \n",
    "    Recommended approach:\n",
    "    1. DO NOT use this data as \"normal\" baseline without review\n",
    "    2. Check timestamps against known BGP incidents\n",
    "    3. Verify RIPE collector status for the time period\n",
    "    4. Consider using only 'likely_normal' samples for training\n",
    "    \"\"\".format(anomaly_pct))\n",
    "else:\n",
    "    print(\"\"\"\n",
    "    ‚ÑπÔ∏è Your data has a TYPICAL distribution.\n",
    "    \n",
    "    Recommended approach:\n",
    "    1. Use 'likely_normal' samples as NORMAL class\n",
    "    2. Use 'high_confidence_anomaly' as ATTACK class (after verification)\n",
    "    3. Keep 'uncertain' samples for validation or manual review\n",
    "    4. Document your labeling decisions for reproducibility\n",
    "    \"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}