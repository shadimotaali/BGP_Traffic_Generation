{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Enhanced Copula Synthetic Data Generation\n",
    "\n",
    "## Overview\n",
    "This notebook implements an **Enhanced Gaussian/t-Copula** for generating high-quality synthetic BGP traffic data.\n",
    "\n",
    "### Key Improvements Over Basic Copula:\n",
    "1. **Hybrid Correlation** - Uses both Pearson AND Spearman (weighted combination)\n",
    "2. **KDE Marginals** - Better estimation for heavy-tailed features\n",
    "3. **Zero-Inflation Handling** - Preserves sparsity in features like flaps, nadas\n",
    "4. **t-Copula Option** - Heavier tails for extreme value generation\n",
    "5. **Integer Constraints** - Built-in handling for count features\n",
    "6. **BGP Domain Constraints** - Enforces domain-specific rules\n",
    "\n",
    "### Author\n",
    "Enhanced Copula implementation for BGP traffic synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, gaussian_kde, spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths - UPDATE THESE FOR YOUR ENVIRONMENT\n",
    "REAL_DATA_PATH = '../data/likely_normal_traffic.csv'\n",
    "OUTPUT_DIR = '../data/'\n",
    "\n",
    "# Generation settings\n",
    "N_SYNTHETIC = 20000  # Number of samples to generate\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# All 27 BGP features\n",
    "ALL_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_avg', 'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'rare_ases_avg',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Integer features (must be rounded)\n",
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Zero-inflated features (high proportion of zeros)\n",
    "ZERO_INFLATED_FEATURES = ['flaps', 'nadas', 'imp_wd', 'number_rare_ases']\n",
    "\n",
    "# Heavy-tailed features (need special handling)\n",
    "HEAVY_TAILED_FEATURES = ['unique_as_path_max', 'edit_distance_max', 'rare_ases_avg', 'as_path_max']\n",
    "\n",
    "print(\"Configuration loaded!\")\n",
    "print(f\"  Total features: {len(ALL_FEATURES)}\")\n",
    "print(f\"  Integer features: {len(INTEGER_FEATURES)}\")\n",
    "print(f\"  Zero-inflated features: {len(ZERO_INFLATED_FEATURES)}\")\n",
    "print(f\"  Heavy-tailed features: {len(HEAVY_TAILED_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD REAL DATA\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    df_real = pd.read_csv(REAL_DATA_PATH)\n",
    "    print(f\"Loaded real data: {df_real.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {REAL_DATA_PATH}\")\n",
    "    print(\"Please update REAL_DATA_PATH to your data location\")\n",
    "    raise\n",
    "\n",
    "# Filter to feature columns only\n",
    "available_features = [f for f in ALL_FEATURES if f in df_real.columns]\n",
    "X_real = df_real[available_features].copy()\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} features\")\n",
    "print(f\"Real data shape: {X_real.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "X_real.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Feature Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE FEATURE CHARACTERISTICS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_features(X):\n",
    "    \"\"\"Analyze each feature's characteristics for optimal copula configuration.\"\"\"\n",
    "    analysis = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        vals = X[col].dropna().values\n",
    "        \n",
    "        # Basic stats\n",
    "        zero_ratio = (vals == 0).mean()\n",
    "        unique_ratio = len(np.unique(vals)) / len(vals)\n",
    "        skewness = stats.skew(vals)\n",
    "        kurtosis = stats.kurtosis(vals)\n",
    "        \n",
    "        # Determine characteristics\n",
    "        is_zero_inflated = zero_ratio > 0.2\n",
    "        is_heavy_tailed = kurtosis > 3 or skewness > 2\n",
    "        is_discrete = unique_ratio < 0.1 or col in INTEGER_FEATURES\n",
    "        \n",
    "        analysis.append({\n",
    "            'feature': col,\n",
    "            'zero_ratio': zero_ratio,\n",
    "            'unique_ratio': unique_ratio,\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis,\n",
    "            'is_zero_inflated': is_zero_inflated,\n",
    "            'is_heavy_tailed': is_heavy_tailed,\n",
    "            'is_discrete': is_discrete\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(analysis)\n",
    "\n",
    "feature_analysis = analyze_features(X_real)\n",
    "\n",
    "print(\"Feature Analysis Summary:\")\n",
    "print(f\"  Zero-inflated features: {feature_analysis['is_zero_inflated'].sum()}\")\n",
    "print(f\"  Heavy-tailed features: {feature_analysis['is_heavy_tailed'].sum()}\")\n",
    "print(f\"  Discrete features: {feature_analysis['is_discrete'].sum()}\")\n",
    "\n",
    "# Show problematic features\n",
    "print(\"\\nFeatures needing special handling:\")\n",
    "problematic = feature_analysis[\n",
    "    feature_analysis['is_zero_inflated'] | feature_analysis['is_heavy_tailed']\n",
    "][['feature', 'zero_ratio', 'skewness', 'kurtosis']]\n",
    "print(problematic.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Copula Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYBRID CORRELATION (PEARSON + SPEARMAN)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_hybrid_correlation(X, pearson_weight=0.3, spearman_weight=0.7):\n",
    "    \"\"\"\n",
    "    Compute hybrid correlation matrix using both Pearson and Spearman.\n",
    "    \n",
    "    Why use both?\n",
    "    - Pearson: Captures linear relationships, sensitive to outliers\n",
    "    - Spearman: Captures monotonic (non-linear) relationships, robust to outliers\n",
    "    \n",
    "    For BGP data with heavy tails and non-linear relationships,\n",
    "    Spearman should have higher weight.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : DataFrame - Input data\n",
    "    pearson_weight : float - Weight for Pearson correlation (default 0.3)\n",
    "    spearman_weight : float - Weight for Spearman correlation (default 0.7)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray - Hybrid correlation matrix\n",
    "    \"\"\"\n",
    "    assert abs(pearson_weight + spearman_weight - 1.0) < 1e-6, \"Weights must sum to 1\"\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    pearson_corr = X.corr(method='pearson').values\n",
    "    \n",
    "    # Compute Spearman correlation\n",
    "    spearman_corr = X.corr(method='spearman').values\n",
    "    \n",
    "    # Handle NaN values\n",
    "    pearson_corr = np.nan_to_num(pearson_corr, nan=0.0)\n",
    "    spearman_corr = np.nan_to_num(spearman_corr, nan=0.0)\n",
    "    \n",
    "    # Weighted combination\n",
    "    hybrid_corr = pearson_weight * pearson_corr + spearman_weight * spearman_corr\n",
    "    \n",
    "    # Ensure diagonal is 1\n",
    "    np.fill_diagonal(hybrid_corr, 1.0)\n",
    "    \n",
    "    # Ensure positive definiteness\n",
    "    eigvals, eigvecs = np.linalg.eigh(hybrid_corr)\n",
    "    eigvals = np.maximum(eigvals, 1e-6)\n",
    "    hybrid_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "    \n",
    "    # Normalize to correlation matrix\n",
    "    d = np.sqrt(np.diag(hybrid_corr))\n",
    "    hybrid_corr = hybrid_corr / np.outer(d, d)\n",
    "    np.fill_diagonal(hybrid_corr, 1.0)\n",
    "    \n",
    "    return hybrid_corr\n",
    "\n",
    "print(\"Hybrid correlation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# MARGINAL ESTIMATION (KDE + EMPIRICAL)\n# =============================================================================\n\nclass MarginalEstimator:\n    \"\"\"\n    Estimates marginal distributions using adaptive methods.\n    \n    - KDE for continuous features with many unique values\n    - Empirical CDF for discrete/low-cardinality features\n    - Special handling for zero-inflated features\n    \"\"\"\n    \n    def __init__(self, use_kde=True, handle_zeros=True, log_transform_heavy=True, kde_bandwidth_factor=1.0):\n        self.use_kde = use_kde\n        self.handle_zeros = handle_zeros\n        self.log_transform_heavy = log_transform_heavy\n        self.kde_bandwidth_factor = kde_bandwidth_factor  # <1 = narrower/sharper, >1 = wider/smoother\n        self.marginals = {}\n    \n    def fit(self, X, feature_analysis=None):\n        \"\"\"\n        Fit marginal distributions for each feature.\n        \"\"\"\n        self.feature_names = X.columns.tolist()\n        self.n_features = len(self.feature_names)\n        \n        for col in self.feature_names:\n            vals = X[col].values.copy()\n            \n            # Analyze feature\n            zero_ratio = (vals == 0).mean()\n            n_unique = len(np.unique(vals))\n            is_heavy = col in HEAVY_TAILED_FEATURES or (vals.max() > 100 and stats.skew(vals) > 2)\n            \n            marginal_info = {\n                'zero_ratio': zero_ratio,\n                'is_zero_inflated': zero_ratio > 0.2,\n                'is_heavy_tailed': is_heavy,\n                'is_integer': col in INTEGER_FEATURES,\n                'original_values': vals,\n                'sorted_values': np.sort(vals),\n                'method': 'empirical'  # default\n            }\n            \n            # Choose estimation method\n            if self.use_kde and n_unique > 20 and not marginal_info['is_integer']:\n                try:\n                    # Apply bandwidth factor to Scott's rule\n                    bw_method = lambda obj: obj.scotts_factor() * self.kde_bandwidth_factor\n                    \n                    if is_heavy and self.log_transform_heavy and vals.min() >= 0:\n                        # Log-transform for heavy-tailed\n                        log_vals = np.log1p(vals)\n                        kde = gaussian_kde(log_vals, bw_method=bw_method)\n                        marginal_info['kde'] = kde\n                        marginal_info['method'] = 'kde_log'\n                        marginal_info['log_sorted'] = np.sort(log_vals)\n                    else:\n                        kde = gaussian_kde(vals, bw_method=bw_method)\n                        marginal_info['kde'] = kde\n                        marginal_info['method'] = 'kde'\n                except:\n                    pass  # Fall back to empirical\n            \n            self.marginals[col] = marginal_info\n        \n        return self\n    \n    def transform_to_uniform(self, X):\n        \"\"\"\n        Transform data to uniform [0,1] using fitted marginals.\n        \"\"\"\n        n_samples = len(X)\n        uniform_data = np.zeros((n_samples, self.n_features))\n        \n        for i, col in enumerate(self.feature_names):\n            vals = X[col].values\n            info = self.marginals[col]\n            \n            # Use empirical CDF (rank-based)\n            ranks = stats.rankdata(vals, method='average')\n            uniform_data[:, i] = np.clip(ranks / (n_samples + 1), 0.001, 0.999)\n        \n        return uniform_data\n    \n    def inverse_transform(self, uniform_data, n_samples):\n        \"\"\"\n        Transform uniform samples back to original scale.\n        \"\"\"\n        synthetic_data = np.zeros((n_samples, self.n_features))\n        \n        for i, col in enumerate(self.feature_names):\n            info = self.marginals[col]\n            u = uniform_data[:, i]\n            \n            if self.handle_zeros and info['is_zero_inflated']:\n                # Handle zero-inflated features\n                synthetic_data[:, i] = self._inverse_zero_inflated(u, info)\n            else:\n                # Standard inverse CDF\n                synthetic_data[:, i] = self._inverse_standard(u, info)\n            \n            # Enforce non-negativity\n            synthetic_data[:, i] = np.maximum(0, synthetic_data[:, i])\n            \n            # Enforce integer constraint\n            if info['is_integer']:\n                synthetic_data[:, i] = np.round(synthetic_data[:, i])\n        \n        return synthetic_data\n    \n    def _inverse_standard(self, u, info):\n        \"\"\"Standard inverse CDF using quantile function.\"\"\"\n        sorted_vals = info['sorted_values']\n        indices = (u * len(sorted_vals)).astype(int)\n        indices = np.clip(indices, 0, len(sorted_vals) - 1)\n        return sorted_vals[indices]\n    \n    def _inverse_zero_inflated(self, u, info):\n        \"\"\"Inverse CDF for zero-inflated features.\"\"\"\n        zero_ratio = info['zero_ratio']\n        original = info['original_values']\n        \n        result = np.zeros(len(u))\n        \n        # Samples with u < zero_ratio become zeros\n        is_zero = u < zero_ratio\n        \n        # For non-zero samples, map to non-zero distribution\n        non_zero_vals = original[original > 0]\n        if len(non_zero_vals) > 0:\n            # Scale u to [0,1] for non-zero part\n            scaled_u = (u[~is_zero] - zero_ratio) / (1 - zero_ratio + 1e-10)\n            scaled_u = np.clip(scaled_u, 0, 1)\n            \n            sorted_non_zero = np.sort(non_zero_vals)\n            indices = (scaled_u * len(sorted_non_zero)).astype(int)\n            indices = np.clip(indices, 0, len(sorted_non_zero) - 1)\n            result[~is_zero] = sorted_non_zero[indices]\n        \n        return result\n\nprint(\"Marginal Estimator class defined (with bandwidth tuning)!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# ENHANCED COPULA GENERATOR\n# =============================================================================\n\nclass EnhancedCopulaGenerator:\n    \"\"\"\n    Enhanced Copula Generator with multiple improvements:\n    \n    1. Hybrid correlation (Pearson + Spearman)\n    2. Adaptive marginal estimation (KDE + Empirical)\n    3. Zero-inflation handling\n    4. t-Copula option for heavy tails\n    5. Built-in constraint enforcement\n    6. KDE bandwidth tuning for fine-grained control\n    \"\"\"\n    \n    def __init__(self, \n                 copula_type='gaussian',  # 'gaussian' or 't'\n                 t_df=5,                  # degrees of freedom for t-copula\n                 pearson_weight=0.3,      # weight for Pearson correlation\n                 spearman_weight=0.7,     # weight for Spearman correlation\n                 use_kde_marginals=True,\n                 kde_bandwidth_factor=1.0,  # <1 = narrower/sharper, >1 = wider/smoother\n                 handle_zeros=True,\n                 random_state=42):\n        \n        self.copula_type = copula_type\n        self.t_df = t_df\n        self.pearson_weight = pearson_weight\n        self.spearman_weight = spearman_weight\n        self.use_kde_marginals = use_kde_marginals\n        self.kde_bandwidth_factor = kde_bandwidth_factor\n        self.handle_zeros = handle_zeros\n        self.random_state = random_state\n        \n        self.marginal_estimator = None\n        self.correlation_matrix = None\n        self.cholesky_L = None\n    \n    def fit(self, X):\n        \"\"\"\n        Fit the copula model to real data.\n        \"\"\"\n        print(\"Fitting Enhanced Copula...\")\n        print(f\"  Configuration:\")\n        print(f\"    - Copula type: {self.copula_type}\" + \n              (f\" (df={self.t_df})\" if self.copula_type == 't' else \"\"))\n        print(f\"    - Correlation: {self.pearson_weight:.0%} Pearson + {self.spearman_weight:.0%} Spearman\")\n        print(f\"    - Marginals: {'KDE' if self.use_kde_marginals else 'Empirical'}\" +\n              (f\" (bw={self.kde_bandwidth_factor}x)\" if self.use_kde_marginals else \"\"))\n        print(f\"    - Zero handling: {self.handle_zeros}\")\n        \n        self.feature_names = X.columns.tolist()\n        self.n_features = len(self.feature_names)\n        \n        # Step 1: Fit marginal distributions\n        print(\"\\n  [1/3] Fitting marginal distributions...\")\n        self.marginal_estimator = MarginalEstimator(\n            use_kde=self.use_kde_marginals,\n            handle_zeros=self.handle_zeros,\n            kde_bandwidth_factor=self.kde_bandwidth_factor\n        )\n        self.marginal_estimator.fit(X)\n        \n        # Step 2: Compute hybrid correlation matrix\n        print(\"  [2/3] Computing hybrid correlation matrix...\")\n        self.correlation_matrix = compute_hybrid_correlation(\n            X, \n            pearson_weight=self.pearson_weight,\n            spearman_weight=self.spearman_weight\n        )\n        \n        # Step 3: Compute Cholesky decomposition\n        print(\"  [3/3] Computing Cholesky decomposition...\")\n        try:\n            self.cholesky_L = np.linalg.cholesky(self.correlation_matrix)\n        except np.linalg.LinAlgError:\n            # Add regularization if needed\n            self.correlation_matrix += 0.01 * np.eye(self.n_features)\n            self.cholesky_L = np.linalg.cholesky(self.correlation_matrix)\n        \n        print(\"  Fitting complete!\")\n        return self\n    \n    def generate(self, n_samples):\n        \"\"\"\n        Generate synthetic samples.\n        \"\"\"\n        np.random.seed(self.random_state)\n        \n        print(f\"\\nGenerating {n_samples} samples...\")\n        \n        # Step 1: Generate independent samples in copula space\n        if self.copula_type == 'gaussian':\n            independent_samples = np.random.randn(n_samples, self.n_features)\n        else:  # t-copula\n            # Multivariate t: Z / sqrt(chi2/df)\n            chi2_samples = np.random.chisquare(self.t_df, n_samples) / self.t_df\n            independent_samples = np.random.randn(n_samples, self.n_features)\n            independent_samples = independent_samples / np.sqrt(chi2_samples)[:, np.newaxis]\n        \n        # Step 2: Induce correlation via Cholesky\n        correlated_samples = independent_samples @ self.cholesky_L.T\n        \n        # Step 3: Transform to uniform via CDF\n        if self.copula_type == 'gaussian':\n            uniform_samples = stats.norm.cdf(correlated_samples)\n        else:\n            uniform_samples = stats.t.cdf(correlated_samples, df=self.t_df)\n        \n        # Step 4: Transform to original scale via inverse marginals\n        synthetic_data = self.marginal_estimator.inverse_transform(uniform_samples, n_samples)\n        \n        # Create DataFrame\n        synthetic_df = pd.DataFrame(synthetic_data, columns=self.feature_names)\n        \n        print(f\"  Generated shape: {synthetic_df.shape}\")\n        \n        return synthetic_df\n    \n    def get_config(self):\n        \"\"\"Return configuration as dictionary.\"\"\"\n        return {\n            'copula_type': self.copula_type,\n            't_df': self.t_df,\n            'pearson_weight': self.pearson_weight,\n            'spearman_weight': self.spearman_weight,\n            'use_kde_marginals': self.use_kde_marginals,\n            'kde_bandwidth_factor': self.kde_bandwidth_factor,\n            'handle_zeros': self.handle_zeros\n        }\n\nprint(\"EnhancedCopulaGenerator class defined (with bandwidth tuning)!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-Processing & Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BGP CONSTRAINT ENFORCEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def enforce_bgp_constraints(synthetic, real, verbose=True):\n",
    "    \"\"\"\n",
    "    Enforce BGP domain-specific constraints.\n",
    "    \n",
    "    Constraints:\n",
    "    1. All features non-negative\n",
    "    2. Integer features properly rounded\n",
    "    3. origin_0 + origin_2 <= announcements\n",
    "    4. edit_distance_dict_i = 0 when i > edit_distance_max\n",
    "    5. Values within realistic bounds\n",
    "    \"\"\"\n",
    "    result = synthetic.copy()\n",
    "    \n",
    "    violations = {\n",
    "        'nan_inf': 0,\n",
    "        'negative': 0,\n",
    "        'origin_constraint': 0,\n",
    "        'edit_distance_constraint': 0,\n",
    "        'bounds': 0\n",
    "    }\n",
    "    \n",
    "    # 1. Handle NaN/Inf\n",
    "    for col in result.columns:\n",
    "        nan_count = result[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            violations['nan_inf'] += nan_count\n",
    "            if col in INTEGER_FEATURES:\n",
    "                result[col] = result[col].fillna(0)\n",
    "            else:\n",
    "                result[col] = result[col].fillna(real[col].median() if col in real.columns else 0)\n",
    "        \n",
    "        # Replace inf\n",
    "        result[col] = result[col].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # 2. Non-negative\n",
    "    for col in result.columns:\n",
    "        neg_count = (result[col] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            violations['negative'] += neg_count\n",
    "            result[col] = np.maximum(0, result[col])\n",
    "    \n",
    "    # 3. Integer features\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = np.round(result[col]).astype(int)\n",
    "    \n",
    "    # 4. Origin constraint\n",
    "    if all(c in result.columns for c in ['origin_0', 'origin_2', 'announcements']):\n",
    "        origin_sum = result['origin_0'] + result['origin_2']\n",
    "        excess = origin_sum > result['announcements']\n",
    "        violations['origin_constraint'] = excess.sum()\n",
    "        \n",
    "        if excess.any():\n",
    "            scale = result.loc[excess, 'announcements'] / (origin_sum[excess] + 1e-10)\n",
    "            result.loc[excess, 'origin_0'] = np.floor(result.loc[excess, 'origin_0'] * scale).astype(int)\n",
    "            result.loc[excess, 'origin_2'] = np.floor(result.loc[excess, 'origin_2'] * scale).astype(int)\n",
    "    \n",
    "    # 5. Realistic bounds (99.5th percentile)\n",
    "    for col in result.columns:\n",
    "        if col in real.columns:\n",
    "            upper = np.percentile(real[col].dropna(), 99.5) * 1.1\n",
    "            over = (result[col] > upper).sum()\n",
    "            if over > 0:\n",
    "                violations['bounds'] += over\n",
    "                result[col] = np.clip(result[col], 0, upper)\n",
    "    \n",
    "    # Final integer enforcement\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = np.maximum(0, np.round(result[col])).astype(int)\n",
    "    \n",
    "    if verbose:\n",
    "        total = sum(violations.values())\n",
    "        if total > 0:\n",
    "            print(f\"  Constraints fixed: {violations}\")\n",
    "        else:\n",
    "            print(\"  No constraint violations\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Constraint enforcement function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CORRELATION ALIGNMENT POST-PROCESSING\n# =============================================================================\n\ndef align_correlation_structure(synthetic, real, n_iterations=10, learning_rate=0.3, verbose=True):\n    \"\"\"\n    Post-process synthetic data to better match real data's correlation structure.\n    \n    This uses an iterative approach:\n    1. Standardize synthetic data\n    2. Compute current vs target correlation\n    3. Apply Cholesky-based adjustment toward target\n    4. Restore original marginals\n    \n    Parameters:\n    -----------\n    synthetic : DataFrame - Generated synthetic data\n    real : DataFrame - Real reference data\n    n_iterations : int - Number of alignment iterations\n    learning_rate : float - How aggressively to adjust (0-1, higher = faster but less stable)\n    verbose : bool - Print progress\n    \n    Returns:\n    --------\n    DataFrame - Correlation-aligned synthetic data\n    \"\"\"\n    result = synthetic.copy()\n    features = list(result.columns)\n    n_features = len(features)\n    \n    # Store original marginals (we'll restore them after correlation adjustment)\n    original_ranks = {}\n    for col in features:\n        original_ranks[col] = stats.rankdata(result[col].values)\n    \n    # Target correlation from real data\n    target_corr = real[features].corr(method='pearson').values\n    target_corr = np.nan_to_num(target_corr, nan=0.0)\n    np.fill_diagonal(target_corr, 1.0)\n    \n    # Ensure positive definiteness of target\n    eigvals, eigvecs = np.linalg.eigh(target_corr)\n    eigvals = np.maximum(eigvals, 1e-6)\n    target_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n    d = np.sqrt(np.diag(target_corr))\n    target_corr = target_corr / np.outer(d, d)\n    np.fill_diagonal(target_corr, 1.0)\n    \n    # Compute target Cholesky\n    try:\n        target_L = np.linalg.cholesky(target_corr)\n    except:\n        target_corr += 0.01 * np.eye(n_features)\n        target_L = np.linalg.cholesky(target_corr)\n    \n    if verbose:\n        # Initial correlation match\n        current_corr = result[features].corr(method='pearson').values\n        current_flat = current_corr[np.triu_indices(n_features, k=1)]\n        target_flat = target_corr[np.triu_indices(n_features, k=1)]\n        initial_match = np.corrcoef(current_flat, target_flat)[0, 1]\n        print(f\"Correlation Alignment: Initial match = {initial_match:.4f}\")\n    \n    # Standardize data\n    data = result[features].values.copy()\n    means = data.mean(axis=0)\n    stds = data.std(axis=0)\n    stds[stds == 0] = 1  # Avoid division by zero\n    Z = (data - means) / stds\n    \n    for iteration in range(n_iterations):\n        # Current correlation\n        current_corr = np.corrcoef(Z.T)\n        current_corr = np.nan_to_num(current_corr, nan=0.0)\n        np.fill_diagonal(current_corr, 1.0)\n        \n        # Ensure positive definiteness\n        eigvals, eigvecs = np.linalg.eigh(current_corr)\n        eigvals = np.maximum(eigvals, 1e-6)\n        current_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n        d = np.sqrt(np.diag(current_corr))\n        current_corr = current_corr / np.outer(d, d)\n        np.fill_diagonal(current_corr, 1.0)\n        \n        try:\n            current_L = np.linalg.cholesky(current_corr)\n        except:\n            current_corr += 0.01 * np.eye(n_features)\n            current_L = np.linalg.cholesky(current_corr)\n        \n        # Compute adjustment: blend current toward target Cholesky\n        # Z_new = Z @ current_L^{-1} @ blended_L\n        current_L_inv = np.linalg.inv(current_L)\n        blended_L = (1 - learning_rate) * current_L + learning_rate * target_L\n        \n        # Apply transformation\n        Z = Z @ current_L_inv.T @ blended_L.T\n        \n        # Re-standardize to avoid scale drift\n        Z = (Z - Z.mean(axis=0)) / (Z.std(axis=0) + 1e-10)\n    \n    # Restore original marginal distributions using rank matching\n    adjusted_data = np.zeros_like(Z)\n    for i, col in enumerate(features):\n        # Get ranks of adjusted data\n        adjusted_ranks = stats.rankdata(Z[:, i])\n        \n        # Map back to original values using rank order\n        sorted_original = np.sort(result[col].values)\n        rank_indices = ((adjusted_ranks - 1) / (len(adjusted_ranks) - 1) * (len(sorted_original) - 1)).astype(int)\n        rank_indices = np.clip(rank_indices, 0, len(sorted_original) - 1)\n        adjusted_data[:, i] = sorted_original[rank_indices]\n    \n    # Create result DataFrame\n    aligned_result = pd.DataFrame(adjusted_data, columns=features)\n    \n    if verbose:\n        # Final correlation match\n        final_corr = aligned_result[features].corr(method='pearson').values\n        final_flat = final_corr[np.triu_indices(n_features, k=1)]\n        final_match = np.corrcoef(final_flat, target_flat)[0, 1]\n        improvement = final_match - initial_match\n        print(f\"Correlation Alignment: Final match = {final_match:.4f} ({'+' if improvement >= 0 else ''}{improvement:.4f})\")\n    \n    return aligned_result\n\n\ndef iterative_correlation_refinement(synthetic, real, target_corr_match=0.95, max_rounds=5, verbose=True):\n    \"\"\"\n    Apply multiple rounds of correlation alignment until target is reached.\n    \n    Parameters:\n    -----------\n    synthetic : DataFrame - Generated synthetic data\n    real : DataFrame - Real reference data\n    target_corr_match : float - Target Pearson correlation between correlation matrices (0-1)\n    max_rounds : int - Maximum alignment rounds\n    verbose : bool - Print progress\n    \n    Returns:\n    --------\n    DataFrame - Refined synthetic data\n    \"\"\"\n    result = synthetic.copy()\n    features = list(result.columns)\n    n_features = len(features)\n    \n    # Target correlation\n    target_corr = real[features].corr(method='pearson').values\n    target_flat = target_corr[np.triu_indices(n_features, k=1)]\n    target_flat = np.nan_to_num(target_flat, nan=0.0)\n    \n    if verbose:\n        print(\"=\"*60)\n        print(\"ITERATIVE CORRELATION REFINEMENT\")\n        print(\"=\"*60)\n    \n    for round_num in range(1, max_rounds + 1):\n        # Check current match\n        current_corr = result[features].corr(method='pearson').values\n        current_flat = current_corr[np.triu_indices(n_features, k=1)]\n        current_flat = np.nan_to_num(current_flat, nan=0.0)\n        \n        valid_mask = ~(np.isnan(current_flat) | np.isnan(target_flat))\n        if valid_mask.sum() > 1:\n            corr_match = np.corrcoef(current_flat[valid_mask], target_flat[valid_mask])[0, 1]\n        else:\n            corr_match = 0\n        \n        if verbose:\n            print(f\"\\nRound {round_num}: Correlation match = {corr_match:.4f}\")\n        \n        if corr_match >= target_corr_match:\n            if verbose:\n                print(f\"Target reached ({target_corr_match:.2f})!\")\n            break\n        \n        # Apply alignment with increasing learning rate\n        lr = min(0.5, 0.2 + 0.1 * round_num)\n        result = align_correlation_structure(result, real, n_iterations=15, learning_rate=lr, verbose=False)\n    \n    # Final check\n    final_corr = result[features].corr(method='pearson').values\n    final_flat = final_corr[np.triu_indices(n_features, k=1)]\n    final_flat = np.nan_to_num(final_flat, nan=0.0)\n    \n    valid_mask = ~(np.isnan(final_flat) | np.isnan(target_flat))\n    final_match = np.corrcoef(final_flat[valid_mask], target_flat[valid_mask])[0, 1] if valid_mask.sum() > 1 else 0\n    \n    if verbose:\n        print(f\"\\nFinal correlation match: {final_match:.4f}\")\n        print(\"=\"*60)\n    \n    return result\n\nprint(\"Correlation alignment functions defined!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION (Aligned with Validation Notebook)\n",
    "# =============================================================================\n",
    "\n",
    "KS_EXCELLENT = 0.05\n",
    "KS_GOOD = 0.10\n",
    "KS_MODERATE = 0.15\n",
    "\n",
    "def evaluate_synthetic(real, synthetic, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate synthetic data quality.\n",
    "    Aligned with bgp_phase1_normal_traffic_validation_v2.ipynb methodology.\n",
    "    \"\"\"\n",
    "    common_cols = [c for c in real.columns if c in synthetic.columns]\n",
    "    n_features = len(common_cols)\n",
    "    \n",
    "    # Per-feature metrics\n",
    "    ks_stats = {}\n",
    "    cohens_d = {}\n",
    "    wasserstein = {}\n",
    "    \n",
    "    for col in common_cols:\n",
    "        real_vals = real[col].dropna().values\n",
    "        syn_vals = synthetic[col].dropna().values\n",
    "        \n",
    "        if len(real_vals) == 0 or len(syn_vals) == 0:\n",
    "            continue\n",
    "        \n",
    "        # KS statistic\n",
    "        ks_stat, _ = ks_2samp(syn_vals, real_vals)\n",
    "        ks_stats[col] = ks_stat\n",
    "        \n",
    "        # Cohen's d (capped at 10)\n",
    "        pooled_std = np.sqrt((real_vals.std()**2 + syn_vals.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            d = (syn_vals.mean() - real_vals.mean()) / pooled_std\n",
    "            d = np.clip(d, -10, 10)\n",
    "        else:\n",
    "            d = 0\n",
    "        cohens_d[col] = d\n",
    "        \n",
    "        # Wasserstein (normalized)\n",
    "        r_range = real_vals.max() - real_vals.min()\n",
    "        s_range = syn_vals.max() - syn_vals.min()\n",
    "        r_norm = (real_vals - real_vals.min()) / (r_range + 1e-10) if r_range > 0 else np.zeros_like(real_vals)\n",
    "        s_norm = (syn_vals - syn_vals.min()) / (s_range + 1e-10) if s_range > 0 else np.zeros_like(syn_vals)\n",
    "        wasserstein[col] = stats.wasserstein_distance(r_norm, s_norm)\n",
    "    \n",
    "    # Component 1: Distribution Score (COUNT-BASED)\n",
    "    good_or_better = sum(1 for ks in ks_stats.values() if ks < KS_GOOD)\n",
    "    distribution_score = (good_or_better / n_features) * 100\n",
    "    \n",
    "    # Component 2: Correlation Score\n",
    "    real_corr = real[common_cols].corr()\n",
    "    syn_corr = synthetic[common_cols].corr()\n",
    "    \n",
    "    real_flat = real_corr.values[np.triu_indices(n_features, k=1)]\n",
    "    syn_flat = syn_corr.values[np.triu_indices(n_features, k=1)]\n",
    "    \n",
    "    valid = ~(np.isnan(real_flat) | np.isnan(syn_flat))\n",
    "    if valid.sum() > 1:\n",
    "        structure_corr = np.corrcoef(real_flat[valid], syn_flat[valid])[0, 1]\n",
    "    else:\n",
    "        structure_corr = 0\n",
    "    \n",
    "    correlation_score = ((structure_corr + 1) / 2) * 100  # Maps [-1,1] to [0,100]\n",
    "    \n",
    "    # Component 3: Effect Size Score (WEIGHTED COUNT)\n",
    "    negligible = sum(1 for d in cohens_d.values() if abs(d) < 0.2)\n",
    "    small = sum(1 for d in cohens_d.values() if 0.2 <= abs(d) < 0.5)\n",
    "    medium = sum(1 for d in cohens_d.values() if 0.5 <= abs(d) < 0.8)\n",
    "    large = sum(1 for d in cohens_d.values() if abs(d) >= 0.8)\n",
    "    \n",
    "    effect_score = ((negligible * 1.0 + small * 0.75 + medium * 0.25 + large * 0.0) / n_features) * 100\n",
    "    \n",
    "    # Component 4: Wasserstein Score\n",
    "    mean_wd = np.mean(list(wasserstein.values()))\n",
    "    wasserstein_score = max(0, (1 - mean_wd * 2)) * 100\n",
    "    \n",
    "    # Overall Score (validation notebook weights)\n",
    "    weights = {'distribution': 0.25, 'correlation': 0.25, 'effect_size': 0.30, 'wasserstein': 0.20}\n",
    "    overall_score = (\n",
    "        distribution_score * weights['distribution'] +\n",
    "        correlation_score * weights['correlation'] +\n",
    "        effect_score * weights['effect_size'] +\n",
    "        wasserstein_score * weights['wasserstein']\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'overall_score': overall_score,\n",
    "        'distribution_score': distribution_score,\n",
    "        'correlation_score': correlation_score,\n",
    "        'effect_score': effect_score,\n",
    "        'wasserstein_score': wasserstein_score,\n",
    "        'mean_ks': np.mean(list(ks_stats.values())),\n",
    "        'structure_corr': structure_corr,\n",
    "        'ks_stats': ks_stats,\n",
    "        'cohens_d': cohens_d,\n",
    "        'effect_counts': {'negligible': negligible, 'small': small, 'medium': medium, 'large': large},\n",
    "        'good_features': good_or_better,\n",
    "        'n_features': n_features\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"EVALUATION RESULTS (Aligned with Validation Notebook)\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nDistribution (KS < {KS_GOOD}): {good_or_better}/{n_features} = {distribution_score:.1f}/100\")\n",
    "        print(f\"Correlation Structure: {structure_corr:.3f} → {correlation_score:.1f}/100\")\n",
    "        print(f\"Effect Size: neg={negligible}, small={small}, med={medium}, large={large} → {effect_score:.1f}/100\")\n",
    "        print(f\"Wasserstein: {mean_wd:.4f} → {wasserstein_score:.1f}/100\")\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"OVERALL SCORE: {overall_score:.1f}/100\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if overall_score >= 80:\n",
    "            print(\"Verdict: ✅ EXCELLENT\")\n",
    "        elif overall_score >= 70:\n",
    "            print(\"Verdict: ✅ GOOD\")\n",
    "        elif overall_score >= 50:\n",
    "            print(\"Verdict: ⚠️ MODERATE\")\n",
    "        else:\n",
    "            print(\"Verdict: ❌ POOR\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined (aligned with validation notebook)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Enhanced Copula Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# TEST MULTIPLE CONFIGURATIONS - FINE-TUNED AROUND t-Copula + Pearson\n# =============================================================================\n\nprint(\"=\"*70)\nprint(\"TESTING FINE-TUNED COPULA CONFIGURATIONS\")\nprint(\"=\"*70)\nprint(\"\\nBased on best performer: t-Copula(df=3) + Pearson Only + KDE\")\nprint(\"Testing variations in: degrees of freedom, correlation mix, bandwidth\\n\")\n\nconfigurations = [\n    # ==========================================================================\n    # BASELINE (previous best)\n    # ==========================================================================\n    {\n        'name': 't-Copula(df=3) + Pearson + KDE [BASELINE]',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    \n    # ==========================================================================\n    # DEGREES OF FREEDOM VARIATIONS (df=2,4,5 around df=3)\n    # ==========================================================================\n    {\n        'name': 't-Copula(df=2) + Pearson + KDE',\n        'copula_type': 't',\n        't_df': 2,  # Heavier tails\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=4) + Pearson + KDE',\n        'copula_type': 't',\n        't_df': 4,\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=5) + Pearson + KDE',\n        'copula_type': 't',\n        't_df': 5,  # Closer to Gaussian\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    \n    # ==========================================================================\n    # SMALL SPEARMAN MIXING (capture some non-linear relationships)\n    # ==========================================================================\n    {\n        'name': 't-Copula(df=3) + 95%Pearson/5%Spearman + KDE',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 0.95,\n        'spearman_weight': 0.05,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=3) + 90%Pearson/10%Spearman + KDE',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 0.90,\n        'spearman_weight': 0.10,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=3) + 85%Pearson/15%Spearman + KDE',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 0.85,\n        'spearman_weight': 0.15,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.0,\n        'handle_zeros': True\n    },\n    \n    # ==========================================================================\n    # KDE BANDWIDTH VARIATIONS (sharper vs smoother marginals)\n    # ==========================================================================\n    {\n        'name': 't-Copula(df=3) + Pearson + KDE(bw=0.5x)',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 0.5,  # Narrower/sharper\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=3) + Pearson + KDE(bw=0.75x)',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 0.75,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=3) + Pearson + KDE(bw=1.25x)',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.25,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=3) + Pearson + KDE(bw=1.5x)',\n        'copula_type': 't',\n        't_df': 3,\n        'pearson_weight': 1.0,\n        'spearman_weight': 0.0,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 1.5,  # Wider/smoother\n        'handle_zeros': True\n    },\n    \n    # ==========================================================================\n    # COMBINED OPTIMIZATIONS (best df + best mix + best bandwidth)\n    # ==========================================================================\n    {\n        'name': 't-Copula(df=2) + 95%Pearson + KDE(bw=0.75x)',\n        'copula_type': 't',\n        't_df': 2,\n        'pearson_weight': 0.95,\n        'spearman_weight': 0.05,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 0.75,\n        'handle_zeros': True\n    },\n    {\n        'name': 't-Copula(df=4) + 90%Pearson + KDE(bw=0.75x)',\n        'copula_type': 't',\n        't_df': 4,\n        'pearson_weight': 0.90,\n        'spearman_weight': 0.10,\n        'use_kde_marginals': True,\n        'kde_bandwidth_factor': 0.75,\n        'handle_zeros': True\n    },\n]\n\nprint(f\"Testing {len(configurations)} fine-tuned configurations...\\n\")\n\nresults_list = []\n\nfor i, config in enumerate(configurations, 1):\n    name = config.pop('name')\n    print(f\"[{i}/{len(configurations)}] {name}\")\n    print(\"-\" * 60)\n    \n    try:\n        # Create and fit generator\n        generator = EnhancedCopulaGenerator(**config, random_state=RANDOM_STATE)\n        generator.fit(X_real)\n        \n        # Generate samples\n        synthetic = generator.generate(N_SYNTHETIC)\n        \n        # Enforce constraints\n        synthetic = enforce_bgp_constraints(synthetic, X_real, verbose=False)\n        \n        # Evaluate\n        eval_result = evaluate_synthetic(X_real, synthetic, verbose=False)\n        \n        print(f\"  Score: {eval_result['overall_score']:.1f}/100\")\n        print(f\"  KS Good: {eval_result['good_features']}/{eval_result['n_features']}\")\n        print(f\"  Correlation: {eval_result['structure_corr']:.3f}\")\n        \n        results_list.append({\n            'name': name,\n            'score': eval_result['overall_score'],\n            'synthetic': synthetic,\n            'eval': eval_result,\n            'config': config\n        })\n        \n    except Exception as e:\n        print(f\"  FAILED: {e}\")\n    \n    config['name'] = name  # Restore\n    print()\n\n# Sort by score and display results\nprint(\"=\"*70)\nprint(\"CONFIGURATION RANKING (Top 10)\")\nprint(\"=\"*70)\nresults_sorted = sorted(results_list, key=lambda x: x['score'], reverse=True)\nfor i, r in enumerate(results_sorted[:10], 1):\n    marker = \" <-- BEST\" if i == 1 else \"\"\n    improvement = \"\"\n    if i > 1 and '[BASELINE]' in results_sorted[0]['name']:\n        baseline_score = next((x['score'] for x in results_list if '[BASELINE]' in x['name']), None)\n        if baseline_score:\n            diff = r['score'] - baseline_score\n            improvement = f\" ({'+' if diff >= 0 else ''}{diff:.1f} vs baseline)\"\n    print(f\"{i:2d}. {r['name']}: {r['score']:.1f}/100{marker}{improvement}\")\n\n# Find best\nif results_list:\n    best = max(results_list, key=lambda x: x['score'])\n    baseline = next((x for x in results_list if '[BASELINE]' in x['name']), None)\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"BEST CONFIGURATION: {best['name']}\")\n    print(f\"SCORE: {best['score']:.1f}/100\")\n    if baseline and best['name'] != baseline['name']:\n        improvement = best['score'] - baseline['score']\n        print(f\"IMPROVEMENT vs BASELINE: {'+' if improvement >= 0 else ''}{improvement:.1f} points\")\n    print(f\"{'='*70}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# USE BEST CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "if results_list:\n",
    "    best_synthetic = best['synthetic']\n",
    "    best_eval = best['eval']\n",
    "    \n",
    "    # Full evaluation\n",
    "    print(\"\\nFull Evaluation of Best Configuration:\")\n",
    "    _ = evaluate_synthetic(X_real, best_synthetic, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# APPLY CORRELATION ALIGNMENT TO IMPROVE PEARSON CORRELATION MATCH\n# =============================================================================\n\nif results_list:\n    print(\"=\"*70)\n    print(\"APPLYING CORRELATION ALIGNMENT POST-PROCESSING\")\n    print(\"=\"*70)\n    \n    # Store pre-alignment metrics\n    pre_corr = best_eval['structure_corr']\n    pre_score = best_eval['overall_score']\n    \n    print(f\"\\nPre-alignment correlation match: {pre_corr:.4f}\")\n    print(f\"Pre-alignment overall score: {pre_score:.1f}/100\")\n    \n    # Apply iterative correlation refinement\n    # Target: 0.90+ correlation match (from current ~0.78)\n    best_synthetic_aligned = iterative_correlation_refinement(\n        best_synthetic, \n        X_real, \n        target_corr_match=0.92,  # Target correlation match\n        max_rounds=5,\n        verbose=True\n    )\n    \n    # Re-apply BGP constraints after alignment\n    print(\"\\nRe-applying BGP constraints...\")\n    best_synthetic_aligned = enforce_bgp_constraints(best_synthetic_aligned, X_real, verbose=True)\n    \n    # Re-evaluate\n    print(\"\\nEvaluating aligned synthetic data:\")\n    aligned_eval = evaluate_synthetic(X_real, best_synthetic_aligned, verbose=True)\n    \n    # Compare improvement\n    post_corr = aligned_eval['structure_corr']\n    post_score = aligned_eval['overall_score']\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"CORRELATION ALIGNMENT RESULTS\")\n    print(\"=\"*70)\n    print(f\"\\nCorrelation match: {pre_corr:.4f} -> {post_corr:.4f} ({'+' if post_corr > pre_corr else ''}{post_corr - pre_corr:.4f})\")\n    print(f\"Overall score: {pre_score:.1f} -> {post_score:.1f} ({'+' if post_score > pre_score else ''}{post_score - pre_score:.1f})\")\n    \n    # Use aligned data if better\n    if post_score >= pre_score:\n        print(\"\\nUsing correlation-aligned data as final output.\")\n        best_synthetic = best_synthetic_aligned\n        best_eval = aligned_eval\n    else:\n        print(\"\\nKeeping original data (alignment didn't improve overall score).\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: CONFIGURATION COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "if results_list:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart of scores\n",
    "    names = [r['name'] for r in results_list]\n",
    "    scores = [r['score'] for r in results_list]\n",
    "    \n",
    "    colors = ['green' if s >= 70 else 'orange' if s >= 50 else 'red' for s in scores]\n",
    "    \n",
    "    ax1 = axes[0]\n",
    "    bars = ax1.barh(names, scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.axvline(x=70, color='green', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "    ax1.axvline(x=50, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "    ax1.set_xlabel('Overall Score')\n",
    "    ax1.set_title('Configuration Comparison')\n",
    "    ax1.set_xlim(0, 100)\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Component breakdown for best\n",
    "    ax2 = axes[1]\n",
    "    components = ['Distribution', 'Correlation', 'Effect Size', 'Wasserstein']\n",
    "    values = [\n",
    "        best_eval['distribution_score'],\n",
    "        best_eval['correlation_score'],\n",
    "        best_eval['effect_score'],\n",
    "        best_eval['wasserstein_score']\n",
    "    ]\n",
    "    \n",
    "    bars2 = ax2.bar(components, values, color=['#3498db', '#2ecc71', '#f39c12', '#9b59b6'], alpha=0.8)\n",
    "    ax2.axhline(y=best_eval['overall_score'], color='red', linestyle='--', label=f'Overall: {best_eval[\"overall_score\"]:.1f}')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.set_title(f'Best Config: {best[\"name\"]}')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'enhanced_copula_comparison.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZATION: DISTRIBUTION COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "if results_list:\n",
    "    # Get worst features by KS\n",
    "    ks_sorted = sorted(best_eval['ks_stats'].items(), key=lambda x: x[1], reverse=True)\n",
    "    worst_5 = [f for f, _ in ks_sorted[:5]]\n",
    "    best_5 = [f for f, _ in ks_sorted[-5:]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    \n",
    "    # Worst features\n",
    "    for idx, feat in enumerate(worst_5):\n",
    "        ax = axes[0, idx]\n",
    "        real_vals = X_real[feat].values\n",
    "        syn_vals = best_synthetic[feat].values\n",
    "        \n",
    "        # Log scale if needed\n",
    "        if real_vals.max() > 100:\n",
    "            real_vals = np.log1p(real_vals)\n",
    "            syn_vals = np.log1p(syn_vals)\n",
    "            ax.set_xlabel('log1p(value)')\n",
    "        \n",
    "        ax.hist(real_vals, bins=50, alpha=0.5, label='Real', density=True)\n",
    "        ax.hist(syn_vals, bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "        ax.set_title(f'{feat}\\nKS={best_eval[\"ks_stats\"][feat]:.3f}')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    # Best features\n",
    "    for idx, feat in enumerate(best_5):\n",
    "        ax = axes[1, idx]\n",
    "        real_vals = X_real[feat].values\n",
    "        syn_vals = best_synthetic[feat].values\n",
    "        \n",
    "        if real_vals.max() > 100:\n",
    "            real_vals = np.log1p(real_vals)\n",
    "            syn_vals = np.log1p(syn_vals)\n",
    "            ax.set_xlabel('log1p(value)')\n",
    "        \n",
    "        ax.hist(real_vals, bins=50, alpha=0.5, label='Real', density=True)\n",
    "        ax.hist(syn_vals, bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "        ax.set_title(f'{feat}\\nKS={best_eval[\"ks_stats\"][feat]:.3f}')\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Worst 5 Features', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Best 5 Features', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, 'enhanced_copula_distributions.png'), dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PREPARE AND SAVE FINAL OUTPUT\n",
    "# =============================================================================\n",
    "\n",
    "if results_list:\n",
    "    # Prepare final output\n",
    "    synthetic_final = best_synthetic.copy()\n",
    "    \n",
    "    # Convert to integers\n",
    "    for col in synthetic_final.columns:\n",
    "        if col in INTEGER_FEATURES:\n",
    "            synthetic_final[col] = synthetic_final[col].fillna(0).astype(int)\n",
    "    \n",
    "    # Add metadata columns\n",
    "    synthetic_final.insert(0, 'sequence_id', range(len(synthetic_final)))\n",
    "    synthetic_final.insert(1, 'timestamp', pd.date_range('2024-01-01', periods=len(synthetic_final), freq='1min'))\n",
    "    synthetic_final['label'] = 'normal'\n",
    "    \n",
    "    # Save\n",
    "    output_path = os.path.join(OUTPUT_DIR, 'synthetic_enhanced_copula.csv')\n",
    "    synthetic_final.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    print(f\"Shape: {synthetic_final.shape}\")\n",
    "    print(f\"Columns: {list(synthetic_final.columns)}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    synthetic_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE EVALUATION RESULTS\n",
    "# =============================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "if results_list:\n",
    "    eval_output = {\n",
    "        'best_config': best['name'],\n",
    "        'overall_score': best_eval['overall_score'],\n",
    "        'component_scores': {\n",
    "            'distribution': best_eval['distribution_score'],\n",
    "            'correlation': best_eval['correlation_score'],\n",
    "            'effect_size': best_eval['effect_score'],\n",
    "            'wasserstein': best_eval['wasserstein_score']\n",
    "        },\n",
    "        'mean_ks': best_eval['mean_ks'],\n",
    "        'structure_corr': best_eval['structure_corr'],\n",
    "        'good_features_count': best_eval['good_features'],\n",
    "        'effect_counts': best_eval['effect_counts'],\n",
    "        'all_configs_tested': [\n",
    "            {'name': r['name'], 'score': r['score']} for r in results_list\n",
    "        ],\n",
    "        'n_samples': N_SYNTHETIC,\n",
    "        'n_features': best_eval['n_features']\n",
    "    }\n",
    "    \n",
    "    eval_path = os.path.join(OUTPUT_DIR, 'enhanced_copula_evaluation.json')\n",
    "    with open(eval_path, 'w') as f:\n",
    "        json.dump(eval_output, f, indent=2)\n",
    "    \n",
    "    print(f\"Evaluation saved to: {eval_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Key Features of Enhanced Copula:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Hybrid Correlation** | Combines Pearson (linear) + Spearman (non-linear) with configurable weights |\n",
    "| **KDE Marginals** | Better estimation for heavy-tailed features |\n",
    "| **Zero-Inflation** | Preserves sparsity in features like flaps, nadas |\n",
    "| **t-Copula** | Optional heavy-tailed dependency structure |\n",
    "| **Constraint Enforcement** | Built-in BGP domain constraints |\n",
    "| **Auto-Configuration** | Tests multiple configs and selects best |\n",
    "\n",
    "### Recommended Configuration:\n",
    "Based on BGP traffic characteristics:\n",
    "- **Copula**: Gaussian or t-copula (df=5) for heavier tails\n",
    "- **Correlation**: 30% Pearson + 70% Spearman (captures non-linear relationships)\n",
    "- **Marginals**: KDE for continuous, empirical for discrete\n",
    "- **Zero handling**: Enabled (important for sparse features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "if results_list:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ENHANCED COPULA GENERATION COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBest Configuration: {best['name']}\")\n",
    "    print(f\"Overall Score: {best_eval['overall_score']:.1f}/100\")\n",
    "    print(f\"\\nComponent Scores:\")\n",
    "    print(f\"  - Distribution (KS < 0.1): {best_eval['distribution_score']:.1f}/100\")\n",
    "    print(f\"  - Correlation Structure: {best_eval['correlation_score']:.1f}/100\")\n",
    "    print(f\"  - Effect Size: {best_eval['effect_score']:.1f}/100\")\n",
    "    print(f\"  - Wasserstein Distance: {best_eval['wasserstein_score']:.1f}/100\")\n",
    "    print(f\"\\nOutput saved to: {output_path}\")\n",
    "    print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}