{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Unified BGP Feature Extraction\n\nThis notebook extracts time-window based features from BGP update data for anomaly detection.\n\n**Features extracted:**\n- Basic counts: announcements, withdrawals, nlri_ann, duplicates\n- Origin attributes: origin_0 (IGP), origin_2 (INCOMPLETE), origin_changes\n- Implicit withdrawals: imp_wd, imp_wd_spath, imp_wd_dpath\n- AS path metrics: as_path_max, unique_as_path_max\n- Edit distance: avg, max, distribution (dict_0 to dict_6), unique distribution\n- Rare AS metrics: number_rare_ases, rare_ases_avg\n- Flaps and NADAS\n\n**Works with multiple schemas:**\n\n| Schema | Columns | Notes |\n|--------|---------|-------|\n| **Standard (new)** | Timestamp, Subtype, Origin, Peer_ASN, Communities | Full BGP Origin attribute support |\n| **RIPE Old** | Time, Entry_Type, Origin_AS, Peer_AS, Community | Missing BGP Origin attribute |\n\n**Important:** Old RIPE data has `Origin_AS` (the originating ASN from AS_Path) but is **missing** the BGP `Origin` attribute (IGP/EGP/INCOMPLETE). Re-collect data with the updated collector to get full feature support."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION - Modify these settings as needed\n",
    "# =============================================================================\n",
    "\n",
    "# Input/Output paths\n",
    "INPUT_FILE = \"/home/smotaali/BGP_Traffic_Generation/results/bgp_updates_analysis.csv\"\n",
    "OUTPUT_FILE = \"/home/smotaali/BGP_Traffic_Generation/results/extracted_features_1s.csv\"\n",
    "\n",
    "# For processing RIPE incidents directory\n",
    "RIPE_INCIDENTS_DIR = \"/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS\"\n",
    "\n",
    "# Feature extraction settings\n",
    "WINDOW_SIZE = '1s'  # Options: '1s', '5s', '30s', '1min', '5min', etc.\n",
    "LABEL_STRATEGY = 'majority'  # Options: 'majority', 'conservative', 'weighted'\n",
    "\n",
    "# Schema type: 'auto', 'ripe', or 'standard'\n",
    "SCHEMA_TYPE = 'auto'\n",
    "\n",
    "print(f\"Window size: {WINDOW_SIZE}\")\n",
    "print(f\"Label strategy: {LABEL_STRATEGY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_as_path(as_path) -> List[int]:\n",
    "    \"\"\"\n",
    "    Convert AS path to list of integers.\n",
    "    \n",
    "    Handles:\n",
    "    - Integer: 65001 -> [65001]\n",
    "    - String: \"65001 65002 65003\" -> [65001, 65002, 65003]\n",
    "    - AS_SET: \"65001 {65002 65003}\" -> [65001, 65002, 65003]\n",
    "    - List: [65001, 65002] -> [65001, 65002]\n",
    "    \"\"\"\n",
    "    if isinstance(as_path, int):\n",
    "        return [as_path]\n",
    "    \n",
    "    if isinstance(as_path, list):\n",
    "        return [int(x) for x in as_path if str(x).isdigit()]\n",
    "    \n",
    "    if isinstance(as_path, str):\n",
    "        # Remove AS_SET brackets\n",
    "        as_path = as_path.replace('{', '').replace('}', '')\n",
    "        return [int(asn) for asn in as_path.split() if asn.isdigit()]\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def get_path_length(as_path) -> int:\n",
    "    \"\"\"Get the length of an AS path.\"\"\"\n",
    "    if pd.isnull(as_path) or as_path == '':\n",
    "        return 0\n",
    "    return len(_normalize_as_path(as_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edit_distance(as_path1, as_path2) -> int:\n",
    "    \"\"\"\n",
    "    Calculate Levenshtein edit distance between two AS paths.\n",
    "    \n",
    "    The edit distance measures how many insertions, deletions, or\n",
    "    substitutions are needed to transform one path into another.\n",
    "    \n",
    "    Args:\n",
    "        as_path1: First AS path (string, int, or list)\n",
    "        as_path2: Second AS path (string, int, or list)\n",
    "    \n",
    "    Returns:\n",
    "        Edit distance as integer\n",
    "    \"\"\"\n",
    "    if not as_path1 or not as_path2:\n",
    "        return 0\n",
    "    \n",
    "    # Normalize to list of integers\n",
    "    path1 = _normalize_as_path(as_path1)\n",
    "    path2 = _normalize_as_path(as_path2)\n",
    "    \n",
    "    if not path1 or not path2:\n",
    "        return 0\n",
    "    \n",
    "    m, n = len(path1), len(path2)\n",
    "    \n",
    "    # Initialize DP table\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Fill DP table\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if path1[i-1] == path2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    \n",
    "    return dp[m][n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attributes_are_same(row1: pd.Series, row2: pd.Series) -> bool:\n",
    "    \"\"\"\n",
    "    Compare BGP attributes between two announcements.\n",
    "    \n",
    "    Compares: AS_Path, Origin, Next_Hop, MED, Local_Pref, Communities\n",
    "    \n",
    "    Returns True if all comparable attributes are the same.\n",
    "    \"\"\"\n",
    "    attrs_to_compare = ['AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "    \n",
    "    for attr in attrs_to_compare:\n",
    "        if attr not in row1.index or attr not in row2.index:\n",
    "            continue\n",
    "        \n",
    "        val1, val2 = row1[attr], row2[attr]\n",
    "        \n",
    "        # Both NaN -> same\n",
    "        if pd.isna(val1) and pd.isna(val2):\n",
    "            continue\n",
    "        # One NaN, one not -> different\n",
    "        if pd.isna(val1) or pd.isna(val2):\n",
    "            return False\n",
    "        # Compare values\n",
    "        if val1 != val2:\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NADAS and Flaps Calculation\n",
    "\n",
    "**Definitions:**\n",
    "- **FLAP**: A prefix is withdrawn and then re-announced with the **same** attributes\n",
    "- **NADAS**: A prefix is withdrawn and then re-announced with **different** attributes\n",
    "\n",
    "This requires tracking the state of each (prefix, peer) pair through the time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nadas_and_flaps(df_window: pd.DataFrame) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Calculate NADAS and FLAPS from BGP update sequence.\n",
    "    \n",
    "    NADAS: Withdrawal followed by Announcement with DIFFERENT attributes\n",
    "    FLAP: Withdrawal followed by Announcement with SAME attributes\n",
    "    \n",
    "    Args:\n",
    "        df_window: DataFrame with BGP updates in a time window\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (nadas_count, flap_count)\n",
    "    \"\"\"\n",
    "    nadas_count = 0\n",
    "    flap_count = 0\n",
    "    \n",
    "    withdrawal_types = ['WITHDRAW', 'WITHDRAW_MP_UNREACH_NLRI_AFI2']\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df_sorted = df_window.sort_values('Timestamp')\n",
    "    \n",
    "    # Track state per (prefix, peer)\n",
    "    # State: {'withdrawn': bool, 'last_ann': row or None}\n",
    "    prefix_state: Dict[Tuple, Dict] = {}\n",
    "    \n",
    "    for _, row in df_sorted.iterrows():\n",
    "        key = (row['Prefix'], row['Peer_IP'])\n",
    "        \n",
    "        if row['Subtype'] == 'ANNOUNCE':\n",
    "            # Check if this prefix was previously withdrawn\n",
    "            if key in prefix_state and prefix_state[key].get('withdrawn', False):\n",
    "                last_ann = prefix_state[key].get('last_ann')\n",
    "                \n",
    "                if last_ann is not None:\n",
    "                    if attributes_are_same(last_ann, row):\n",
    "                        flap_count += 1\n",
    "                    else:\n",
    "                        nadas_count += 1\n",
    "                else:\n",
    "                    # No previous announcement to compare -> count as NADAS\n",
    "                    nadas_count += 1\n",
    "            \n",
    "            # Update state\n",
    "            if key not in prefix_state:\n",
    "                prefix_state[key] = {}\n",
    "            prefix_state[key]['last_ann'] = row\n",
    "            prefix_state[key]['withdrawn'] = False\n",
    "        \n",
    "        elif row['Subtype'] in withdrawal_types:\n",
    "            # Mark prefix as withdrawn\n",
    "            if key not in prefix_state:\n",
    "                prefix_state[key] = {'last_ann': None}\n",
    "            prefix_state[key]['withdrawn'] = True\n",
    "    \n",
    "    return nadas_count, flap_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Feature Extraction Function\n",
    "\n",
    "This function extracts all 25+ features from a single time window of BGP updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df_window: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract BGP features from a time window.\n",
    "    \n",
    "    Args:\n",
    "        df_window: DataFrame containing BGP updates within a time window\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of feature name -> value\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Separate announcements and withdrawals\n",
    "    announcements = df_window[df_window['Subtype'] == 'ANNOUNCE']\n",
    "    withdrawal_types = ['WITHDRAW', 'WITHDRAW_MP_UNREACH_NLRI_AFI2']\n",
    "    withdrawals = df_window[df_window['Subtype'].isin(withdrawal_types)]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # BASIC COUNTS\n",
    "    # =========================================================================\n",
    "    features['announcements'] = len(announcements)\n",
    "    features['withdrawals'] = len(withdrawals)\n",
    "    \n",
    "    # NLRI_ANN: Number of unique prefixes announced (NOT total announcements)\n",
    "    features['nlri_ann'] = announcements['Prefix'].nunique()\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DUPLICATES\n",
    "    # =========================================================================\n",
    "    if not announcements.empty:\n",
    "        dup_cols = ['Peer_IP', 'Peer_ASN', 'Prefix', 'AS_Path', 'Origin',\n",
    "                    'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "        dup_cols = [c for c in dup_cols if c in announcements.columns]\n",
    "        \n",
    "        announcement_counts = announcements.groupby(dup_cols).size()\n",
    "        features['dups'] = sum(count - 1 for count in announcement_counts if count > 1)\n",
    "    else:\n",
    "        features['dups'] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # ORIGIN ATTRIBUTES\n",
    "    # =========================================================================\n",
    "    if not announcements.empty and 'Origin' in announcements.columns:\n",
    "        origin_counts = announcements['Origin'].value_counts()\n",
    "        features['origin_0'] = origin_counts.get('IGP', 0)\n",
    "        features['origin_2'] = origin_counts.get('INCOMPLETE', 0)\n",
    "        \n",
    "        # Origin changes: prefixes announced with multiple different origins\n",
    "        unique_prefix_origins = announcements.groupby('Prefix')['Origin'].nunique()\n",
    "        features['origin_changes'] = (unique_prefix_origins > 1).sum()\n",
    "    else:\n",
    "        features['origin_0'] = 0\n",
    "        features['origin_2'] = 0\n",
    "        features['origin_changes'] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # IMPLICIT WITHDRAWALS\n",
    "    # An implicit withdrawal occurs when a prefix is re-announced with\n",
    "    # different attributes (replacing the previous announcement)\n",
    "    # =========================================================================\n",
    "    imp_wd_count = 0\n",
    "    imp_wd_spath_count = 0  # Same AS_Path, other attrs changed\n",
    "    imp_wd_dpath_count = 0  # Different AS_Path\n",
    "    \n",
    "    edit_distances = []\n",
    "    edit_distance_dict = defaultdict(list)\n",
    "    \n",
    "    attrs_to_compare = ['AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "    \n",
    "    if not announcements.empty:\n",
    "        available_attrs = [c for c in attrs_to_compare if c in announcements.columns]\n",
    "        \n",
    "        for (prefix, peer), group in announcements.groupby(['Prefix', 'Peer_IP']):\n",
    "            if len(group) < 2:\n",
    "                continue\n",
    "            \n",
    "            sorted_group = group.sort_values('Timestamp')\n",
    "            prev_row = None\n",
    "            \n",
    "            for _, row in sorted_group.iterrows():\n",
    "                if prev_row is not None:\n",
    "                    attributes_changed = False\n",
    "                    as_path_changed = False\n",
    "                    \n",
    "                    for attr in available_attrs:\n",
    "                        prev_val = prev_row.get(attr)\n",
    "                        curr_val = row.get(attr)\n",
    "                        \n",
    "                        prev_nan = pd.isna(prev_val)\n",
    "                        curr_nan = pd.isna(curr_val)\n",
    "                        \n",
    "                        if prev_nan and curr_nan:\n",
    "                            continue\n",
    "                        if prev_nan or curr_nan or prev_val != curr_val:\n",
    "                            attributes_changed = True\n",
    "                            if attr == 'AS_Path':\n",
    "                                as_path_changed = True\n",
    "                    \n",
    "                    if attributes_changed:\n",
    "                        imp_wd_count += 1\n",
    "                        \n",
    "                        if as_path_changed:\n",
    "                            imp_wd_dpath_count += 1\n",
    "                            \n",
    "                            # Calculate edit distance for AS_Path changes\n",
    "                            prev_path = prev_row.get('AS_Path', '')\n",
    "                            curr_path = row.get('AS_Path', '')\n",
    "                            dist = calculate_edit_distance(prev_path, curr_path)\n",
    "                            if dist is not None:\n",
    "                                edit_distances.append(dist)\n",
    "                                edit_distance_dict[prefix].append(dist)\n",
    "                        else:\n",
    "                            imp_wd_spath_count += 1\n",
    "                \n",
    "                prev_row = row\n",
    "    \n",
    "    features['imp_wd'] = imp_wd_count\n",
    "    features['imp_wd_spath'] = imp_wd_spath_count\n",
    "    features['imp_wd_dpath'] = imp_wd_dpath_count\n",
    "    \n",
    "    # =========================================================================\n",
    "    # AS PATH METRICS\n",
    "    # =========================================================================\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        valid_paths = announcements[\n",
    "            announcements['AS_Path'].notna() & (announcements['AS_Path'] != '')\n",
    "        ]\n",
    "        \n",
    "        if not valid_paths.empty:\n",
    "            path_lengths = valid_paths['AS_Path'].apply(get_path_length)\n",
    "            features['as_path_max'] = path_lengths.max() if not path_lengths.empty else 0\n",
    "            \n",
    "            unique_paths_per_prefix = valid_paths.groupby('Prefix')['AS_Path'].nunique()\n",
    "            features['unique_as_path_max'] = unique_paths_per_prefix.max() if not unique_paths_per_prefix.empty else 0\n",
    "        else:\n",
    "            features['as_path_max'] = 0\n",
    "            features['unique_as_path_max'] = 0\n",
    "    else:\n",
    "        features['as_path_max'] = 0\n",
    "        features['unique_as_path_max'] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # EDIT DISTANCE FEATURES\n",
    "    # =========================================================================\n",
    "    if edit_distances:\n",
    "        features['edit_distance_avg'] = float(np.mean(edit_distances))\n",
    "        features['edit_distance_max'] = max(edit_distances)\n",
    "        \n",
    "        # Distribution of edit distances (0-6)\n",
    "        ed_counter = Counter(edit_distances)\n",
    "        for i in range(7):\n",
    "            features[f'edit_distance_dict_{i}'] = ed_counter.get(i, 0)\n",
    "        \n",
    "        # Unique edit distances per prefix\n",
    "        unique_ed = {}\n",
    "        for prefix, dists in edit_distance_dict.items():\n",
    "            for d in set(dists):\n",
    "                unique_ed[d] = unique_ed.get(d, 0) + 1\n",
    "        \n",
    "        for i in range(2):\n",
    "            features[f'edit_distance_unique_dict_{i}'] = unique_ed.get(i, 0)\n",
    "    else:\n",
    "        features['edit_distance_avg'] = 0\n",
    "        features['edit_distance_max'] = 0\n",
    "        for i in range(7):\n",
    "            features[f'edit_distance_dict_{i}'] = 0\n",
    "        for i in range(2):\n",
    "            features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # RARE AS FEATURES\n",
    "    # =========================================================================\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        all_asns = []\n",
    "        \n",
    "        for as_path in announcements['AS_Path']:\n",
    "            if pd.isnull(as_path) or as_path == '':\n",
    "                continue\n",
    "            all_asns.extend([str(asn) for asn in _normalize_as_path(as_path)])\n",
    "        \n",
    "        if all_asns:\n",
    "            asn_counts = Counter(all_asns)\n",
    "            rare_threshold = 3\n",
    "            rare_asns = [asn for asn, count in asn_counts.items() if count < rare_threshold]\n",
    "            \n",
    "            features['number_rare_ases'] = len(rare_asns)\n",
    "            features['rare_ases_avg'] = len(rare_asns) / len(all_asns)\n",
    "        else:\n",
    "            features['number_rare_ases'] = 0\n",
    "            features['rare_ases_avg'] = 0\n",
    "    else:\n",
    "        features['number_rare_ases'] = 0\n",
    "        features['rare_ases_avg'] = 0\n",
    "    \n",
    "    # =========================================================================\n",
    "    # NADAS AND FLAPS\n",
    "    # =========================================================================\n",
    "    nadas_count, flap_count = calculate_nadas_and_flaps(df_window)\n",
    "    features['nadas'] = nadas_count\n",
    "    features['flaps'] = flap_count\n",
    "    \n",
    "    # =========================================================================\n",
    "    # LABEL AGGREGATION\n",
    "    # =========================================================================\n",
    "    if 'Label' in df_window.columns:\n",
    "        labels = df_window['Label'].value_counts()\n",
    "        if not labels.empty:\n",
    "            if LABEL_STRATEGY == 'majority':\n",
    "                features['label'] = labels.idxmax()\n",
    "            elif LABEL_STRATEGY == 'conservative':\n",
    "                abnormal = [l for l in labels.index if l != 'normal']\n",
    "                features['label'] = abnormal[0] if abnormal else 'normal'\n",
    "            elif LABEL_STRATEGY == 'weighted':\n",
    "                total = labels.sum()\n",
    "                abnormal_weight = sum(c for l, c in labels.items() if l != 'normal') / total\n",
    "                if abnormal_weight > 0.4:\n",
    "                    abnormal = [l for l in labels.index if l != 'normal']\n",
    "                    features['label'] = abnormal[0] if abnormal else 'normal'\n",
    "                else:\n",
    "                    features['label'] = 'normal'\n",
    "            else:\n",
    "                features['label'] = labels.idxmax()\n",
    "        else:\n",
    "            features['label'] = 'unknown'\n",
    "    else:\n",
    "        features['label'] = 'unknown'\n",
    "    \n",
    "    # Keep incident name if present\n",
    "    if 'Incident' in df_window.columns:\n",
    "        features['Incident'] = df_window['Incident'].iloc[0]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def prepare_dataframe(df: pd.DataFrame, schema_type: str = 'auto') -> pd.DataFrame:\n    \"\"\"\n    Prepare DataFrame by normalizing column names and data types.\n    \n    Handles three schema types:\n    1. 'standard' - New unified schema (Timestamp, Subtype, Origin, Peer_ASN, Communities)\n    2. 'ripe_old' - Old RIPE schema (Time, Entry_Type, Origin_AS, Peer_AS, Community)\n    3. 'auto' - Auto-detect based on column names\n    \n    Args:\n        df: Input DataFrame\n        schema_type: 'ripe_old', 'standard', or 'auto'\n    \n    Returns:\n        Prepared DataFrame with standardized columns\n    \"\"\"\n    df = df.copy()\n    \n    # Auto-detect schema type\n    if schema_type == 'auto':\n        # Check for old RIPE schema (Time + Entry_Type columns)\n        if 'Time' in df.columns and 'Entry_Type' in df.columns:\n            schema_type = 'ripe_old'\n            print(\"Detected OLD RIPE schema (Time, Entry_Type, Origin_AS)\")\n        # Check for new standard schema (Timestamp + Subtype columns)  \n        elif 'Timestamp' in df.columns and 'Subtype' in df.columns:\n            schema_type = 'standard'\n            print(\"Detected STANDARD schema (Timestamp, Subtype, Origin)\")\n        else:\n            # Fallback - try to detect by other columns\n            if 'Time' in df.columns:\n                schema_type = 'ripe_old'\n                print(\"Detected OLD RIPE schema (fallback)\")\n            else:\n                schema_type = 'standard'\n                print(\"Detected STANDARD schema (fallback)\")\n    \n    # =========================================================================\n    # OLD RIPE SCHEMA MAPPING\n    # Columns: Time, Entry_Type (A/W), Peer_AS, Origin_AS, Community\n    # Note: Origin_AS is the originating ASN, NOT the BGP Origin attribute!\n    # =========================================================================\n    if schema_type == 'ripe_old':\n        # Time -> Timestamp\n        df['Timestamp'] = pd.to_datetime(df['Time'])\n        \n        # Entry_Type (A/W) -> Subtype (ANNOUNCE/WITHDRAW)\n        def map_subtype(entry_type):\n            if entry_type == 'A':\n                return 'ANNOUNCE'\n            elif entry_type == 'W':\n                return 'WITHDRAW'\n            return 'UNKNOWN'\n        \n        df['Subtype'] = df['Entry_Type'].apply(map_subtype)\n        \n        # Column renames for old RIPE schema\n        rename_map = {}\n        if 'Peer_AS' in df.columns:\n            rename_map['Peer_AS'] = 'Peer_ASN'\n        if 'Community' in df.columns:\n            rename_map['Community'] = 'Communities'\n        \n        if rename_map:\n            df.rename(columns=rename_map, inplace=True)\n        \n        # IMPORTANT: Old RIPE schema has Origin_AS (originating ASN), \n        # but NOT the BGP Origin attribute (IGP/EGP/INCOMPLETE)\n        # We need to handle this - set Origin to empty if not present\n        if 'Origin_AS' in df.columns and 'Origin' not in df.columns:\n            # Origin_AS is the originating ASN, not the BGP Origin attribute\n            # The BGP Origin attribute is missing in old schema\n            df['Origin'] = ''  # Set to empty - this data is missing\n            print(\"WARNING: Old RIPE schema missing BGP Origin attribute (IGP/EGP/INCOMPLETE)\")\n            print(\"         Origin-based features (origin_0, origin_2) will be 0\")\n    \n    # =========================================================================\n    # STANDARD SCHEMA (new collectors output this format)\n    # Columns: Timestamp, Subtype, Peer_ASN, Origin, Origin_ASN, Communities\n    # =========================================================================\n    else:  # standard schema\n        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n        # All columns should already have correct names\n    \n    # Final validation - ensure required columns exist\n    required_cols = ['Timestamp', 'Subtype', 'Prefix', 'Peer_IP']\n    missing = [c for c in required_cols if c not in df.columns]\n    if missing:\n        print(f\"WARNING: Missing required columns: {missing}\")\n    \n    return df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(input_path: str, output_path: str = None, \n",
    "                 schema_type: str = 'auto') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a single BGP data file and extract features.\n",
    "    \n",
    "    Args:\n",
    "        input_path: Path to input CSV file\n",
    "        output_path: Path to output features CSV (optional)\n",
    "        schema_type: 'ripe', 'standard', or 'auto'\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with extracted features\n",
    "    \"\"\"\n",
    "    print(f\"Reading {input_path}...\")\n",
    "    df = pd.read_csv(input_path, low_memory=False)\n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    \n",
    "    # Prepare data\n",
    "    df = prepare_dataframe(df, schema_type)\n",
    "    \n",
    "    # Sort and set index\n",
    "    df = df.sort_values('Timestamp')\n",
    "    \n",
    "    start_time = df['Timestamp'].min()\n",
    "    end_time = df['Timestamp'].max()\n",
    "    print(f\"Time range: {start_time} to {end_time}\")\n",
    "    \n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    \n",
    "    # Process windows\n",
    "    features_list = []\n",
    "    count = 0\n",
    "    \n",
    "    grouped = df.groupby(pd.Grouper(freq=WINDOW_SIZE))\n",
    "    total_windows = len(grouped)\n",
    "    \n",
    "    for window_start, window_df in grouped:\n",
    "        if window_df.empty:\n",
    "            continue\n",
    "        \n",
    "        w = window_df.reset_index()\n",
    "        f = extract_features(w)\n",
    "        \n",
    "        if f:\n",
    "            window_end = window_start + pd.Timedelta(WINDOW_SIZE)\n",
    "            f['window_start'] = window_start\n",
    "            f['window_end'] = window_end\n",
    "            features_list.append(f)\n",
    "            count += 1\n",
    "            \n",
    "            if count % 500 == 0:\n",
    "                print(f\"  Processed {count} windows ({count/total_windows*100:.1f}%)...\")\n",
    "    \n",
    "    print(f\"Total windows processed: {count}\")\n",
    "    \n",
    "    if not features_list:\n",
    "        print(\"No features extracted!\")\n",
    "        return None\n",
    "    \n",
    "    out_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    if output_path:\n",
    "        out_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved features to {output_path}\")\n",
    "    \n",
    "    # Print diagnostics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DIAGNOSTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total announcements: {out_df['announcements'].sum()}\")\n",
    "    print(f\"Total withdrawals:   {out_df['withdrawals'].sum()}\")\n",
    "    print(f\"Total flaps:         {out_df['flaps'].sum()}\")\n",
    "    print(f\"Total nadas:         {out_df['nadas'].sum()}\")\n",
    "    print(f\"Total imp_wd:        {out_df['imp_wd'].sum()}\")\n",
    "    print(f\"  - imp_wd_spath:    {out_df['imp_wd_spath'].sum()}\")\n",
    "    print(f\"  - imp_wd_dpath:    {out_df['imp_wd_dpath'].sum()}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_incidents(base_dir: str) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Process all RIPE incident directories.\n",
    "    \n",
    "    Args:\n",
    "        base_dir: Path to RIPE_INCIDENTS directory\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping incident name to features DataFrame\n",
    "    \"\"\"\n",
    "    base_path = Path(base_dir)\n",
    "    results = {}\n",
    "    \n",
    "    for incident_dir in sorted(base_path.iterdir()):\n",
    "        if not incident_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing incident: {incident_dir.name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for csv_path in incident_dir.glob(\"*_labeled.csv\"):\n",
    "            out_path = incident_dir / (csv_path.stem + \"_features.csv\")\n",
    "            \n",
    "            # Uncomment to skip existing files\n",
    "            # if out_path.exists():\n",
    "            #     print(f\"Skipping (exists): {out_path}\")\n",
    "            #     continue\n",
    "            \n",
    "            features_df = process_file(str(csv_path), str(out_path), schema_type='ripe')\n",
    "            if features_df is not None:\n",
    "                results[incident_dir.name] = features_df\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Process a Single File\n",
    "\n",
    "Use this to process a single BGP data file (either normal or incident data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process single file\n",
    "# Uncomment and modify the paths as needed\n",
    "\n",
    "# features_df = process_file(INPUT_FILE, OUTPUT_FILE, schema_type=SCHEMA_TYPE)\n",
    "# features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Process All RIPE Incidents\n",
    "\n",
    "Use this to batch process all incident directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all RIPE incidents\n",
    "# Uncomment to run\n",
    "\n",
    "# all_results = process_all_incidents(RIPE_INCIDENTS_DIR)\n",
    "# print(f\"\\nProcessed {len(all_results)} incidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Analysis (Optional)\n",
    "\n",
    "After extraction, you can analyze the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_features(features_df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Display summary statistics for extracted features.\n",
    "    \"\"\"\n",
    "    print(\"Feature Summary Statistics\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Numeric columns only\n",
    "    numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    summary = features_df[numeric_cols].describe().T\n",
    "    summary['non_zero'] = (features_df[numeric_cols] != 0).sum()\n",
    "    summary['non_zero_pct'] = summary['non_zero'] / len(features_df) * 100\n",
    "    \n",
    "    print(summary[['mean', 'std', 'min', 'max', 'non_zero_pct']].round(2).to_string())\n",
    "    \n",
    "    # Label distribution\n",
    "    if 'label' in features_df.columns:\n",
    "        print(\"\\nLabel Distribution\")\n",
    "        print(\"-\"*40)\n",
    "        label_counts = features_df['label'].value_counts()\n",
    "        for label, count in label_counts.items():\n",
    "            pct = count / len(features_df) * 100\n",
    "            print(f\"  {label}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "# Example usage:\n",
    "# analyze_features(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Test\n",
    "\n",
    "Test the feature extraction with sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample test data\n",
    "test_data = pd.DataFrame({\n",
    "    'Timestamp': pd.to_datetime([\n",
    "        '2025-01-01 00:00:00.100',\n",
    "        '2025-01-01 00:00:00.200',\n",
    "        '2025-01-01 00:00:00.300',\n",
    "        '2025-01-01 00:00:00.400',\n",
    "        '2025-01-01 00:00:00.500',\n",
    "    ]),\n",
    "    'Subtype': ['ANNOUNCE', 'ANNOUNCE', 'WITHDRAW', 'ANNOUNCE', 'ANNOUNCE'],\n",
    "    'Prefix': ['10.0.0.0/24', '10.0.0.0/24', '10.0.0.0/24', '10.0.0.0/24', '10.0.1.0/24'],\n",
    "    'Peer_IP': ['192.168.1.1', '192.168.1.1', '192.168.1.1', '192.168.1.1', '192.168.1.1'],\n",
    "    'Peer_ASN': [65001, 65001, 65001, 65001, 65001],\n",
    "    'AS_Path': ['65001 65002', '65001 65003', '', '65001 65002', '65001 65004'],\n",
    "    'Origin': ['IGP', 'IGP', '', 'IGP', 'INCOMPLETE'],\n",
    "    'Next_Hop': ['192.168.1.1', '192.168.1.1', '', '192.168.1.1', '192.168.1.1'],\n",
    "    'Label': ['normal', 'normal', 'normal', 'normal', 'normal'],\n",
    "})\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print(test_data.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Extract features\n",
    "test_features = extract_features(test_data)\n",
    "\n",
    "print(\"Extracted Features:\")\n",
    "for k, v in sorted(test_features.items()):\n",
    "    print(f\"  {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}