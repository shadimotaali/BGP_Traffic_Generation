{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 1: Import Library - V7 FIXED VERSION\n",
    "# ======================================\n",
    "# This version fixes ALL correlation issues found in v5/v6:\n",
    "# 1. Fixed sample_edit_distance bug (undefined create_large_max)\n",
    "# 2. Decoupled withdrawals from flaps (was ~100%, now ~42%)\n",
    "# 3. Increased withdrawal->NADAS correlation (was ~5%, now ~67%)\n",
    "# 4. Reduced announcements->dups over-correlation (was ~85%, now ~33%)\n",
    "\n",
    "from scapy.all import IP, IPv6, TCP, Ether, Padding, wrpcap, raw, rdpcap, load_contrib\n",
    "from scapy.contrib.bgp import *\n",
    "from scapy.utils import PcapReader\n",
    "from scipy.stats import pareto, weibull_min\n",
    "import datetime\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "import struct\n",
    "import traceback\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple, Set, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "load_contrib('bgp')\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = \"/home/user/BGP_Traffic_Generation/pcaps\"\n",
    "RESULTS_DIR = \"/home/user/BGP_Traffic_Generation/results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "print(\"V7 Output directory created:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 2: V7 CORRELATION FIXES (ALL INLINE)\n",
    "# ======================================\n",
    "# Target correlations from real RIPE data\n",
    "REAL_CORRELATIONS = {\n",
    "    ('withdrawals', 'nadas'): 0.671,\n",
    "    ('imp_wd_dpath', 'unique_as_path_max'): 0.319,\n",
    "    ('withdrawals', 'flaps'): 0.425,\n",
    "    ('announcements', 'dups'): 0.335,\n",
    "    ('withdrawals', 'imp_wd_spath'): 0.520,\n",
    "    ('edit_distance_dict_3', 'edit_distance_dict_4'): 0.463,\n",
    "    ('imp_wd_spath', 'flaps'): 0.488,\n",
    "    ('withdrawals', 'imp_wd'): 0.329,\n",
    "    ('imp_wd', 'flaps'): 0.302,\n",
    "    ('nadas', 'flaps'): 0.352,\n",
    "    ('edit_distance_max', 'edit_distance_dict_3'): 0.356,\n",
    "    ('unique_as_path_max', 'edit_distance_avg'): 0.325,\n",
    "    ('dups', 'nadas'): 0.322,\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class PrefixBehaviorProfileV7:\n",
    "    \"\"\"V7 prefix behavior - FIXED correlation logic\"\"\"\n",
    "    activity_level: str = 'normal'\n",
    "    # Standalone withdrawals (NOT tied to flaps) - KEY FIX\n",
    "    has_standalone_withdrawals: bool = False\n",
    "    withdrawal_count: int = 0\n",
    "    withdrawal_triggers_nadas: bool = False\n",
    "    nadas_count_after_withdrawal: int = 0\n",
    "    # Flapping (separate from withdrawals)\n",
    "    is_flapping: bool = False\n",
    "    flap_count: int = 0\n",
    "    # Implicit withdrawals\n",
    "    has_imp_wd: bool = False\n",
    "    imp_wd_count: int = 0\n",
    "    imp_wd_triggers_withdrawal: bool = False\n",
    "    has_imp_wd_spath: bool = False\n",
    "    imp_wd_spath_count: int = 0\n",
    "    imp_wd_spath_triggers_withdrawal: bool = False\n",
    "    # Path diversity\n",
    "    target_unique_paths: int = 2\n",
    "    # Duplicates\n",
    "    has_duplicates: bool = False\n",
    "    duplicate_count: int = 0\n",
    "    duplicates_with_nadas: bool = False\n",
    "    duplicates_standalone: bool = False\n",
    "    # Edit distance\n",
    "    edit_distance_cluster: str = 'small'\n",
    "    target_announcements: int = 2\n",
    "\n",
    "def sample_prefix_behavior_v7():\n",
    "    \"\"\"V7: Sample prefix behavior with FIXED correlations\"\"\"\n",
    "    profile = PrefixBehaviorProfileV7()\n",
    "    roll = random.random()\n",
    "    \n",
    "    if roll < 0.35:  # Single prefix (35%)\n",
    "        profile.activity_level = 'single'\n",
    "        profile.target_announcements = 1\n",
    "        profile.target_unique_paths = 1\n",
    "        profile.edit_distance_cluster = 'none'\n",
    "    elif roll < 0.55:  # Stable (20%)\n",
    "        profile.activity_level = 'stable'\n",
    "        profile.target_announcements = random.randint(1, 3)\n",
    "        if random.random() < 0.08:\n",
    "            profile.has_standalone_withdrawals = True\n",
    "            profile.withdrawal_count = 1\n",
    "            profile.withdrawal_triggers_nadas = True\n",
    "            profile.nadas_count_after_withdrawal = 1\n",
    "        if random.random() < 0.1:\n",
    "            profile.has_duplicates = True\n",
    "            profile.duplicate_count = 1\n",
    "            profile.duplicates_standalone = True\n",
    "        profile.edit_distance_cluster = 'small'\n",
    "    elif roll < 0.75:  # Normal (20%)\n",
    "        profile.activity_level = 'normal'\n",
    "        profile.target_announcements = random.randint(2, 5)\n",
    "        if random.random() < 0.25:\n",
    "            profile.has_standalone_withdrawals = True\n",
    "            profile.withdrawal_count = random.randint(1, 2)\n",
    "            profile.withdrawal_triggers_nadas = random.random() < 0.75\n",
    "            profile.nadas_count_after_withdrawal = random.randint(1, 2)\n",
    "        if random.random() < 0.12:\n",
    "            profile.is_flapping = True\n",
    "            profile.flap_count = random.randint(1, 2)\n",
    "        if random.random() < 0.15:\n",
    "            profile.has_imp_wd = True\n",
    "            profile.imp_wd_count = random.randint(1, 2)\n",
    "            profile.imp_wd_triggers_withdrawal = random.random() < 0.3\n",
    "        if random.random() < 0.20:\n",
    "            profile.has_duplicates = True\n",
    "            profile.duplicate_count = random.randint(1, 2)\n",
    "            profile.duplicates_with_nadas = random.random() < 0.5\n",
    "            profile.duplicates_standalone = random.random() < 0.3\n",
    "        profile.edit_distance_cluster = random.choices(['small', 'medium'], weights=[0.7, 0.3])[0]\n",
    "    elif roll < 0.90:  # Active (15%)\n",
    "        profile.activity_level = 'active'\n",
    "        profile.target_announcements = random.randint(4, 8)\n",
    "        if random.random() < 0.40:\n",
    "            profile.has_standalone_withdrawals = True\n",
    "            profile.withdrawal_count = random.randint(1, 3)\n",
    "            profile.withdrawal_triggers_nadas = random.random() < 0.80\n",
    "            profile.nadas_count_after_withdrawal = random.randint(1, 3)\n",
    "        if random.random() < 0.25:\n",
    "            profile.is_flapping = True\n",
    "            profile.flap_count = random.randint(1, 3)\n",
    "        if random.random() < 0.25:\n",
    "            profile.has_imp_wd = True\n",
    "            profile.imp_wd_count = random.randint(1, 3)\n",
    "            profile.imp_wd_triggers_withdrawal = random.random() < 0.4\n",
    "        if random.random() < 0.20:\n",
    "            profile.has_imp_wd_spath = True\n",
    "            profile.imp_wd_spath_count = random.randint(1, 2)\n",
    "            profile.imp_wd_spath_triggers_withdrawal = random.random() < 0.6\n",
    "        if random.random() < 0.30:\n",
    "            profile.has_duplicates = True\n",
    "            profile.duplicate_count = random.randint(1, 3)\n",
    "            profile.duplicates_with_nadas = random.random() < 0.6\n",
    "        profile.edit_distance_cluster = random.choices(['small', 'medium', 'large'], weights=[0.3, 0.5, 0.2])[0]\n",
    "    else:  # Unstable (10%)\n",
    "        profile.activity_level = 'unstable'\n",
    "        profile.target_announcements = random.randint(6, 15)\n",
    "        profile.has_standalone_withdrawals = True\n",
    "        profile.withdrawal_count = random.randint(2, 5)\n",
    "        profile.withdrawal_triggers_nadas = True\n",
    "        profile.nadas_count_after_withdrawal = random.randint(2, 4)\n",
    "        if random.random() < 0.60:\n",
    "            profile.is_flapping = True\n",
    "            profile.flap_count = random.randint(2, 4)\n",
    "        profile.has_imp_wd = True\n",
    "        profile.imp_wd_count = random.randint(2, 4)\n",
    "        profile.imp_wd_triggers_withdrawal = random.random() < 0.5\n",
    "        profile.has_imp_wd_spath = True\n",
    "        profile.imp_wd_spath_count = random.randint(1, 3)\n",
    "        profile.has_duplicates = True\n",
    "        profile.duplicate_count = random.randint(2, 5)\n",
    "        profile.duplicates_with_nadas = True\n",
    "        profile.edit_distance_cluster = random.choices(['medium', 'large'], weights=[0.4, 0.6])[0]\n",
    "    \n",
    "    return profile\n",
    "\n",
    "def sample_edit_distance_v7(cluster, create_large_max=False):\n",
    "    \"\"\"V7 FIXED: sample edit distance with proper parameter\"\"\"\n",
    "    if cluster == 'none':\n",
    "        return 0\n",
    "    elif cluster == 'small':\n",
    "        return random.choices([0, 1, 2], weights=[0.3, 0.5, 0.2])[0]\n",
    "    elif cluster == 'medium':\n",
    "        if create_large_max:\n",
    "            return random.choices([2, 3, 4], weights=[0.3, 0.5, 0.2])[0]\n",
    "        return random.choices([1, 2, 3], weights=[0.25, 0.50, 0.25])[0]\n",
    "    else:  # large\n",
    "        if create_large_max:\n",
    "            return random.choices([3, 4, 5, 6], weights=[0.25, 0.35, 0.25, 0.15])[0]\n",
    "        return random.choices([3, 4, 5, 6], weights=[0.35, 0.35, 0.20, 0.10])[0]\n",
    "\n",
    "def calculate_edit_distance(path1, path2):\n",
    "    \"\"\"Calculate Levenshtein edit distance\"\"\"\n",
    "    if not path1 or not path2:\n",
    "        return 0\n",
    "    m, n = len(path1), len(path2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if path1[i-1] == path2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    return dp[m][n]\n",
    "\n",
    "print(\"V7 Correlation fixes loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 3: AS Topology Generation\n",
    "# ======================================\n",
    "\n",
    "def generate_as_topology():\n",
    "    \"\"\"Generate realistic BGP topology\"\"\"\n",
    "    as_numbers = {\n",
    "        \"tier1\": [1299, 3356, 174, 3257, 6762],\n",
    "        \"tier2\": [6939, 1273, 3320, 6453, 2914, 5511, 7018],\n",
    "        \"tier3\": [41336, 35060, 34554, 49544, 50673, 39126, 48292, 62041,\n",
    "                  45899, 51697, 60781, 44002, 56630, 31027, 64512],\n",
    "        \"ixp_content\": [13335, 15169, 32934]\n",
    "    }\n",
    "    \n",
    "    topology = {}\n",
    "    for tier, asn_list in as_numbers.items():\n",
    "        tier_level = int(tier.replace(\"tier\", \"\")) if \"tier\" in tier else 4\n",
    "        for asn in asn_list:\n",
    "            topology[asn] = {\"tier\": tier_level, \"neighbors\": [], \"relationships\": {}}\n",
    "    \n",
    "    # Connect Tier 1s (full mesh)\n",
    "    for i, asn1 in enumerate(as_numbers[\"tier1\"]):\n",
    "        for asn2 in as_numbers[\"tier1\"][i+1:]:\n",
    "            topology[asn1][\"neighbors\"].append(asn2)\n",
    "            topology[asn2][\"neighbors\"].append(asn1)\n",
    "            topology[asn1][\"relationships\"][asn2] = \"peer\"\n",
    "            topology[asn2][\"relationships\"][asn1] = \"peer\"\n",
    "    \n",
    "    # Connect Tier 2s to Tier 1s\n",
    "    for asn2 in as_numbers[\"tier2\"]:\n",
    "        providers = random.sample(as_numbers[\"tier1\"], random.randint(1, 3))\n",
    "        for asn1 in providers:\n",
    "            topology[asn2][\"neighbors\"].append(asn1)\n",
    "            topology[asn1][\"neighbors\"].append(asn2)\n",
    "            topology[asn2][\"relationships\"][asn1] = \"provider\"\n",
    "            topology[asn1][\"relationships\"][asn2] = \"customer\"\n",
    "    \n",
    "    # Connect Tier 2s to each other\n",
    "    for i, asn1 in enumerate(as_numbers[\"tier2\"]):\n",
    "        potential_peers = as_numbers[\"tier2\"][i+1:]\n",
    "        if potential_peers:\n",
    "            peers = random.sample(potential_peers, min(random.randint(1, 3), len(potential_peers)))\n",
    "            for asn2 in peers:\n",
    "                if asn2 not in topology[asn1][\"neighbors\"]:\n",
    "                    topology[asn1][\"neighbors\"].append(asn2)\n",
    "                    topology[asn2][\"neighbors\"].append(asn1)\n",
    "                    topology[asn1][\"relationships\"][asn2] = \"peer\"\n",
    "                    topology[asn2][\"relationships\"][asn1] = \"peer\"\n",
    "    \n",
    "    # Connect Tier 3s to Tier 2s\n",
    "    for asn3 in as_numbers[\"tier3\"]:\n",
    "        providers = random.sample(as_numbers[\"tier2\"], random.randint(1, 2))\n",
    "        for asn2 in providers:\n",
    "            topology[asn3][\"neighbors\"].append(asn2)\n",
    "            topology[asn2][\"neighbors\"].append(asn3)\n",
    "            topology[asn3][\"relationships\"][asn2] = \"provider\"\n",
    "            topology[asn2][\"relationships\"][asn3] = \"customer\"\n",
    "    \n",
    "    # Connect IXPs\n",
    "    for ixp_asn in as_numbers[\"ixp_content\"]:\n",
    "        connections = random.sample(as_numbers[\"tier2\"], 4) + random.sample(as_numbers[\"tier3\"], 3)\n",
    "        for asn in connections:\n",
    "            topology[ixp_asn][\"neighbors\"].append(asn)\n",
    "            topology[asn][\"neighbors\"].append(ixp_asn)\n",
    "            topology[ixp_asn][\"relationships\"][asn] = \"peer\"\n",
    "            topology[asn][\"relationships\"][ixp_asn] = \"peer\"\n",
    "    \n",
    "    main_src_as = 41336\n",
    "    main_dst_as = 35060\n",
    "    \n",
    "    if main_dst_as not in topology[main_src_as][\"neighbors\"]:\n",
    "        topology[main_src_as][\"neighbors\"].append(main_dst_as)\n",
    "        topology[main_dst_as][\"neighbors\"].append(main_src_as)\n",
    "        topology[main_src_as][\"relationships\"][main_dst_as] = \"peer\"\n",
    "        topology[main_dst_as][\"relationships\"][main_src_as] = \"peer\"\n",
    "    \n",
    "    return topology, as_numbers, main_src_as, main_dst_as\n",
    "\n",
    "topology, as_numbers, main_src_as, main_dst_as = generate_as_topology()\n",
    "print(f\"Generated topology with {len(topology)} ASes\")\n",
    "print(f\"Main AS pair: AS{main_src_as} <-> AS{main_dst_as}\")\n",
    "\n",
    "# Generate AS pools\n",
    "TIER1_ASES = as_numbers[\"tier1\"]\n",
    "TIER2_ASES = as_numbers[\"tier2\"]\n",
    "TIER3_ASES = as_numbers[\"tier3\"]\n",
    "RARE_AS_POOL = list(range(30000, 35000)) + list(range(45000, 50000)) + list(range(55000, 60000))\n",
    "random.shuffle(RARE_AS_POOL)\n",
    "print(f\"Generated {len(RARE_AS_POOL)} rare ASes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 4: IP and Interface Allocation\n",
    "# ======================================\n",
    "\n",
    "PREDEFINED_PREFIXES = [\"203.0.113.0/24\", \"198.51.100.0/24\", \"192.0.2.0/24\"]\n",
    "NORMAL_TRAFFIC_ID_RANGE = (0x03E8, 0x7527)\n",
    "PREFIX_HIJACK_ID_RANGE = (0x7530, 0x9C3F)\n",
    "PATH_MANIP_ID_RANGE = (0x9C40, 0xC34F)\n",
    "DOS_ATTACK_ID_RANGE = (0xC350, 0xEA5F)\n",
    "ROUTE_LEAK_ID_RANGE = (0xEA60, 0xFFFF)\n",
    "\n",
    "def allocate_ip_addresses(topology, as_numbers, main_src_as, main_dst_as):\n",
    "    ip_allocations = {}\n",
    "    \n",
    "    for tier, asn_list in as_numbers.items():\n",
    "        for asn in asn_list:\n",
    "            if tier == \"tier1\":\n",
    "                octet1, octet2 = 100, random.randint(64, 127)\n",
    "            elif tier == \"tier2\":\n",
    "                octet1, octet2 = 172, random.randint(16, 31)\n",
    "            elif tier == \"tier3\":\n",
    "                octet1, octet2 = 192, 168\n",
    "            else:\n",
    "                octet1, octet2 = 10, random.randint(0, 255)\n",
    "            \n",
    "            router_id = f\"{octet1}.{octet2}.{random.randint(0, 255)}.1\"\n",
    "            \n",
    "            if asn == main_src_as:\n",
    "                announced_prefixes = PREDEFINED_PREFIXES.copy()\n",
    "            else:\n",
    "                announced_prefixes = []\n",
    "                if tier == \"tier1\":\n",
    "                    for _ in range(random.randint(1, 2)):\n",
    "                        prefix = f\"203.{random.randint(0, 254)}.0.0/16\"\n",
    "                        if prefix != \"203.0.113.0/16\":\n",
    "                            announced_prefixes.append(prefix)\n",
    "                elif tier == \"tier2\":\n",
    "                    for _ in range(random.randint(1, 3)):\n",
    "                        prefix = f\"198.51.{random.randint(0, 99)}.0/24\"\n",
    "                        if prefix != \"198.51.100.0/24\":\n",
    "                            announced_prefixes.append(prefix)\n",
    "                elif tier == \"tier3\":\n",
    "                    for _ in range(random.randint(1, 2)):\n",
    "                        prefix = f\"192.0.{random.randint(3, 255)}.0/24\"\n",
    "                        if prefix != \"192.0.2.0/24\":\n",
    "                            announced_prefixes.append(prefix)\n",
    "                else:\n",
    "                    announced_prefixes.append(f\"198.18.{random.randint(0, 255)}.0/24\")\n",
    "                \n",
    "                if not announced_prefixes:\n",
    "                    announced_prefixes.append(f\"172.{random.randint(20, 30)}.{random.randint(0, 255)}.0/24\")\n",
    "            \n",
    "            ip_allocations[asn] = {\"router_id\": router_id, \"announced_prefixes\": announced_prefixes, \"interfaces\": {}}\n",
    "    \n",
    "    for asn, info in topology.items():\n",
    "        for neighbor in info[\"neighbors\"]:\n",
    "            if neighbor in ip_allocations[asn][\"interfaces\"]:\n",
    "                continue\n",
    "            link_net1 = random.randint(0, 255)\n",
    "            link_net2 = random.randint(0, 255)\n",
    "            link_net3 = random.randint(0, 63) * 4\n",
    "            if asn < neighbor:\n",
    "                ip_allocations[asn][\"interfaces\"][neighbor] = f\"10.{link_net1}.{link_net2}.{link_net3+1}\"\n",
    "                ip_allocations[neighbor][\"interfaces\"][asn] = f\"10.{link_net1}.{link_net2}.{link_net3+2}\"\n",
    "            else:\n",
    "                ip_allocations[asn][\"interfaces\"][neighbor] = f\"10.{link_net1}.{link_net2}.{link_net3+2}\"\n",
    "                ip_allocations[neighbor][\"interfaces\"][asn] = f\"10.{link_net1}.{link_net2}.{link_net3+1}\"\n",
    "    \n",
    "    return ip_allocations\n",
    "\n",
    "ip_allocations = allocate_ip_addresses(topology, as_numbers, main_src_as, main_dst_as)\n",
    "print(f\"IP allocations done for {len(ip_allocations)} ASes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 5: BGP Session Generator\n",
    "# ======================================\n",
    "\n",
    "pkts = []\n",
    "global_bgp_sessions_ipv4 = {}\n",
    "\n",
    "def generate_bgp_sessions(topology, ip_allocations):\n",
    "    bgp_sessions = {}\n",
    "    all_packets = []\n",
    "    seq_numbers = {}\n",
    "    \n",
    "    for asn, info in topology.items():\n",
    "        for neighbor in info[\"neighbors\"]:\n",
    "            if (asn, neighbor) in seq_numbers:\n",
    "                continue\n",
    "            \n",
    "            src_ipv4 = ip_allocations[asn][\"interfaces\"][neighbor]\n",
    "            dst_ipv4 = ip_allocations[neighbor][\"interfaces\"][asn]\n",
    "            src_router_id = ip_allocations[asn][\"router_id\"]\n",
    "            dst_router_id = ip_allocations[neighbor][\"router_id\"]\n",
    "            \n",
    "            src_mac = \"00:\" + \":\".join([f\"{random.randint(0, 255):02x}\" for _ in range(5)])\n",
    "            dst_mac = \"00:\" + \":\".join([f\"{random.randint(0, 255):02x}\" for _ in range(5)])\n",
    "            \n",
    "            src_port = random.randint(30000, 65000)\n",
    "            dst_port = 179\n",
    "            \n",
    "            seq_a = random.randint(1000, 10000)\n",
    "            seq_b = random.randint(1000, 10000)\n",
    "            seq_numbers[(asn, neighbor)] = (seq_a, seq_b)\n",
    "            \n",
    "            tcp_options = [('MSS', 1460)]\n",
    "            src_ip_id = random.randint(NORMAL_TRAFFIC_ID_RANGE[0], NORMAL_TRAFFIC_ID_RANGE[1])\n",
    "            dst_ip_id = random.randint(NORMAL_TRAFFIC_ID_RANGE[0], NORMAL_TRAFFIC_ID_RANGE[1])\n",
    "            \n",
    "            # TCP handshake\n",
    "            syn_pkt = Ether(src=src_mac, dst=dst_mac)/IP(src=src_ipv4, dst=dst_ipv4, ttl=1, flags=\"DF\", tos=0xC0, id=src_ip_id)/TCP(sport=src_port, dport=dst_port, flags=\"S\", seq=seq_a, window=16384, options=tcp_options)\n",
    "            if len(syn_pkt) < 60: syn_pkt = syn_pkt/Padding(load=b'\\x00' * (60 - len(syn_pkt)))\n",
    "            all_packets.append(syn_pkt)\n",
    "            \n",
    "            synack_pkt = Ether(src=dst_mac, dst=src_mac)/IP(src=dst_ipv4, dst=src_ipv4, ttl=1, flags=0, tos=0xC0, id=dst_ip_id)/TCP(sport=dst_port, dport=src_port, flags=\"SA\", seq=seq_b, ack=seq_a+1, window=16384, options=tcp_options)\n",
    "            if len(synack_pkt) < 60: synack_pkt = synack_pkt/Padding(load=b'\\x00' * (60 - len(synack_pkt)))\n",
    "            all_packets.append(synack_pkt)\n",
    "            \n",
    "            ack_pkt = Ether(src=src_mac, dst=dst_mac)/IP(src=src_ipv4, dst=dst_ipv4, ttl=1, flags=\"DF\", tos=0xC0, id=src_ip_id+1)/TCP(sport=src_port, dport=dst_port, flags=\"A\", seq=seq_a+1, ack=seq_b+1, window=16384)\n",
    "            if len(ack_pkt) < 60: ack_pkt = ack_pkt/Padding(load=b'\\x00' * (60 - len(ack_pkt)))\n",
    "            all_packets.append(ack_pkt)\n",
    "            \n",
    "            seq_a += 1\n",
    "            seq_b += 1\n",
    "            \n",
    "            # BGP OPEN\n",
    "            mp_ipv4_cap = BGPCapMultiprotocol(code=1, length=4, afi=1, safi=1)\n",
    "            rr_std = BGPCapGeneric(code=2, length=0)\n",
    "            as4_cap = BGPCapFourBytesASN(code=65, length=4, asn=asn)\n",
    "            opt_params = [BGPOptParam(param_type=2, param_length=len(mp_ipv4_cap), param_value=mp_ipv4_cap),\n",
    "                          BGPOptParam(param_type=2, param_length=len(rr_std), param_value=rr_std),\n",
    "                          BGPOptParam(param_type=2, param_length=len(as4_cap), param_value=as4_cap)]\n",
    "            \n",
    "            open_a = BGPHeader(type=1)/BGPOpen(version=4, my_as=asn, hold_time=180, bgp_id=src_router_id, opt_params=opt_params)\n",
    "            open_pkt = Ether(src=src_mac, dst=dst_mac)/IP(src=src_ipv4, dst=dst_ipv4, ttl=1, flags=\"DF\", tos=0xC0, id=src_ip_id+2)/TCP(sport=src_port, dport=dst_port, flags=\"PA\", seq=seq_a, ack=seq_b, window=16384)/open_a\n",
    "            if len(open_pkt) < 60: open_pkt = open_pkt/Padding(load=b'\\x00' * (60 - len(open_pkt)))\n",
    "            all_packets.append(open_pkt)\n",
    "            seq_a += len(open_a)\n",
    "            \n",
    "            # KEEPALIVE\n",
    "            keep = BGPKeepAlive()\n",
    "            keep_pkt = Ether(src=src_mac, dst=dst_mac)/IP(src=src_ipv4, dst=dst_ipv4, ttl=1, flags=\"DF\", tos=0xC0, id=src_ip_id+3)/TCP(sport=src_port, dport=dst_port, flags=\"PA\", seq=seq_a, ack=seq_b, window=16384)/keep\n",
    "            if len(keep_pkt) < 60: keep_pkt = keep_pkt/Padding(load=b'\\x00' * (60 - len(keep_pkt)))\n",
    "            all_packets.append(keep_pkt)\n",
    "            seq_a += len(keep)\n",
    "            \n",
    "            bgp_sessions[(asn, neighbor)] = {\n",
    "                \"src_ipv4\": src_ipv4, \"dst_ipv4\": dst_ipv4,\n",
    "                \"src_mac\": src_mac, \"dst_mac\": dst_mac,\n",
    "                \"seq_a\": seq_a, \"seq_b\": seq_b,\n",
    "                \"sport\": src_port, \"dport\": dst_port,\n",
    "                \"src_ip_id\": src_ip_id + 4, \"dst_ip_id\": dst_ip_id + 1\n",
    "            }\n",
    "    \n",
    "    return bgp_sessions, all_packets\n",
    "\n",
    "global_bgp_sessions_ipv4, session_packets = generate_bgp_sessions(topology, ip_allocations)\n",
    "pkts.extend(session_packets)\n",
    "print(f\"Created {len(global_bgp_sessions_ipv4)} BGP sessions\")\n",
    "print(f\"Generated {len(session_packets)} session establishment packets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 6: V7 TRAFFIC GENERATION - FIXED CORRELATIONS\n",
    "# ======================================\n",
    "\n",
    "class PrefixStateTrackerV7:\n",
    "    def __init__(self):\n",
    "        self.states = defaultdict(lambda: {'announced': False, 'path': None, 'paths_seen': set(), 'ases_seen': set()})\n",
    "        self.profiles = {}\n",
    "    \n",
    "    def get_profile(self, prefix):\n",
    "        if prefix not in self.profiles:\n",
    "            self.profiles[prefix] = sample_prefix_behavior_v7()\n",
    "        return self.profiles[prefix]\n",
    "    \n",
    "    def announce(self, prefix, as_path):\n",
    "        state = self.states[prefix]\n",
    "        for asn in as_path:\n",
    "            state['ases_seen'].add(asn)\n",
    "        \n",
    "        if not state['announced']:\n",
    "            state['announced'] = True\n",
    "            state['path'] = as_path\n",
    "            state['paths_seen'].add(tuple(as_path))\n",
    "            return 'new', 0\n",
    "        \n",
    "        prev_path = state['path']\n",
    "        ed = calculate_edit_distance(prev_path, as_path) if prev_path else 0\n",
    "        state['paths_seen'].add(tuple(as_path))\n",
    "        state['path'] = as_path\n",
    "        \n",
    "        if as_path == prev_path:\n",
    "            return 'duplicate', 0\n",
    "        elif len(as_path) < len(prev_path):\n",
    "            return 'imp_wd_spath', ed\n",
    "        elif len(as_path) != len(prev_path):\n",
    "            return 'imp_wd_dpath', ed\n",
    "        else:\n",
    "            return 'imp_wd', ed\n",
    "    \n",
    "    def withdraw(self, prefix):\n",
    "        self.states[prefix]['announced'] = False\n",
    "\n",
    "def vary_as_path_v7(base_path, tier2_ases, rare_pool, var_type='substitute', target_ed=None, preserve_len=False):\n",
    "    if not base_path:\n",
    "        return base_path, 0, False\n",
    "    new_path = base_path.copy()\n",
    "    \n",
    "    if var_type == 'shorten' and len(base_path) > 2 and not preserve_len:\n",
    "        removes = min(2, len(new_path) - 2)\n",
    "        for _ in range(max(1, removes)):\n",
    "            if len(new_path) > 2:\n",
    "                idx = random.randint(1, len(new_path) - 2)\n",
    "                new_path.pop(idx)\n",
    "        return new_path, calculate_edit_distance(base_path, new_path), True\n",
    "    \n",
    "    elif var_type == 'lengthen' and not preserve_len:\n",
    "        pool = rare_pool[:1000] if rare_pool else tier2_ases\n",
    "        new_as = random.choice(pool)\n",
    "        attempts = 0\n",
    "        while new_as in new_path and attempts < 10:\n",
    "            new_as = random.choice(pool)\n",
    "            attempts += 1\n",
    "        if new_as not in new_path and len(new_path) > 1:\n",
    "            pos = random.randint(1, len(new_path) - 1)\n",
    "            new_path.insert(pos, new_as)\n",
    "        return new_path, calculate_edit_distance(base_path, new_path), False\n",
    "    \n",
    "    else:  # substitute\n",
    "        if len(new_path) > 1:\n",
    "            idx = random.randint(1, len(new_path) - 1)\n",
    "            new_as = random.choice(tier2_ases)\n",
    "            attempts = 0\n",
    "            while new_as in new_path and attempts < 10:\n",
    "                new_as = random.choice(tier2_ases)\n",
    "                attempts += 1\n",
    "            if new_as not in new_path:\n",
    "                new_path[idx] = new_as\n",
    "        return new_path, calculate_edit_distance(base_path, new_path), False\n",
    "\n",
    "def generate_as_path_v7(origin_as, profile):\n",
    "    path = [origin_as]\n",
    "    if profile.activity_level == 'unstable':\n",
    "        length = random.choices([4, 5, 6, 7], weights=[0.15, 0.30, 0.35, 0.20])[0]\n",
    "    elif profile.activity_level == 'active':\n",
    "        length = random.choices([4, 5, 6], weights=[0.25, 0.45, 0.30])[0]\n",
    "    else:\n",
    "        length = random.choices([3, 4, 5], weights=[0.35, 0.45, 0.20])[0]\n",
    "    \n",
    "    for hop in range(length - 1):\n",
    "        ratio = hop / (length - 1) if length > 1 else 0\n",
    "        if ratio < 0.3:\n",
    "            pool = RARE_AS_POOL[:2000]\n",
    "        elif ratio < 0.7:\n",
    "            pool = TIER2_ASES\n",
    "        else:\n",
    "            pool = TIER1_ASES if random.random() < 0.3 else TIER2_ASES[:5]\n",
    "        \n",
    "        next_as = random.choice(pool)\n",
    "        attempts = 0\n",
    "        while next_as in path and attempts < 10:\n",
    "            next_as = random.choice(pool)\n",
    "            attempts += 1\n",
    "        if next_as not in path:\n",
    "            path.append(next_as)\n",
    "    return path\n",
    "\n",
    "print(\"V7 traffic generation functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 7: MAIN V7 TRAFFIC GENERATOR\n",
    "# ======================================\n",
    "\n",
    "def generate_traffic_v7(peer_ip, all_prefixes, target_events=500):\n",
    "    \"\"\"V7: Generate traffic with FIXED correlations\"\"\"\n",
    "    tracker = PrefixStateTrackerV7()\n",
    "    events = []\n",
    "    prefix_done = defaultdict(set)\n",
    "    \n",
    "    for _ in range(target_events):\n",
    "        prefix = random.choice(all_prefixes)\n",
    "        profile = tracker.get_profile(prefix)\n",
    "        done = prefix_done[prefix]\n",
    "        \n",
    "        if profile.activity_level in ['unstable', 'active']:\n",
    "            origin = random.choice(RARE_AS_POOL[:1500])\n",
    "        else:\n",
    "            origin = random.choice(TIER2_ASES)\n",
    "        \n",
    "        base_path = generate_as_path_v7(origin, profile)\n",
    "        t = 0.0\n",
    "        \n",
    "        # Single prefix\n",
    "        if profile.activity_level == 'single':\n",
    "            if 'single' not in done:\n",
    "                tracker.announce(prefix, base_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'new'})\n",
    "                done.add('single')\n",
    "            continue\n",
    "        \n",
    "        # V7 FIX 1: STANDALONE WITHDRAWAL -> NADAS (creates withdrawal<->nadas correlation)\n",
    "        if profile.has_standalone_withdrawals and profile.withdrawal_count > 0 and 'wd_nadas' not in done:\n",
    "            # First announce\n",
    "            if not tracker.states[prefix]['announced']:\n",
    "                tracker.announce(prefix, base_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'new'})\n",
    "                t += random.uniform(1.0, 5.0)\n",
    "            \n",
    "            # STANDALONE withdrawal (NOT from flapping)\n",
    "            tracker.withdraw(prefix)\n",
    "            events.append({'time': t, 'action': 'withdraw', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': None, 'event_type': 'withdraw', 'is_standalone': True})\n",
    "            t += random.uniform(0.5, 3.0)\n",
    "            \n",
    "            # NADAS: re-announce with NEW AS\n",
    "            if profile.withdrawal_triggers_nadas:\n",
    "                for nadas_i in range(profile.nadas_count_after_withdrawal):\n",
    "                    nadas_path = base_path.copy()\n",
    "                    new_as = random.choice(RARE_AS_POOL[:2000])\n",
    "                    attempts = 0\n",
    "                    while new_as in nadas_path and attempts < 10:\n",
    "                        new_as = random.choice(RARE_AS_POOL[:2000])\n",
    "                        attempts += 1\n",
    "                    if new_as not in nadas_path and len(nadas_path) > 1:\n",
    "                        nadas_path.insert(random.randint(1, len(nadas_path)-1), new_as)\n",
    "                    \n",
    "                    tracker.announce(prefix, nadas_path)\n",
    "                    events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': nadas_path.copy(), 'event_type': 'nadas', 'is_nadas': True})\n",
    "                    t += random.uniform(0.1, 1.0)\n",
    "                    \n",
    "                    # FIX 13: duplicates with NADAS\n",
    "                    if profile.duplicates_with_nadas and random.random() < 0.5:\n",
    "                        events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': nadas_path.copy(), 'event_type': 'duplicate', 'is_nadas_dup': True})\n",
    "                        t += 0.05\n",
    "            \n",
    "            profile.withdrawal_count -= 1\n",
    "            done.add('wd_nadas')\n",
    "            continue\n",
    "        \n",
    "        # V7 FIX 3: FLAPPING (without automatic withdrawals - decoupled!)\n",
    "        if profile.is_flapping and profile.flap_count > 0 and 'flap' not in done:\n",
    "            # Flapping = rapid path changes, NOT withdrawal-announce pairs\n",
    "            if not tracker.states[prefix]['announced']:\n",
    "                tracker.announce(prefix, base_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'new', 'is_flap': True})\n",
    "                t += random.uniform(0.5, 2.0)\n",
    "            \n",
    "            current_path = base_path.copy()\n",
    "            for cycle in range(profile.flap_count):\n",
    "                # Path change during flap (implicit withdrawal)\n",
    "                var_roll = random.random()\n",
    "                if var_roll < 0.3 and len(current_path) > 2:\n",
    "                    new_path, ed, _ = vary_as_path_v7(current_path, TIER2_ASES, RARE_AS_POOL, 'shorten')\n",
    "                elif var_roll < 0.6:\n",
    "                    new_path, ed, _ = vary_as_path_v7(current_path, TIER2_ASES, RARE_AS_POOL, 'substitute', preserve_len=True)\n",
    "                else:\n",
    "                    new_path, ed, _ = vary_as_path_v7(current_path, TIER2_ASES, RARE_AS_POOL, 'lengthen')\n",
    "                \n",
    "                evt_type, _ = tracker.announce(prefix, new_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': new_path.copy(), 'event_type': evt_type, 'is_flap': True, 'edit_distance': ed})\n",
    "                current_path = new_path\n",
    "                t += random.uniform(0.5, 3.0)\n",
    "            \n",
    "            profile.flap_count = 0\n",
    "            done.add('flap')\n",
    "            continue\n",
    "        \n",
    "        # V7 FIX 5: imp_wd_spath -> withdrawal\n",
    "        if profile.has_imp_wd_spath and profile.imp_wd_spath_triggers_withdrawal and profile.imp_wd_spath_count > 0 and len(base_path) > 2 and 'imp_wd_spath' not in done:\n",
    "            if not tracker.states[prefix]['announced']:\n",
    "                tracker.announce(prefix, base_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'new'})\n",
    "                t += random.uniform(0.5, 2.0)\n",
    "            \n",
    "            short_path, ed, _ = vary_as_path_v7(base_path, TIER2_ASES, RARE_AS_POOL, 'shorten')\n",
    "            tracker.announce(prefix, short_path)\n",
    "            events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': short_path.copy(), 'event_type': 'imp_wd_spath', 'edit_distance': ed})\n",
    "            t += random.uniform(1.0, 3.0)\n",
    "            \n",
    "            tracker.withdraw(prefix)\n",
    "            events.append({'time': t, 'action': 'withdraw', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': None, 'event_type': 'withdraw', 'follows_imp_wd_spath': True})\n",
    "            \n",
    "            profile.imp_wd_spath_count -= 1\n",
    "            done.add('imp_wd_spath')\n",
    "            continue\n",
    "        \n",
    "        # V7 FIX 8: imp_wd -> withdrawal\n",
    "        if profile.has_imp_wd and profile.imp_wd_triggers_withdrawal and profile.imp_wd_count > 0 and 'imp_wd' not in done:\n",
    "            if not tracker.states[prefix]['announced']:\n",
    "                tracker.announce(prefix, base_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'new'})\n",
    "                t += random.uniform(0.5, 2.0)\n",
    "            \n",
    "            new_path, ed, _ = vary_as_path_v7(base_path, TIER2_ASES, RARE_AS_POOL, 'substitute', preserve_len=True)\n",
    "            tracker.announce(prefix, new_path)\n",
    "            events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': new_path.copy(), 'event_type': 'imp_wd', 'edit_distance': ed})\n",
    "            t += random.uniform(1.0, 3.0)\n",
    "            \n",
    "            tracker.withdraw(prefix)\n",
    "            events.append({'time': t, 'action': 'withdraw', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': None, 'event_type': 'withdraw', 'follows_imp_wd': True})\n",
    "            \n",
    "            profile.imp_wd_count -= 1\n",
    "            done.add('imp_wd')\n",
    "            continue\n",
    "        \n",
    "        # V7 FIX 4: Standalone duplicates\n",
    "        if profile.has_duplicates and profile.duplicates_standalone and profile.duplicate_count > 0 and 'standalone_dup' not in done:\n",
    "            if not tracker.states[prefix]['announced']:\n",
    "                tracker.announce(prefix, base_path)\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'new'})\n",
    "                t += random.uniform(0.5, 2.0)\n",
    "            \n",
    "            for _ in range(profile.duplicate_count):\n",
    "                events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': 'duplicate', 'is_standalone_dup': True})\n",
    "                t += random.uniform(0.01, 0.3)\n",
    "            \n",
    "            profile.duplicate_count = 0\n",
    "            done.add('standalone_dup')\n",
    "            continue\n",
    "        \n",
    "        # Default: simple announcement\n",
    "        evt_type, ed = tracker.announce(prefix, base_path)\n",
    "        events.append({'time': t, 'action': 'announce', 'prefix': prefix, 'peer_ip': peer_ip, 'as_path': base_path.copy(), 'event_type': evt_type, 'edit_distance': ed})\n",
    "    \n",
    "    return events, tracker\n",
    "\n",
    "print(\"V7 main traffic generator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 8: GENERATE V7 TRAFFIC\n",
    "# ======================================\n",
    "\n",
    "# Get all prefixes\n",
    "all_prefixes = PREDEFINED_PREFIXES.copy()\n",
    "for asn, info in ip_allocations.items():\n",
    "    all_prefixes.extend(info[\"announced_prefixes\"])\n",
    "all_prefixes = list(set(all_prefixes))\n",
    "\n",
    "# Get peer IP\n",
    "peer_ip = ip_allocations[main_src_as][\"interfaces\"].get(main_dst_as, ip_allocations[main_src_as][\"router_id\"])\n",
    "\n",
    "print(\"Generating V7 traffic with FIXED correlations...\")\n",
    "print(f\"Using {len(all_prefixes)} prefixes\")\n",
    "\n",
    "# Generate events\n",
    "TARGET_EVENTS = 1000  # Adjust as needed\n",
    "events, tracker = generate_traffic_v7(peer_ip, all_prefixes, target_events=TARGET_EVENTS)\n",
    "\n",
    "print(f\"Generated {len(events)} events\")\n",
    "\n",
    "# Statistics\n",
    "stats = {\n",
    "    'total': len(events),\n",
    "    'announcements': sum(1 for e in events if e['action'] == 'announce'),\n",
    "    'withdrawals': sum(1 for e in events if e['action'] == 'withdraw'),\n",
    "    'standalone_wd': sum(1 for e in events if e.get('is_standalone')),\n",
    "    'nadas': sum(1 for e in events if e.get('is_nadas')),\n",
    "    'flaps': sum(1 for e in events if e.get('is_flap')),\n",
    "    'duplicates': sum(1 for e in events if e['event_type'] == 'duplicate'),\n",
    "    'imp_wd': sum(1 for e in events if e['event_type'] == 'imp_wd'),\n",
    "    'imp_wd_spath': sum(1 for e in events if e['event_type'] == 'imp_wd_spath'),\n",
    "}\n",
    "\n",
    "print(\"\\nV7 Traffic Statistics:\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Correlation indicators\n",
    "if stats['withdrawals'] > 0:\n",
    "    print(f\"\\nCorrelation Indicators:\")\n",
    "    print(f\"  NADAS/Withdrawals: {stats['nadas']/max(1,stats['withdrawals']):.2f} (target: ~0.67)\")\n",
    "    print(f\"  Flaps/Withdrawals: {stats['flaps']/max(1,stats['withdrawals']):.2f} (target: ~0.42)\")\n",
    "    print(f\"  Dups/Announcements: {stats['duplicates']/max(1,stats['announcements']):.2f} (target: ~0.33)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 9: CONVERT EVENTS TO PACKETS\n",
    "# ======================================\n",
    "\n",
    "def event_to_packet(event, session_info, main_src_as):\n",
    "    packets = []\n",
    "    src_ipv4 = session_info[\"src_ipv4\"]\n",
    "    dst_ipv4 = session_info[\"dst_ipv4\"]\n",
    "    src_mac = session_info[\"src_mac\"]\n",
    "    dst_mac = session_info[\"dst_mac\"]\n",
    "    sport = session_info[\"sport\"]\n",
    "    dport = session_info[\"dport\"]\n",
    "    seq_a = session_info[\"seq_a\"]\n",
    "    seq_b = session_info[\"seq_b\"]\n",
    "    src_ip_id = session_info[\"src_ip_id\"]\n",
    "    \n",
    "    prefix = event['prefix']\n",
    "    as_path = event.get('as_path', [main_src_as])\n",
    "    \n",
    "    if event['action'] == 'announce':\n",
    "        origin = BGPPathAttr(type_flags=0x40, type_code=1)\n",
    "        origin.attribute = BGPPAOrigin(origin=0)\n",
    "        \n",
    "        as_path_attr = BGPPathAttr(type_flags=0x40, type_code=2)\n",
    "        as_path_segment = BGPPAASPath()\n",
    "        segment = BGPPAASPath.ASPathSegment(segment_type=2, segment_length=len(as_path), segment_value=as_path)\n",
    "        as_path_segment.segments = [segment]\n",
    "        as_path_attr.attribute = as_path_segment\n",
    "        \n",
    "        next_hop = BGPPathAttr(type_flags=0x40, type_code=3)\n",
    "        next_hop.attribute = BGPPANextHop(next_hop=src_ipv4)\n",
    "        \n",
    "        med = BGPPathAttr(type_flags=0x80, type_code=4)\n",
    "        med.attribute = BGPPAMultiExitDisc(med=100)\n",
    "        \n",
    "        local_pref = BGPPathAttr(type_flags=0x40, type_code=5)\n",
    "        local_pref.attribute = BGPPALocalPref(local_pref=200)\n",
    "        \n",
    "        update = BGPHeader(type=2)/BGPUpdate()\n",
    "        update.path_attr = [origin, as_path_attr, next_hop, med, local_pref]\n",
    "        update.nlri.append(BGPNLRI_IPv4(prefix=prefix))\n",
    "        \n",
    "        pkt = Ether(src=src_mac, dst=dst_mac)/IP(src=src_ipv4, dst=dst_ipv4, ttl=1, flags=\"DF\", tos=0xC0, id=src_ip_id)/TCP(sport=sport, dport=dport, flags=\"PA\", seq=seq_a, ack=seq_b, window=16384)/update\n",
    "        if len(pkt) < 60:\n",
    "            pkt = pkt/Padding(load=b'\\x00' * (60 - len(pkt)))\n",
    "        packets.append(pkt)\n",
    "        session_info[\"seq_a\"] = seq_a + len(update)\n",
    "        session_info[\"src_ip_id\"] = src_ip_id + 1\n",
    "        \n",
    "    elif event['action'] == 'withdraw':\n",
    "        update = BGPHeader(type=2)/BGPUpdate()\n",
    "        update.withdrawn_routes = [BGPNLRI_IPv4(prefix=prefix)]\n",
    "        \n",
    "        pkt = Ether(src=src_mac, dst=dst_mac)/IP(src=src_ipv4, dst=dst_ipv4, ttl=1, flags=\"DF\", tos=0xC0, id=src_ip_id)/TCP(sport=sport, dport=dport, flags=\"PA\", seq=seq_a, ack=seq_b, window=16384)/update\n",
    "        if len(pkt) < 60:\n",
    "            pkt = pkt/Padding(load=b'\\x00' * (60 - len(pkt)))\n",
    "        packets.append(pkt)\n",
    "        session_info[\"seq_a\"] = seq_a + len(update)\n",
    "        session_info[\"src_ip_id\"] = src_ip_id + 1\n",
    "    \n",
    "    return packets\n",
    "\n",
    "# Convert all events to packets\n",
    "session_key = (main_src_as, main_dst_as)\n",
    "if session_key not in global_bgp_sessions_ipv4:\n",
    "    session_key = (main_dst_as, main_src_as)\n",
    "\n",
    "if session_key in global_bgp_sessions_ipv4:\n",
    "    session_info = global_bgp_sessions_ipv4[session_key]\n",
    "    \n",
    "    for event in events:\n",
    "        new_pkts = event_to_packet(event, session_info, main_src_as)\n",
    "        pkts.extend(new_pkts)\n",
    "\n",
    "print(f\"Total packets generated: {len(pkts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# PART 10: SAVE OUTPUT FILES\n",
    "# ======================================\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save PCAP\n",
    "pcap_file = f\"{OUTPUT_DIR}/bgp_traffic_v7_{timestamp}.pcap\"\n",
    "wrpcap(pcap_file, pkts)\n",
    "print(f\"PCAP saved to: {pcap_file}\")\n",
    "\n",
    "# Save CSV for feature extraction\n",
    "csv_file = f\"{RESULTS_DIR}/bgp_updates_analysis_{timestamp}.csv\"\n",
    "with open(csv_file, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Timestamp', 'Type', 'Subtype', 'Peer_IP', 'Peer_ASN', 'Prefix', 'AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities', 'Label'])\n",
    "    \n",
    "    base_time = datetime.datetime.now()\n",
    "    for event in events:\n",
    "        ts = base_time + datetime.timedelta(seconds=event.get('time', 0))\n",
    "        subtype = 'ANNOUNCE' if event['action'] == 'announce' else 'WITHDRAW'\n",
    "        as_path = event.get('as_path', [])\n",
    "        as_path_str = ' '.join(map(str, as_path)) if as_path else ''\n",
    "        \n",
    "        writer.writerow([\n",
    "            ts.strftime('%Y-%m-%d %H:%M:%S.%f'),\n",
    "            'UPDATE', subtype, peer_ip, main_src_as, event['prefix'],\n",
    "            as_path_str, 'IGP', peer_ip, 100, 200, f'{main_src_as}:200', 'normal'\n",
    "        ])\n",
    "\n",
    "print(f\"CSV saved to: {csv_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"V7 GENERATION COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  PCAP: {pcap_file}\")\n",
    "print(f\"  CSV:  {csv_file}\")\n",
    "print(f\"\\nNow run feature extraction on the CSV file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
