{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries",
        "import pandas as pd",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "import seaborn as sns",
        "from scipy import stats",
        "from scipy.stats import wasserstein_distance",
        "from scipy.spatial.distance import mahalanobis",
        "from sklearn.preprocessing import StandardScaler",
        "from sklearn.decomposition import PCA",
        "from sklearn.manifold import TSNE",
        "from statsmodels.stats.multitest import multipletests",
        "import warnings",
        "import json",
        "warnings.filterwarnings('ignore')",
        "",
        "# Set style",
        "plt.style.use('seaborn-v0_8-whitegrid')",
        "sns.set_palette('husl')",
        "from datetime import datetime",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")",
        "",
        "print(\"Libraries loaded successfully!\")",
        "print(f\"Analysis timestamp: {TIMESTAMP}\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================",
        "# CONFIGURATION - Modify these paths as needed",
        "# =============================================================================",
        "",
        "# Data paths",
        "SYNTHETIC_DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/results/SMOTE_enhanced/synthetic_likely_normal_enhanced_smote.csv'",
        "REAL_DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/results/final_label_results_HDBSCAN/rrc05_updates_20251216_extracted_discovered.csv'",
        "",
        "# Output directory for results",
        "OUTPUT_DIR = '/home/smotaali/BGP_Traffic_Generation/results/SMOTE_enhanced/normal/'",
        "# Random seed for reproducibility",
        "RANDOM_SEED = 42",
        "",
        "# Significance level for statistical tests",
        "ALPHA = 0.05",
        "",
        "# Columns to exclude from analysis",
        "EXCLUDE_COLS = ['sequence_id', 'timestep', 'label', 'window_start', 'window_end', ",
        "                'discovered_label', 'generation_method', 'log_transform_used', 'bgp_constraints_enforced']",
        "",
        "# KS statistic thresholds (for large sample interpretation)",
        "KS_EXCELLENT_THRESHOLD = 0.05  # Very similar distributions",
        "KS_GOOD_THRESHOLD = 0.10       # Reasonably similar",
        "KS_MODERATE_THRESHOLD = 0.15   # Moderate similarity",
        "",
        "# Cohen's d cap for numeric stability",
        "COHENS_D_CAP = 10.0  # Cap extreme values",
        "",
        "# =============================================================================",
        "# ENHANCED: Per-Feature Importance Weights for BGP Semantics",
        "# =============================================================================",
        "# Operationally critical BGP features get higher weights in similarity scoring",
        "# These weights reflect the importance of each feature for BGP traffic validation",
        "",
        "FEATURE_IMPORTANCE_WEIGHTS = {",
        "    # --- PRIMARY BGP METRICS (Critical for traffic characterization) ---",
        "    'announcements': 1.5,           # Core BGP activity",
        "    'withdrawals': 1.5,             # Core BGP activity  ",
        "    'flaps': 1.5,                   # Network instability indicator",
        "    'nadas': 1.3,                   # No-Advertise-to-Any (policy-related)",
        "    'rare_ases': 1.3,               # Unusual AS appearances",
        "    ",
        "    # --- SECONDARY BGP METRICS (Important but less critical) ---",
        "    'unique_prefixes': 1.2,         # Prefix diversity",
        "    'unique_ases': 1.2,             # AS diversity",
        "    'avg_path_length': 1.2,         # Routing characteristics",
        "    'max_path_length': 1.1,",
        "    'avg_edit_distance': 1.1,",
        "    ",
        "    # --- EDIT DISTANCE BUCKETS (Statistical, less operationally critical) ---",
        "    'edit_dist_0': 0.8,             # No change",
        "    'edit_dist_1': 0.9,",
        "    'edit_dist_2': 0.9,",
        "    'edit_dist_3': 0.9,",
        "    'edit_dist_4_plus': 0.8,",
        "    ",
        "    # --- DEFAULT for unlisted features ---",
        "    '_default': 1.0",
        "}",
        "",
        "# Top-K worst features to display",
        "TOP_K_WORST_FEATURES = 10",
        "",
        "print(\"Configuration set!\")",
        "print(f\"\\nFeature importance weights defined for {len(FEATURE_IMPORTANCE_WEIGHTS)-1} features\")",
        "print(f\"Top-K worst features to report: {TOP_K_WORST_FEATURES}\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets",
        "synthetic_full = pd.read_csv(SYNTHETIC_DATA_PATH)",
        "real_full = pd.read_csv(REAL_DATA_PATH)",
        "",
        "print(\"=\" * 60)",
        "print(\"DATASET OVERVIEW\")",
        "print(\"=\" * 60)",
        "print(f\"\\nSynthetic Data Shape: {synthetic_full.shape}\")",
        "print(f\"Real Data Shape: {real_full.shape}\")",
        "print(f\"\\nSynthetic Labels:\\n{synthetic_full['label'].value_counts()}\")",
        "print(f\"\\nReal Labels:\\n{real_full['discovered_label'].value_counts()}\")",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter only normal traffic from both datasets",
        "synthetic_normal = synthetic_full[synthetic_full['label'] == 'synthetic'].copy()",
        "real_normal = real_full[real_full['discovered_label'] == 'likely_normal'].copy()",
        "",
        "print(f\"Synthetic Normal samples: {len(synthetic_normal)}\")",
        "print(f\"Real Normal samples: {len(real_normal)}\")",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample equal amounts for fair comparison",
        "n_samples = min(len(synthetic_normal), len(real_normal))",
        "print(f\"\\nUsing {n_samples} samples from each dataset for comparison\")",
        "",
        "# Use all synthetic normal (smaller dataset)",
        "synthetic_sampled = synthetic_normal.copy()",
        "",
        "# Random sample from real data",
        "np.random.seed(RANDOM_SEED)",
        "real_sampled = real_normal.sample(n=n_samples, random_state=RANDOM_SEED).copy()",
        "",
        "print(f\"Synthetic sampled: {len(synthetic_sampled)}\")",
        "print(f\"Real sampled: {len(real_sampled)}\")",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature columns (exclude metadata columns)",
        "feature_cols = [col for col in synthetic_sampled.columns if col not in EXCLUDE_COLS]",
        "print(f\"\\nNumber of features to compare: {len(feature_cols)}\")",
        "print(f\"\\nFeatures: {feature_cols}\")",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPROVED: Comprehensive constant feature handling",
        "# - Constant in both: exclude from analysis (no information)",
        "# - Constant in synthetic only: FLAG as GAN issue (not learning variance)",
        "# - Constant in real only: FLAG as data artifact (rare)",
        "constant_both = []",
        "constant_synthetic_only = []  # Potential GAN issue",
        "constant_real_only = []       # Potential data issue",
        "valid_features = []",
        "",
        "for col in feature_cols:",
        "    syn_std = synthetic_sampled[col].std()",
        "    real_std = real_sampled[col].std()",
        "    ",
        "    if syn_std == 0 and real_std == 0:",
        "        constant_both.append(col)",
        "    elif syn_std == 0 and real_std > 0:",
        "        constant_synthetic_only.append({",
        "            'feature': col, ",
        "            'real_std': real_std, ",
        "            'real_mean': real_sampled[col].mean(),",
        "            'syn_value': synthetic_sampled[col].iloc[0]",
        "        })",
        "    elif syn_std > 0 and real_std == 0:",
        "        constant_real_only.append({",
        "            'feature': col, ",
        "            'syn_std': syn_std, ",
        "            'syn_mean': synthetic_sampled[col].mean(),",
        "            'real_value': real_sampled[col].iloc[0]",
        "        })",
        "    else:",
        "        valid_features.append(col)",
        "",
        "print(f\"Total feature columns: {len(feature_cols)}\")",
        "print(f\"\\n\" + \"=\" * 60)",
        "print(\"CONSTANT FEATURE ANALYSIS\")",
        "print(\"=\" * 60)",
        "",
        "print(f\"\\nConstant in BOTH datasets (excluded - no information): {len(constant_both)}\")",
        "if constant_both:",
        "    print(f\"   Features: {constant_both}\")",
        "",
        "print(f\"\\n[WARNING] Constant in SYNTHETIC only (GAN issue - not learning variance): {len(constant_synthetic_only)}\")",
        "if constant_synthetic_only:",
        "    for item in constant_synthetic_only:",
        "        print(f\"   - {item['feature']}:\")",
        "        print(f\"     Synthetic: constant at {item['syn_value']:.4f}\")",
        "        print(f\"     Real: mean={item['real_mean']:.4f}, std={item['real_std']:.4f}\")",
        "    print(f\"   ACTION NEEDED: GAN is not learning variance for these features!\")",
        "",
        "print(f\"\\n[WARNING] Constant in REAL only (unusual - check data): {len(constant_real_only)}\")",
        "if constant_real_only:",
        "    for item in constant_real_only:",
        "        print(f\"   - {item['feature']}:\")",
        "        print(f\"     Real: constant at {item['real_value']:.4f}\")",
        "        print(f\"     Synthetic: mean={item['syn_mean']:.4f}, std={item['syn_std']:.4f}\")",
        "",
        "# Store problematic features for final report",
        "problematic_features = {",
        "    'constant_both': constant_both,",
        "    'constant_synthetic_only': constant_synthetic_only,",
        "    'constant_real_only': constant_real_only",
        "}",
        "",
        "# Use only fully valid features for main analysis",
        "feature_cols = valid_features",
        "print(f\"\\nValid features for statistical comparison: {len(feature_cols)}\")",
        "print(f\"   Features: {feature_cols}\")",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract feature matrices",
        "X_synthetic = synthetic_sampled[feature_cols].values",
        "X_real = real_sampled[feature_cols].values",
        "",
        "# Create DataFrames for analysis",
        "df_synthetic = synthetic_sampled[feature_cols].copy()",
        "df_real = real_sampled[feature_cols].copy()",
        "",
        "print(f\"Synthetic feature matrix shape: {X_synthetic.shape}\")",
        "print(f\"Real feature matrix shape: {X_real.shape}\")",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Calibration Check (Preprocessing Bias Detection)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENHANCED: Calibration Check - Detect Preprocessing Bias\n",
        "# =============================================================================\n",
        "# Compare means and standard deviations before and after scaling/normalization\n",
        "# to ensure there is no preprocessing bias between real and synthetic datasets\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CALIBRATION CHECK: Preprocessing Bias Detection\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Store raw statistics BEFORE any scaling\n",
        "calibration_results = []\n",
        "\n",
        "for col in feature_cols:\n",
        "    syn_mean_raw = df_synthetic[col].mean()\n",
        "    syn_std_raw = df_synthetic[col].std()\n",
        "    real_mean_raw = df_real[col].mean()\n",
        "    real_std_raw = df_real[col].std()\n",
        "    \n",
        "    calibration_results.append({\n",
        "        'feature': col,\n",
        "        'syn_mean_raw': syn_mean_raw,\n",
        "        'syn_std_raw': syn_std_raw,\n",
        "        'real_mean_raw': real_mean_raw,\n",
        "        'real_std_raw': real_std_raw,\n",
        "        'mean_diff_raw': abs(syn_mean_raw - real_mean_raw),\n",
        "        'std_ratio_raw': syn_std_raw / real_std_raw if real_std_raw > 0 else np.nan\n",
        "    })\n",
        "\n",
        "calibration_df_raw = pd.DataFrame(calibration_results)\n",
        "\n",
        "# Combine and scale data (same as later analysis)\n",
        "X_combined_for_calibration = np.vstack([X_synthetic, X_real])\n",
        "scaler_calibration = StandardScaler()\n",
        "X_scaled_calibration = scaler_calibration.fit_transform(X_combined_for_calibration)\n",
        "\n",
        "# Split back\n",
        "X_syn_scaled = X_scaled_calibration[:len(X_synthetic)]\n",
        "X_real_scaled = X_scaled_calibration[len(X_synthetic):]\n",
        "\n",
        "# Calculate post-scaling statistics\n",
        "for i, col in enumerate(feature_cols):\n",
        "    calibration_df_raw.loc[calibration_df_raw['feature'] == col, 'syn_mean_scaled'] = X_syn_scaled[:, i].mean()\n",
        "    calibration_df_raw.loc[calibration_df_raw['feature'] == col, 'syn_std_scaled'] = X_syn_scaled[:, i].std()\n",
        "    calibration_df_raw.loc[calibration_df_raw['feature'] == col, 'real_mean_scaled'] = X_real_scaled[:, i].mean()\n",
        "    calibration_df_raw.loc[calibration_df_raw['feature'] == col, 'real_std_scaled'] = X_real_scaled[:, i].std()\n",
        "\n",
        "# Calculate scaled differences\n",
        "calibration_df_raw['mean_diff_scaled'] = abs(calibration_df_raw['syn_mean_scaled'] - calibration_df_raw['real_mean_scaled'])\n",
        "calibration_df_raw['std_ratio_scaled'] = calibration_df_raw['syn_std_scaled'] / calibration_df_raw['real_std_scaled']\n",
        "\n",
        "# Detect potential bias\n",
        "BIAS_THRESHOLD_MEAN = 0.5  # Scaled mean difference threshold\n",
        "BIAS_THRESHOLD_STD = 0.3   # Deviation from 1.0 for std ratio\n",
        "\n",
        "calibration_df_raw['potential_mean_bias'] = calibration_df_raw['mean_diff_scaled'] > BIAS_THRESHOLD_MEAN\n",
        "calibration_df_raw['potential_std_bias'] = abs(calibration_df_raw['std_ratio_scaled'] - 1.0) > BIAS_THRESHOLD_STD\n",
        "\n",
        "print(\"\\n--- RAW DATA STATISTICS (Before Scaling) ---\")\n",
        "print(f\"Mean of mean differences: {calibration_df_raw['mean_diff_raw'].mean():.4f}\")\n",
        "print(f\"Mean of std ratios (syn/real): {calibration_df_raw['std_ratio_raw'].mean():.4f}\")\n",
        "\n",
        "print(\"\\n--- SCALED DATA STATISTICS (After StandardScaler) ---\")\n",
        "print(f\"Mean of mean differences: {calibration_df_raw['mean_diff_scaled'].mean():.4f}\")\n",
        "print(f\"Mean of std ratios (syn/real): {calibration_df_raw['std_ratio_scaled'].mean():.4f}\")\n",
        "\n",
        "# Check for bias\n",
        "n_mean_bias = calibration_df_raw['potential_mean_bias'].sum()\n",
        "n_std_bias = calibration_df_raw['potential_std_bias'].sum()\n",
        "\n",
        "print(f\"\\n--- BIAS DETECTION ---\")\n",
        "print(f\"Features with potential mean bias (scaled diff > {BIAS_THRESHOLD_MEAN}): {n_mean_bias}/{len(feature_cols)}\")\n",
        "print(f\"Features with potential std bias (ratio deviation > {BIAS_THRESHOLD_STD}): {n_std_bias}/{len(feature_cols)}\")\n",
        "\n",
        "if n_mean_bias > 0 or n_std_bias > 0:\n",
        "    print(\"\\n[WARNING] Potential preprocessing bias detected!\")\n",
        "    biased_features = calibration_df_raw[\n",
        "        calibration_df_raw['potential_mean_bias'] | calibration_df_raw['potential_std_bias']\n",
        "    ][['feature', 'mean_diff_scaled', 'std_ratio_scaled', 'potential_mean_bias', 'potential_std_bias']]\n",
        "    print(biased_features.to_string(index=False))\n",
        "else:\n",
        "    print(\"\\n[OK] No significant preprocessing bias detected.\")\n",
        "\n",
        "calibration_df_raw.to_csv(f'{OUTPUT_DIR}/calibration_check_results.csv', index=False)\n",
        "print(f\"\\nCalibration results saved to: {OUTPUT_DIR}/calibration_check_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize calibration check\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Plot 1: Mean differences (raw vs scaled)\n",
        "ax1 = axes[0]\n",
        "x_pos = np.arange(len(feature_cols))\n",
        "width = 0.35\n",
        "ax1.bar(x_pos - width/2, calibration_df_raw['mean_diff_raw'], width, label='Raw', alpha=0.7)\n",
        "ax1.bar(x_pos + width/2, calibration_df_raw['mean_diff_scaled'], width, label='Scaled', alpha=0.7)\n",
        "ax1.set_xlabel('Feature Index')\n",
        "ax1.set_ylabel('Absolute Mean Difference')\n",
        "ax1.set_title('Mean Differences: Raw vs Scaled')\n",
        "ax1.legend()\n",
        "ax1.axhline(y=BIAS_THRESHOLD_MEAN, color='r', linestyle='--', alpha=0.5, label=f'Bias threshold ({BIAS_THRESHOLD_MEAN})')\n",
        "\n",
        "# Plot 2: Std ratios\n",
        "ax2 = axes[1]\n",
        "ax2.bar(x_pos, calibration_df_raw['std_ratio_scaled'], alpha=0.7, color='green')\n",
        "ax2.axhline(y=1.0, color='black', linestyle='-', linewidth=2, label='Perfect match')\n",
        "ax2.axhline(y=1.0 + BIAS_THRESHOLD_STD, color='r', linestyle='--', alpha=0.5)\n",
        "ax2.axhline(y=1.0 - BIAS_THRESHOLD_STD, color='r', linestyle='--', alpha=0.5, label=f'Bias threshold')\n",
        "ax2.set_xlabel('Feature Index')\n",
        "ax2.set_ylabel('Std Ratio (Synthetic/Real)')\n",
        "ax2.set_title('Standard Deviation Ratios (Scaled Data)')\n",
        "ax2.legend()\n",
        "\n",
        "# Plot 3: Scatter of raw vs scaled mean differences\n",
        "ax3 = axes[2]\n",
        "ax3.scatter(calibration_df_raw['mean_diff_raw'], calibration_df_raw['mean_diff_scaled'], alpha=0.6)\n",
        "ax3.set_xlabel('Raw Mean Difference')\n",
        "ax3.set_ylabel('Scaled Mean Difference')\n",
        "ax3.set_title('Calibration: Raw vs Scaled Mean Differences')\n",
        "ax3.axhline(y=BIAS_THRESHOLD_MEAN, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "# Add feature labels for outliers\n",
        "for idx, row in calibration_df_raw.iterrows():\n",
        "    if row['potential_mean_bias'] or row['potential_std_bias']:\n",
        "        ax3.annotate(row['feature'], (row['mean_diff_raw'], row['mean_diff_scaled']), fontsize=7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/calibration_check_visualization.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/calibration_check_visualization.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Descriptive Statistics Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate descriptive statistics for both datasets\n",
        "stats_synthetic = df_synthetic.describe().T\n",
        "stats_real = df_real.describe().T\n",
        "\n",
        "# Rename columns for clarity\n",
        "stats_synthetic.columns = [f'syn_{col}' for col in stats_synthetic.columns]\n",
        "stats_real.columns = [f'real_{col}' for col in stats_real.columns]\n",
        "\n",
        "# Combine into comparison table\n",
        "stats_comparison = pd.concat([stats_synthetic, stats_real], axis=1)\n",
        "\n",
        "# Reorder columns for side-by-side comparison\n",
        "ordered_cols = []\n",
        "for stat in ['count', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']:\n",
        "    ordered_cols.extend([f'syn_{stat}', f'real_{stat}'])\n",
        "stats_comparison = stats_comparison[ordered_cols]\n",
        "stats_comparison.to_csv(f'{OUTPUT_DIR}/enhanced_v3_descriptive_stats.csv')\n",
        "\n",
        "print(\"Descriptive Statistics Comparison (Synthetic vs Real)\")\n",
        "print(\"=\" * 80)\n",
        "stats_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPROVED: Mean Difference Analysis with better edge case handling\n",
        "mean_diff = pd.DataFrame({\n",
        "    'synthetic_mean': df_synthetic.mean(),\n",
        "    'real_mean': df_real.mean()\n",
        "})\n",
        "mean_diff['abs_diff'] = abs(mean_diff['synthetic_mean'] - mean_diff['real_mean'])\n",
        "\n",
        "# Handle edge cases where real_mean is near zero\n",
        "MIN_MEANINGFUL_VALUE = 0.01\n",
        "mean_diff['pct_diff'] = np.where(\n",
        "    mean_diff['real_mean'].abs() > MIN_MEANINGFUL_VALUE,\n",
        "    (mean_diff['abs_diff'] / mean_diff['real_mean'].abs()) * 100,\n",
        "    np.where(\n",
        "        mean_diff['abs_diff'] < MIN_MEANINGFUL_VALUE,\n",
        "        0.0,  # Both near zero = no meaningful difference\n",
        "        np.nan  # Real is ~0 but synthetic is not = undefined percentage\n",
        "    )\n",
        ")\n",
        "\n",
        "mean_diff = mean_diff.sort_values('pct_diff', ascending=False)\n",
        "mean_diff.to_csv(f'{OUTPUT_DIR}/enhanced_v3_mean_diff.csv')\n",
        "\n",
        "print(\"\\nMean Difference Analysis (sorted by % difference)\")\n",
        "print(\"=\" * 60)\n",
        "print(mean_diff.round(4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Distribution Comparison (KS-Test + Wasserstein Distance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform Kolmogorov-Smirnov test with Wasserstein Distance\n",
        "ks_results = []\n",
        "\n",
        "for col in feature_cols:\n",
        "    # KS Test\n",
        "    ks_stat, ks_pvalue = stats.ks_2samp(df_synthetic[col], df_real[col])\n",
        "    \n",
        "    # Wasserstein Distance (Earth Mover's Distance)\n",
        "    syn_normalized = (df_synthetic[col] - df_synthetic[col].mean()) / (df_synthetic[col].std() + 1e-10)\n",
        "    real_normalized = (df_real[col] - df_real[col].mean()) / (df_real[col].std() + 1e-10)\n",
        "    wd = wasserstein_distance(syn_normalized, real_normalized)\n",
        "    \n",
        "    # Use KS statistic threshold for large samples\n",
        "    if ks_stat < KS_EXCELLENT_THRESHOLD:\n",
        "        similarity_level = 'Excellent'\n",
        "    elif ks_stat < KS_GOOD_THRESHOLD:\n",
        "        similarity_level = 'Good'\n",
        "    elif ks_stat < KS_MODERATE_THRESHOLD:\n",
        "        similarity_level = 'Moderate'\n",
        "    else:\n",
        "        similarity_level = 'Poor'\n",
        "    \n",
        "    # Get feature importance weight\n",
        "    weight = FEATURE_IMPORTANCE_WEIGHTS.get(col, FEATURE_IMPORTANCE_WEIGHTS['_default'])\n",
        "    \n",
        "    ks_results.append({\n",
        "        'feature': col,\n",
        "        'ks_statistic': ks_stat,\n",
        "        'ks_pvalue': ks_pvalue,\n",
        "        'wasserstein_distance': wd,\n",
        "        'similarity_level': similarity_level,\n",
        "        'similar_by_threshold': ks_stat < KS_GOOD_THRESHOLD,\n",
        "        'importance_weight': weight\n",
        "    })\n",
        "\n",
        "ks_df = pd.DataFrame(ks_results)\n",
        "\n",
        "# Apply multiple testing correction (FDR - Benjamini-Hochberg)\n",
        "_, adjusted_pvalues, _, _ = multipletests(ks_df['ks_pvalue'], method='fdr_bh')\n",
        "ks_df['adjusted_pvalue'] = adjusted_pvalues\n",
        "ks_df['similar_by_adjusted_pvalue'] = adjusted_pvalues > ALPHA\n",
        "\n",
        "ks_df = ks_df.sort_values('ks_statistic', ascending=False)\n",
        "ks_df.to_csv(f'{OUTPUT_DIR}/enhanced_v3_ks_results.csv', index=False)\n",
        "\n",
        "print(\"\\nKolmogorov-Smirnov Test Results (with Feature Importance Weights)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nBy KS Statistic Threshold (recommended for large N):\")\n",
        "print(f\"   Excellent (KS < {KS_EXCELLENT_THRESHOLD}): {(ks_df['similarity_level'] == 'Excellent').sum()}/{len(feature_cols)}\")\n",
        "print(f\"   Good (KS < {KS_GOOD_THRESHOLD}): {(ks_df['similarity_level'].isin(['Excellent', 'Good'])).sum()}/{len(feature_cols)}\")\n",
        "print(f\"   Moderate (KS < {KS_MODERATE_THRESHOLD}): {(ks_df['similarity_level'].isin(['Excellent', 'Good', 'Moderate'])).sum()}/{len(feature_cols)}\")\n",
        "print(f\"\\nBy Adjusted P-value (FDR corrected):\")\n",
        "print(f\"   Similar distributions: {ks_df['similar_by_adjusted_pvalue'].sum()}/{len(feature_cols)}\")\n",
        "print(f\"\\nWasserstein Distance (lower = more similar):\")\n",
        "print(f\"   Mean: {ks_df['wasserstein_distance'].mean():.4f}\")\n",
        "print(f\"   Median: {ks_df['wasserstein_distance'].median():.4f}\")\n",
        "print(\"\\n\")\n",
        "print(ks_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize KS statistics with improved interpretation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Left plot: KS Statistics\n",
        "ax1 = axes[0]\n",
        "colors_ks = {\n",
        "    'Excellent': 'darkgreen',\n",
        "    'Good': 'limegreen',\n",
        "    'Moderate': 'orange',\n",
        "    'Poor': 'red'\n",
        "}\n",
        "bar_colors = [colors_ks[level] for level in ks_df['similarity_level']]\n",
        "bars = ax1.barh(ks_df['feature'], ks_df['ks_statistic'], color=bar_colors, alpha=0.7)\n",
        "\n",
        "ax1.axvline(x=KS_EXCELLENT_THRESHOLD, color='darkgreen', linestyle='--', alpha=0.7, label=f'Excellent ({KS_EXCELLENT_THRESHOLD})')\n",
        "ax1.axvline(x=KS_GOOD_THRESHOLD, color='limegreen', linestyle='--', alpha=0.7, label=f'Good ({KS_GOOD_THRESHOLD})')\n",
        "ax1.axvline(x=KS_MODERATE_THRESHOLD, color='orange', linestyle='--', alpha=0.7, label=f'Moderate ({KS_MODERATE_THRESHOLD})')\n",
        "ax1.set_xlabel('KS Statistic (lower = more similar)')\n",
        "ax1.set_title('Distribution Similarity: KS-Test Results\\n(Threshold-based interpretation for large N)')\n",
        "ax1.legend(loc='lower right')\n",
        "\n",
        "# Right plot: Wasserstein Distance\n",
        "ax2 = axes[1]\n",
        "ks_df_sorted_wd = ks_df.sort_values('wasserstein_distance', ascending=False)\n",
        "colors_wd = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(ks_df_sorted_wd)))\n",
        "ax2.barh(ks_df_sorted_wd['feature'], ks_df_sorted_wd['wasserstein_distance'], color=colors_wd, alpha=0.7)\n",
        "ax2.set_xlabel('Wasserstein Distance (lower = more similar)')\n",
        "ax2.set_title(\"Distribution Similarity: Wasserstein Distance\\n(Earth Mover's Distance on normalized data)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_distribution_tests.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_distribution_tests.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions for selected features\n",
        "n_features_to_plot = 6\n",
        "\n",
        "# Most different (highest KS statistic)\n",
        "most_different = ks_df.head(n_features_to_plot)['feature'].tolist()\n",
        "# Most similar (lowest KS statistic)\n",
        "most_similar = ks_df.tail(n_features_to_plot)['feature'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(2, n_features_to_plot, figsize=(20, 8))\n",
        "\n",
        "# Plot most different\n",
        "for i, col in enumerate(most_different):\n",
        "    ax = axes[0, i]\n",
        "    ax.hist(df_synthetic[col], bins=50, alpha=0.5, label='Synthetic', density=True)\n",
        "    ax.hist(df_real[col], bins=50, alpha=0.5, label='Real', density=True)\n",
        "    ks_val = ks_df[ks_df['feature'] == col]['ks_statistic'].values[0]\n",
        "    weight = ks_df[ks_df['feature'] == col]['importance_weight'].values[0]\n",
        "    ax.set_title(f'{col}\\nKS={ks_val:.3f}, W={weight:.1f}', fontsize=9)\n",
        "    ax.legend(fontsize=7)\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Most Different', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Plot most similar\n",
        "for i, col in enumerate(most_similar):\n",
        "    ax = axes[1, i]\n",
        "    ax.hist(df_synthetic[col], bins=50, alpha=0.5, label='Synthetic', density=True)\n",
        "    ax.hist(df_real[col], bins=50, alpha=0.5, label='Real', density=True)\n",
        "    ks_val = ks_df[ks_df['feature'] == col]['ks_statistic'].values[0]\n",
        "    weight = ks_df[ks_df['feature'] == col]['importance_weight'].values[0]\n",
        "    ax.set_title(f'{col}\\nKS={ks_val:.3f}, W={weight:.1f}', fontsize=9)\n",
        "    ax.legend(fontsize=7)\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Most Similar', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Distribution Comparison: Most Different vs Most Similar Features (W=importance weight)', fontsize=12, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_distribution_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_distribution_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Correlation Structure Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate correlation matrices\n",
        "corr_synthetic = df_synthetic.corr()\n",
        "corr_real = df_real.corr()\n",
        "\n",
        "# Absolute difference in correlations\n",
        "corr_diff = abs(corr_synthetic - corr_real)\n",
        "\n",
        "print(\"Correlation Matrix Comparison\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Mean absolute correlation difference: {corr_diff.values.mean():.4f}\")\n",
        "print(f\"Max absolute correlation difference: {corr_diff.values.max():.4f}\")\n",
        "print(f\"Median absolute correlation difference: {np.median(corr_diff.values):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot correlation matrices side by side\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Synthetic correlation\n",
        "sns.heatmap(corr_synthetic, ax=axes[0], cmap='RdBu_r', center=0, \n",
        "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.5},\n",
        "            vmin=-1, vmax=1)\n",
        "axes[0].set_title('Synthetic Data Correlation', fontsize=12)\n",
        "axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=90, fontsize=7)\n",
        "axes[0].set_yticklabels(axes[0].get_yticklabels(), fontsize=7)\n",
        "\n",
        "# Real correlation\n",
        "sns.heatmap(corr_real, ax=axes[1], cmap='RdBu_r', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.5},\n",
        "            vmin=-1, vmax=1)\n",
        "axes[1].set_title('Real Data Correlation', fontsize=12)\n",
        "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=90, fontsize=7)\n",
        "axes[1].set_yticklabels(axes[1].get_yticklabels(), fontsize=7)\n",
        "\n",
        "# Difference\n",
        "sns.heatmap(corr_diff, ax=axes[2], cmap='Reds',\n",
        "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.5},\n",
        "            vmin=0, vmax=1)\n",
        "axes[2].set_title('Absolute Correlation Difference', fontsize=12)\n",
        "axes[2].set_xticklabels(axes[2].get_xticklabels(), rotation=90, fontsize=7)\n",
        "axes[2].set_yticklabels(axes[2].get_yticklabels(), fontsize=7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_correlation_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_correlation_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation of correlations (how similar are the correlation structures?)\n",
        "def get_upper_triangle(matrix):\n",
        "    return matrix.values[np.triu_indices(len(matrix), k=1)]\n",
        "\n",
        "corr_syn_flat = get_upper_triangle(corr_synthetic)\n",
        "corr_real_flat = get_upper_triangle(corr_real)\n",
        "\n",
        "# Calculate correlation between correlation structures\n",
        "structure_corr, structure_p = stats.pearsonr(corr_syn_flat, corr_real_flat)\n",
        "\n",
        "# Also calculate Spearman (rank-based) correlation\n",
        "structure_spearman, spearman_p = stats.spearmanr(corr_syn_flat, corr_real_flat)\n",
        "\n",
        "print(f\"\\nCorrelation Structure Similarity\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Pearson correlation of correlation matrices: {structure_corr:.4f}\")\n",
        "print(f\"Spearman correlation of correlation matrices: {structure_spearman:.4f}\")\n",
        "print(f\"\\nInterpretation: {'High' if structure_corr > 0.8 else 'Moderate' if structure_corr > 0.5 else 'Low'} structural similarity\")\n",
        "\n",
        "# Warn if correlation is negative (anti-aligned structures)\n",
        "if structure_corr < 0:\n",
        "    print(f\"\\n[WARNING] Negative correlation ({structure_corr:.4f}) indicates anti-aligned structures!\")\n",
        "    print(f\"   This suggests the GAN may be learning inverted relationships.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot of correlations\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "ax.scatter(corr_real_flat, corr_syn_flat, alpha=0.5, s=20)\n",
        "ax.plot([-1, 1], [-1, 1], 'r--', label='Perfect match')\n",
        "ax.set_xlabel('Real Data Correlations')\n",
        "ax.set_ylabel('Synthetic Data Correlations')\n",
        "ax.set_title(f'Correlation Structure Comparison\\nPearson r = {structure_corr:.4f}')\n",
        "ax.set_xlim(-1, 1)\n",
        "ax.set_ylim(-1, 1)\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_correlation_scatter.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_correlation_scatter.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Statistical Tests and Effect Sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mann-Whitney U test (non-parametric alternative to t-test)\n",
        "mw_results = []\n",
        "\n",
        "for col in feature_cols:\n",
        "    stat, p_value = stats.mannwhitneyu(df_synthetic[col], df_real[col], alternative='two-sided')\n",
        "    weight = FEATURE_IMPORTANCE_WEIGHTS.get(col, FEATURE_IMPORTANCE_WEIGHTS['_default'])\n",
        "    mw_results.append({\n",
        "        'feature': col,\n",
        "        'mw_statistic': stat,\n",
        "        'p_value': p_value,\n",
        "        'importance_weight': weight\n",
        "    })\n",
        "\n",
        "mw_df = pd.DataFrame(mw_results)\n",
        "\n",
        "# Apply FDR correction to Mann-Whitney results\n",
        "_, mw_adjusted_pvalues, _, _ = multipletests(mw_df['p_value'], method='fdr_bh')\n",
        "mw_df['adjusted_p_value'] = mw_adjusted_pvalues\n",
        "mw_df['similar'] = mw_adjusted_pvalues > ALPHA\n",
        "\n",
        "mw_df = mw_df.sort_values('p_value', ascending=True)\n",
        "mw_df.to_csv(f'{OUTPUT_DIR}/enhanced_v3_mw_results.csv', index=False)\n",
        "\n",
        "print(\"Mann-Whitney U Test Results (with FDR correction)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Features with similar medians (adjusted p > {ALPHA}): {mw_df['similar'].sum()}/{len(feature_cols)}\")\n",
        "print(\"\\n\")\n",
        "print(mw_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cohen's d effect size with capping for numeric stability\n",
        "def cohens_d(group1, group2, cap=COHENS_D_CAP):\n",
        "    \"\"\"\n",
        "    Calculate Cohen's d effect size with proper handling of edge cases.\n",
        "    \"\"\"\n",
        "    n1, n2 = len(group1), len(group2)\n",
        "    var1, var2 = group1.var(), group2.var()\n",
        "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
        "    \n",
        "    if pooled_std == 0:\n",
        "        if group1.mean() == group2.mean():\n",
        "            return 0.0\n",
        "        else:\n",
        "            return cap if group1.mean() > group2.mean() else -cap\n",
        "    \n",
        "    d = (group1.mean() - group2.mean()) / pooled_std\n",
        "    return np.clip(d, -cap, cap)\n",
        "\n",
        "effect_sizes = []\n",
        "for col in feature_cols:\n",
        "    d = cohens_d(df_synthetic[col], df_real[col])\n",
        "    weight = FEATURE_IMPORTANCE_WEIGHTS.get(col, FEATURE_IMPORTANCE_WEIGHTS['_default'])\n",
        "    \n",
        "    abs_d = abs(d)\n",
        "    if abs_d < 0.2:\n",
        "        interpretation = 'Negligible'\n",
        "    elif abs_d < 0.5:\n",
        "        interpretation = 'Small'\n",
        "    elif abs_d < 0.8:\n",
        "        interpretation = 'Medium'\n",
        "    else:\n",
        "        interpretation = 'Large'\n",
        "    \n",
        "    effect_sizes.append({\n",
        "        'feature': col,\n",
        "        'cohens_d': d,\n",
        "        'abs_d': abs_d,\n",
        "        'interpretation': interpretation,\n",
        "        'capped': abs_d >= COHENS_D_CAP,\n",
        "        'importance_weight': weight\n",
        "    })\n",
        "\n",
        "effect_df = pd.DataFrame(effect_sizes).sort_values('abs_d', ascending=False)\n",
        "effect_df.to_csv(f'{OUTPUT_DIR}/enhanced_v3_effect_sizes.csv', index=False)\n",
        "\n",
        "print(\"Cohen's d Effect Size Analysis (with Feature Importance Weights)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nEffect Size Distribution:\")\n",
        "print(effect_df['interpretation'].value_counts())\n",
        "\n",
        "# Warn about capped values\n",
        "n_capped = effect_df['capped'].sum()\n",
        "if n_capped > 0:\n",
        "    print(f\"\\n[WARNING] {n_capped} feature(s) had Cohen's d capped at +/-{COHENS_D_CAP}\")\n",
        "    print(f\"   Features: {effect_df[effect_df['capped']]['feature'].tolist()}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(effect_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize effect sizes with importance weights\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Left plot: Effect sizes\n",
        "ax1 = axes[0]\n",
        "colors_effect = {\n",
        "    'Negligible': 'green',\n",
        "    'Small': 'yellowgreen', \n",
        "    'Medium': 'orange',\n",
        "    'Large': 'red'\n",
        "}\n",
        "bar_colors = [colors_effect[interp] for interp in effect_df['interpretation']]\n",
        "\n",
        "ax1.barh(effect_df['feature'], effect_df['abs_d'], color=bar_colors, alpha=0.7)\n",
        "ax1.axvline(x=0.2, color='yellowgreen', linestyle='--', alpha=0.7, label='Small (0.2)')\n",
        "ax1.axvline(x=0.5, color='orange', linestyle='--', alpha=0.7, label='Medium (0.5)')\n",
        "ax1.axvline(x=0.8, color='red', linestyle='--', alpha=0.7, label='Large (0.8)')\n",
        "ax1.set_xlabel(\"Cohen's d (absolute)\")\n",
        "ax1.set_title(\"Effect Size: Difference Between Synthetic and Real Data\")\n",
        "ax1.legend(loc='lower right')\n",
        "\n",
        "# Right plot: Effect sizes weighted by importance\n",
        "ax2 = axes[1]\n",
        "weighted_effects = effect_df['abs_d'] * effect_df['importance_weight']\n",
        "ax2.barh(effect_df['feature'], weighted_effects, color=bar_colors, alpha=0.7)\n",
        "ax2.set_xlabel(\"Weighted Cohen's d (effect * importance)\")\n",
        "ax2.set_title(\"Importance-Weighted Effect Size\")\n",
        "\n",
        "# Add importance weight annotations\n",
        "for i, (idx, row) in enumerate(effect_df.iterrows()):\n",
        "    ax2.text(weighted_effects.iloc[i] + 0.02, i, f'W={row[\"importance_weight\"]:.1f}', \n",
        "             va='center', fontsize=7, alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_effect_sizes.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_effect_sizes.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Top-K Worst Features Analysis (BGP Semantics Mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENHANCED: Top-K Worst Features Table\n",
        "# =============================================================================\n",
        "# This section identifies the features with the largest discrepancies between\n",
        "# synthetic and real data, ranked by both KS statistic and Cohen's d.\n",
        "# This helps link model weaknesses to BGP semantics for paper discussion.\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"TOP-{TOP_K_WORST_FEATURES} WORST FEATURES ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Combine metrics for comprehensive ranking\n",
        "worst_features_df = ks_df[['feature', 'ks_statistic', 'wasserstein_distance', 'similarity_level', 'importance_weight']].merge(\n",
        "    effect_df[['feature', 'cohens_d', 'abs_d', 'interpretation']], \n",
        "    on='feature'\n",
        ")\n",
        "\n",
        "# Add a composite badness score (weighted by importance)\n",
        "# Higher is worse\n",
        "worst_features_df['ks_weighted'] = worst_features_df['ks_statistic'] * worst_features_df['importance_weight']\n",
        "worst_features_df['effect_weighted'] = worst_features_df['abs_d'] * worst_features_df['importance_weight']\n",
        "worst_features_df['composite_score'] = (\n",
        "    worst_features_df['ks_weighted'] * 0.5 + \n",
        "    worst_features_df['effect_weighted'] * 0.5\n",
        ")\n",
        "\n",
        "# Sort by composite score (worst first)\n",
        "worst_features_ranked = worst_features_df.sort_values('composite_score', ascending=False)\n",
        "\n",
        "print(f\"\\n--- Top {TOP_K_WORST_FEATURES} Worst Features by Composite Score ---\")\n",
        "print(\"(Composite = 0.5*KS_weighted + 0.5*Effect_weighted)\")\n",
        "print()\n",
        "\n",
        "top_k_worst = worst_features_ranked.head(TOP_K_WORST_FEATURES)\n",
        "print(top_k_worst[['feature', 'ks_statistic', 'abs_d', 'importance_weight', \n",
        "                   'composite_score', 'similarity_level', 'interpretation']].to_string(index=False))\n",
        "\n",
        "# Save top-K worst features\n",
        "top_k_worst.to_csv(f'{OUTPUT_DIR}/enhanced_v3_top_k_worst_features.csv', index=False)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR}/enhanced_v3_top_k_worst_features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed breakdown for paper discussion\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DETAILED WORST FEATURES BREAKDOWN (for paper discussion)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for i, (idx, row) in enumerate(top_k_worst.iterrows()):\n",
        "    print(f\"\\n{i+1}. {row['feature']}\")\n",
        "    print(f\"   - KS Statistic: {row['ks_statistic']:.4f} ({row['similarity_level']})\")\n",
        "    print(f\"   - Cohen's d: {row['cohens_d']:.4f} ({row['interpretation']})\")\n",
        "    print(f\"   - Wasserstein Distance: {row['wasserstein_distance']:.4f}\")\n",
        "    print(f\"   - Importance Weight: {row['importance_weight']:.1f}\")\n",
        "    print(f\"   - Composite Score: {row['composite_score']:.4f}\")\n",
        "    \n",
        "    # BGP semantic interpretation\n",
        "    feature_name = row['feature'].lower()\n",
        "    if 'announcement' in feature_name:\n",
        "        print(\"   - BGP Semantic: Core routing update activity - critical for traffic realism\")\n",
        "    elif 'withdrawal' in feature_name:\n",
        "        print(\"   - BGP Semantic: Route withdrawal patterns - important for instability detection\")\n",
        "    elif 'flap' in feature_name:\n",
        "        print(\"   - BGP Semantic: Network instability indicator - key anomaly signature\")\n",
        "    elif 'nada' in feature_name:\n",
        "        print(\"   - BGP Semantic: No-Advertise-to-Any policy - unusual routing behavior\")\n",
        "    elif 'rare_as' in feature_name:\n",
        "        print(\"   - BGP Semantic: Unusual AS appearances - potential anomaly indicator\")\n",
        "    elif 'edit_dist' in feature_name:\n",
        "        print(\"   - BGP Semantic: AS-path change magnitude - routing dynamics measure\")\n",
        "    elif 'prefix' in feature_name:\n",
        "        print(\"   - BGP Semantic: Prefix diversity/activity - address space coverage\")\n",
        "    elif 'path' in feature_name:\n",
        "        print(\"   - BGP Semantic: AS-path characteristics - routing topology indicator\")\n",
        "    else:\n",
        "        print(\"   - BGP Semantic: General traffic characteristic\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize top-K worst features\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "# Left: Bar chart of composite scores\n",
        "ax1 = axes[0]\n",
        "colors = ['red' if row['interpretation'] == 'Large' else \n",
        "          'orange' if row['interpretation'] == 'Medium' else \n",
        "          'yellowgreen' if row['interpretation'] == 'Small' else 'green'\n",
        "          for _, row in top_k_worst.iterrows()]\n",
        "bars = ax1.barh(top_k_worst['feature'], top_k_worst['composite_score'], color=colors, alpha=0.8)\n",
        "ax1.set_xlabel('Composite Score (higher = worse)')\n",
        "ax1.set_title(f'Top-{TOP_K_WORST_FEATURES} Worst Features by Composite Score')\n",
        "ax1.invert_yaxis()\n",
        "\n",
        "# Add weight annotations\n",
        "for i, (idx, row) in enumerate(top_k_worst.iterrows()):\n",
        "    ax1.text(row['composite_score'] + 0.01, i, f'W={row[\"importance_weight\"]:.1f}', \n",
        "             va='center', fontsize=8)\n",
        "\n",
        "# Right: Radar/spider chart for top 5\n",
        "ax2 = axes[1]\n",
        "top_5 = top_k_worst.head(5)\n",
        "\n",
        "# Normalize metrics for radar chart\n",
        "metrics = ['ks_statistic', 'abs_d', 'wasserstein_distance']\n",
        "max_vals = {m: worst_features_df[m].max() for m in metrics}\n",
        "\n",
        "angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()\n",
        "angles += angles[:1]  # Complete the loop\n",
        "\n",
        "ax2 = plt.subplot(122, projection='polar')\n",
        "for idx, row in top_5.iterrows():\n",
        "    values = [row[m] / max_vals[m] for m in metrics]\n",
        "    values += values[:1]\n",
        "    ax2.plot(angles, values, 'o-', linewidth=2, label=row['feature'])\n",
        "    ax2.fill(angles, values, alpha=0.1)\n",
        "\n",
        "ax2.set_xticks(angles[:-1])\n",
        "ax2.set_xticklabels(['KS Stat', 'Cohen\\'s d', 'Wasserstein'])\n",
        "ax2.set_title('Top-5 Worst Features: Normalized Metrics')\n",
        "ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_top_k_worst_features.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_top_k_worst_features.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Multivariate Analysis (PCA Centroid Distance + t-SNE Overlay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine and scale data for dimensionality reduction\n",
        "X_combined = np.vstack([X_synthetic, X_real])\n",
        "labels = ['Synthetic'] * len(X_synthetic) + ['Real'] * len(X_real)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_combined)\n",
        "\n",
        "# Split back for centroid calculation\n",
        "X_syn_scaled = X_scaled[:len(X_synthetic)]\n",
        "X_real_scaled = X_scaled[len(X_synthetic):]\n",
        "\n",
        "print(f\"Combined data shape: {X_combined.shape}\")\n",
        "print(f\"Synthetic scaled shape: {X_syn_scaled.shape}\")\n",
        "print(f\"Real scaled shape: {X_real_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENHANCED: PCA Centroid Distance Analysis\n",
        "# =============================================================================\n",
        "# Complement 1D tests with multivariate check: distance between centroids in PCA space\n",
        "\n",
        "# Perform PCA\n",
        "pca = PCA(n_components=min(10, len(feature_cols)))\n",
        "X_pca_full = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Split back\n",
        "X_pca_syn = X_pca_full[:len(X_synthetic)]\n",
        "X_pca_real = X_pca_full[len(X_synthetic):]\n",
        "\n",
        "# Calculate centroids in PCA space\n",
        "centroid_syn = X_pca_syn.mean(axis=0)\n",
        "centroid_real = X_pca_real.mean(axis=0)\n",
        "\n",
        "# Euclidean distance between centroids\n",
        "centroid_distance = np.linalg.norm(centroid_syn - centroid_real)\n",
        "\n",
        "# Also calculate distance using only top-2 PCs (for visualization)\n",
        "centroid_distance_2d = np.linalg.norm(centroid_syn[:2] - centroid_real[:2])\n",
        "\n",
        "# Calculate Mahalanobis distance for more robust measure\n",
        "# Use pooled covariance\n",
        "cov_pooled = (np.cov(X_pca_syn.T) + np.cov(X_pca_real.T)) / 2\n",
        "try:\n",
        "    cov_inv = np.linalg.inv(cov_pooled)\n",
        "    mahal_distance = mahalanobis(centroid_syn, centroid_real, cov_inv)\n",
        "except np.linalg.LinAlgError:\n",
        "    # Fallback if covariance is singular\n",
        "    mahal_distance = np.nan\n",
        "    print(\"[WARNING] Covariance matrix is singular, Mahalanobis distance unavailable\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"MULTIVARIATE ANALYSIS: PCA Centroid Distance\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nPCA explained variance (cumulative):\")\n",
        "cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
        "for i, cv in enumerate(cumvar[:5]):\n",
        "    print(f\"   PC1-PC{i+1}: {cv*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nCentroid Distance Metrics:\")\n",
        "print(f\"   Euclidean distance (all PCs): {centroid_distance:.4f}\")\n",
        "print(f\"   Euclidean distance (PC1-PC2): {centroid_distance_2d:.4f}\")\n",
        "if not np.isnan(mahal_distance):\n",
        "    print(f\"   Mahalanobis distance: {mahal_distance:.4f}\")\n",
        "\n",
        "# Interpretation guidelines\n",
        "print(f\"\\nInterpretation:\")\n",
        "if centroid_distance < 0.5:\n",
        "    print(\"   [EXCELLENT] Very close centroids - synthetic data matches real multivariate structure\")\n",
        "elif centroid_distance < 1.0:\n",
        "    print(\"   [GOOD] Reasonably close centroids - acceptable multivariate similarity\")\n",
        "elif centroid_distance < 2.0:\n",
        "    print(\"   [MODERATE] Some separation between centroids - room for improvement\")\n",
        "else:\n",
        "    print(\"   [POOR] Large centroid separation - significant multivariate differences\")\n",
        "\n",
        "# Save PCA results\n",
        "pca_results = {\n",
        "    'centroid_distance_euclidean': centroid_distance,\n",
        "    'centroid_distance_2d': centroid_distance_2d,\n",
        "    'mahalanobis_distance': mahal_distance if not np.isnan(mahal_distance) else None,\n",
        "    'explained_variance_ratio': pca.explained_variance_ratio_.tolist(),\n",
        "    'cumulative_variance': cumvar.tolist(),\n",
        "    'centroid_synthetic': centroid_syn.tolist(),\n",
        "    'centroid_real': centroid_real.tolist()\n",
        "}\n",
        "with open(f'{OUTPUT_DIR}/enhanced_v3_pca_centroid_analysis.json', 'w') as f:\n",
        "    json.dump(pca_results, f, indent=2)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR}/enhanced_v3_pca_centroid_analysis.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize PCA with centroids marked\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Left: PCA scatter with centroids\n",
        "ax1 = axes[0]\n",
        "for label, color, marker in [('Synthetic', 'blue', 'o'), ('Real', 'red', 'o')]:\n",
        "    mask = np.array(labels) == label\n",
        "    ax1.scatter(X_pca_full[mask, 0], X_pca_full[mask, 1], c=color, alpha=0.2, s=5, label=label, marker=marker)\n",
        "\n",
        "# Plot centroids\n",
        "ax1.scatter(centroid_syn[0], centroid_syn[1], c='blue', s=200, marker='*', \n",
        "            edgecolors='black', linewidths=2, label='Synthetic Centroid', zorder=5)\n",
        "ax1.scatter(centroid_real[0], centroid_real[1], c='red', s=200, marker='*', \n",
        "            edgecolors='black', linewidths=2, label='Real Centroid', zorder=5)\n",
        "\n",
        "# Draw line between centroids\n",
        "ax1.plot([centroid_syn[0], centroid_real[0]], [centroid_syn[1], centroid_real[1]], \n",
        "         'k--', linewidth=2, alpha=0.7)\n",
        "ax1.annotate(f'd={centroid_distance_2d:.2f}', \n",
        "             xy=((centroid_syn[0]+centroid_real[0])/2, (centroid_syn[1]+centroid_real[1])/2),\n",
        "             fontsize=10, fontweight='bold', \n",
        "             bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "ax1.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "ax1.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "ax1.set_title('PCA: Synthetic vs Real with Centroids\\n(Stars mark centroids, dashed line shows distance)')\n",
        "ax1.legend()\n",
        "\n",
        "# Right: Explained variance\n",
        "ax2 = axes[1]\n",
        "components = range(1, len(pca.explained_variance_ratio_) + 1)\n",
        "ax2.bar(components, pca.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
        "ax2.plot(components, cumvar, 'ro-', label='Cumulative')\n",
        "ax2.axhline(y=0.9, color='g', linestyle='--', alpha=0.7, label='90% threshold')\n",
        "ax2.set_xlabel('Principal Component')\n",
        "ax2.set_ylabel('Explained Variance Ratio')\n",
        "ax2.set_title('PCA Explained Variance')\n",
        "ax2.legend()\n",
        "ax2.set_xticks(components)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_pca_centroid_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_pca_centroid_analysis.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENHANCED: t-SNE Overlay with Density Comparison\n",
        "# =============================================================================\n",
        "# Stratified t-SNE with visual overlay to assess distribution matching\n",
        "\n",
        "n_tsne_samples = min(5000, len(X_scaled))\n",
        "n_each = n_tsne_samples // 2\n",
        "\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Stratified sampling: equal from synthetic and real\n",
        "syn_indices = np.random.choice(len(X_synthetic), n_each, replace=False)\n",
        "real_indices = np.random.choice(len(X_real), n_each, replace=False) + len(X_synthetic)\n",
        "tsne_idx = np.concatenate([syn_indices, real_indices])\n",
        "np.random.shuffle(tsne_idx)\n",
        "\n",
        "# Store indices for reproducibility\n",
        "tsne_indices_info = {\n",
        "    'synthetic_indices': syn_indices.tolist(),\n",
        "    'real_indices': (real_indices - len(X_synthetic)).tolist(),\n",
        "    'combined_indices': tsne_idx.tolist(),\n",
        "    'random_seed': RANDOM_SEED,\n",
        "    'n_synthetic': n_each,\n",
        "    'n_real': n_each\n",
        "}\n",
        "with open(f'{OUTPUT_DIR}/enhanced_v3_tsne_indices.json', 'w') as f:\n",
        "    json.dump(tsne_indices_info, f, indent=2)\n",
        "\n",
        "# Perform t-SNE\n",
        "print(\"Running t-SNE (this may take a moment)...\")\n",
        "tsne = TSNE(n_components=2, random_state=RANDOM_SEED, perplexity=30, n_iter=1000)\n",
        "X_tsne = tsne.fit_transform(X_scaled[tsne_idx])\n",
        "labels_tsne = [labels[i] for i in tsne_idx]\n",
        "\n",
        "print(f\"t-SNE completed on {n_tsne_samples} samples\")\n",
        "print(f\"  - Synthetic samples: {n_each}\")\n",
        "print(f\"  - Real samples: {n_each}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize t-SNE with density overlay\n",
        "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "# Left: Standard t-SNE scatter\n",
        "ax1 = axes[0]\n",
        "for label, color in [('Synthetic', 'blue'), ('Real', 'red')]:\n",
        "    mask = np.array(labels_tsne) == label\n",
        "    ax1.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=color, alpha=0.3, s=10, label=label)\n",
        "ax1.set_xlabel('t-SNE 1')\n",
        "ax1.set_ylabel('t-SNE 2')\n",
        "ax1.set_title(f't-SNE: Synthetic vs Real Overlay\\n(n={n_each} each)')\n",
        "ax1.legend()\n",
        "\n",
        "# Middle: 2D KDE density plot for synthetic\n",
        "ax2 = axes[1]\n",
        "mask_syn = np.array(labels_tsne) == 'Synthetic'\n",
        "mask_real = np.array(labels_tsne) == 'Real'\n",
        "\n",
        "try:\n",
        "    # KDE for synthetic (blue contours)\n",
        "    from scipy.stats import gaussian_kde\n",
        "    xy_syn = np.vstack([X_tsne[mask_syn, 0], X_tsne[mask_syn, 1]])\n",
        "    kde_syn = gaussian_kde(xy_syn)\n",
        "    \n",
        "    xy_real = np.vstack([X_tsne[mask_real, 0], X_tsne[mask_real, 1]])\n",
        "    kde_real = gaussian_kde(xy_real)\n",
        "    \n",
        "    # Create grid\n",
        "    x_min, x_max = X_tsne[:, 0].min() - 1, X_tsne[:, 0].max() + 1\n",
        "    y_min, y_max = X_tsne[:, 1].min() - 1, X_tsne[:, 1].max() + 1\n",
        "    xx, yy = np.mgrid[x_min:x_max:100j, y_min:y_max:100j]\n",
        "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
        "    \n",
        "    # Evaluate KDEs\n",
        "    z_syn = np.reshape(kde_syn(positions).T, xx.shape)\n",
        "    z_real = np.reshape(kde_real(positions).T, xx.shape)\n",
        "    \n",
        "    # Plot contours\n",
        "    ax2.contour(xx, yy, z_syn, levels=5, colors='blue', alpha=0.6, linestyles='-')\n",
        "    ax2.contour(xx, yy, z_real, levels=5, colors='red', alpha=0.6, linestyles='--')\n",
        "    ax2.set_xlabel('t-SNE 1')\n",
        "    ax2.set_ylabel('t-SNE 2')\n",
        "    ax2.set_title('t-SNE Density Contours\\n(Solid=Synthetic, Dashed=Real)')\n",
        "    \n",
        "except Exception as e:\n",
        "    ax2.text(0.5, 0.5, f'KDE failed: {str(e)}', transform=ax2.transAxes, ha='center')\n",
        "    ax2.set_title('t-SNE Density Contours (Failed)')\n",
        "\n",
        "# Right: Density difference heatmap\n",
        "ax3 = axes[2]\n",
        "try:\n",
        "    # Calculate density difference\n",
        "    z_diff = z_syn - z_real\n",
        "    \n",
        "    im = ax3.imshow(z_diff.T, extent=[x_min, x_max, y_min, y_max], \n",
        "                    origin='lower', cmap='RdBu_r', aspect='auto')\n",
        "    plt.colorbar(im, ax=ax3, label='Density Difference (Syn - Real)')\n",
        "    ax3.set_xlabel('t-SNE 1')\n",
        "    ax3.set_ylabel('t-SNE 2')\n",
        "    ax3.set_title('t-SNE Density Difference\\n(Blue=More Synthetic, Red=More Real)')\n",
        "except Exception as e:\n",
        "    ax3.text(0.5, 0.5, f'Density diff failed: {str(e)}', transform=ax3.transAxes, ha='center')\n",
        "    ax3.set_title('t-SNE Density Difference (Failed)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_tsne_overlay.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_tsne_overlay.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate t-SNE centroids and distance\n",
        "tsne_centroid_syn = X_tsne[mask_syn].mean(axis=0)\n",
        "tsne_centroid_real = X_tsne[mask_real].mean(axis=0)\n",
        "tsne_centroid_distance = np.linalg.norm(tsne_centroid_syn - tsne_centroid_real)\n",
        "\n",
        "print(\"\\nt-SNE Centroid Analysis:\")\n",
        "print(f\"   Synthetic centroid: ({tsne_centroid_syn[0]:.2f}, {tsne_centroid_syn[1]:.2f})\")\n",
        "print(f\"   Real centroid: ({tsne_centroid_real[0]:.2f}, {tsne_centroid_real[1]:.2f})\")\n",
        "print(f\"   Distance: {tsne_centroid_distance:.4f}\")\n",
        "\n",
        "# Note: t-SNE distances are not directly interpretable, but relative comparison is useful\n",
        "print(\"\\n   Note: t-SNE distances are not directly interpretable as absolute measures,\")\n",
        "print(\"   but provide relative comparison of cluster separation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Enhanced Summary Report with Feature-Weighted Scoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# ENHANCED: Calculate Overall Similarity Score with Feature Importance Weights\n",
        "# =============================================================================\n",
        "# This version weights operationally critical BGP features more heavily\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PHASE 1: NORMAL TRAFFIC VALIDATION - ENHANCED SUMMARY REPORT v3\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get weights for each feature\n",
        "feature_weights = np.array([\n",
        "    FEATURE_IMPORTANCE_WEIGHTS.get(col, FEATURE_IMPORTANCE_WEIGHTS['_default']) \n",
        "    for col in feature_cols\n",
        "])\n",
        "total_weight = feature_weights.sum()\n",
        "\n",
        "# Component 1: WEIGHTED Distribution Similarity\n",
        "# Weight the KS threshold pass/fail by feature importance\n",
        "ks_pass_weighted = sum(\n",
        "    FEATURE_IMPORTANCE_WEIGHTS.get(row['feature'], 1.0) \n",
        "    for _, row in ks_df.iterrows() \n",
        "    if row['similarity_level'] in ['Excellent', 'Good']\n",
        ")\n",
        "distribution_score_weighted = (ks_pass_weighted / total_weight) * 100\n",
        "\n",
        "# Also calculate unweighted for comparison\n",
        "good_or_better = (ks_df['similarity_level'].isin(['Excellent', 'Good'])).sum()\n",
        "distribution_score_unweighted = (good_or_better / len(feature_cols)) * 100\n",
        "\n",
        "# Component 2: Correlation Structure Similarity (unchanged - global metric)\n",
        "correlation_score = ((structure_corr + 1) / 2) * 100\n",
        "\n",
        "# Component 3: WEIGHTED Effect Size Score\n",
        "effect_counts = effect_df['interpretation'].value_counts()\n",
        "negligible_count = effect_counts.get('Negligible', 0)\n",
        "small_count = effect_counts.get('Small', 0)\n",
        "medium_count = effect_counts.get('Medium', 0)\n",
        "large_count = effect_counts.get('Large', 0)\n",
        "\n",
        "# Calculate weighted effect score\n",
        "effect_score_weighted = 0\n",
        "for _, row in effect_df.iterrows():\n",
        "    weight = row['importance_weight']\n",
        "    if row['interpretation'] == 'Negligible':\n",
        "        effect_score_weighted += weight * 1.0\n",
        "    elif row['interpretation'] == 'Small':\n",
        "        effect_score_weighted += weight * 0.75\n",
        "    elif row['interpretation'] == 'Medium':\n",
        "        effect_score_weighted += weight * 0.25\n",
        "    else:  # Large\n",
        "        effect_score_weighted += weight * 0.0\n",
        "effect_score_weighted = (effect_score_weighted / total_weight) * 100\n",
        "\n",
        "# Unweighted for comparison\n",
        "effect_score_unweighted = ((negligible_count * 1.0 + small_count * 0.75 + \n",
        "                            medium_count * 0.25 + large_count * 0.0) / len(feature_cols)) * 100\n",
        "\n",
        "# Component 4: WEIGHTED Wasserstein Distance Score\n",
        "wd_weighted = sum(\n",
        "    row['wasserstein_distance'] * FEATURE_IMPORTANCE_WEIGHTS.get(row['feature'], 1.0)\n",
        "    for _, row in ks_df.iterrows()\n",
        ") / total_weight\n",
        "wasserstein_score_weighted = max(0, (1 - wd_weighted * 2)) * 100\n",
        "\n",
        "# Unweighted for comparison\n",
        "mean_wd = ks_df['wasserstein_distance'].mean()\n",
        "wasserstein_score_unweighted = max(0, (1 - mean_wd * 2)) * 100\n",
        "\n",
        "# Component 5: NEW - Multivariate (PCA Centroid) Score\n",
        "# Scale centroid distance to a 0-100 score\n",
        "pca_score = max(0, (1 - centroid_distance / 3)) * 100  # distance of 3 = 0%\n",
        "\n",
        "# ENHANCED Overall Score with feature weights\n",
        "weights_components = {\n",
        "    'distribution': 0.20,\n",
        "    'correlation': 0.20,\n",
        "    'effect_size': 0.25,\n",
        "    'wasserstein': 0.15,\n",
        "    'multivariate': 0.20\n",
        "}\n",
        "\n",
        "overall_score_weighted = (\n",
        "    distribution_score_weighted * weights_components['distribution'] +\n",
        "    correlation_score * weights_components['correlation'] +\n",
        "    effect_score_weighted * weights_components['effect_size'] +\n",
        "    wasserstein_score_weighted * weights_components['wasserstein'] +\n",
        "    pca_score * weights_components['multivariate']\n",
        ")\n",
        "\n",
        "# Also calculate unweighted for comparison\n",
        "overall_score_unweighted = (\n",
        "    distribution_score_unweighted * weights_components['distribution'] +\n",
        "    correlation_score * weights_components['correlation'] +\n",
        "    effect_score_unweighted * weights_components['effect_size'] +\n",
        "    wasserstein_score_unweighted * weights_components['wasserstein'] +\n",
        "    pca_score * weights_components['multivariate']\n",
        ")\n",
        "\n",
        "# Count problematic features\n",
        "n_constant_syn_only = len(problematic_features['constant_synthetic_only'])\n",
        "n_constant_real_only = len(problematic_features['constant_real_only'])\n",
        "n_constant_both = len(problematic_features['constant_both'])\n",
        "mean_ks = ks_df['ks_statistic'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print detailed summary report\n",
        "print(f\"\\nDATA OVERVIEW\")\n",
        "print(f\"   Synthetic normal samples: {len(synthetic_sampled):,}\")\n",
        "print(f\"   Real normal samples: {len(real_sampled):,}\")\n",
        "print(f\"   Total features: {len(feature_cols) + n_constant_both + n_constant_syn_only + n_constant_real_only}\")\n",
        "print(f\"   Valid features for comparison: {len(feature_cols)}\")\n",
        "\n",
        "print(f\"\\nFEATURE QUALITY ISSUES\")\n",
        "print(f\"   Constant in both (excluded): {n_constant_both}\")\n",
        "print(f\"   [WARNING] Constant in synthetic only (GAN issue): {n_constant_syn_only}\")\n",
        "print(f\"   [WARNING] Constant in real only (data issue): {n_constant_real_only}\")\n",
        "\n",
        "print(f\"\\nDISTRIBUTION SIMILARITY (Feature-Weighted)\")\n",
        "print(f\"   KS-Test (threshold-based):\")\n",
        "print(f\"     - Excellent (KS < {KS_EXCELLENT_THRESHOLD}): {(ks_df['similarity_level'] == 'Excellent').sum()}/{len(feature_cols)}\")\n",
        "print(f\"     - Good or Better: {good_or_better}/{len(feature_cols)}\")\n",
        "print(f\"     - Mean KS statistic: {mean_ks:.4f}\")\n",
        "print(f\"   Wasserstein Distance:\")\n",
        "print(f\"     - Mean: {mean_wd:.4f}\")\n",
        "print(f\"     - Weighted Mean: {wd_weighted:.4f}\")\n",
        "\n",
        "print(f\"\\nCORRELATION STRUCTURE\")\n",
        "print(f\"   Pearson correlation of matrices: {structure_corr:.4f}\")\n",
        "print(f\"   Spearman correlation of matrices: {structure_spearman:.4f}\")\n",
        "print(f\"   Mean absolute correlation difference: {corr_diff.values.mean():.4f}\")\n",
        "\n",
        "print(f\"\\nEFFECT SIZES (Cohen's d)\")\n",
        "for interp in ['Negligible', 'Small', 'Medium', 'Large']:\n",
        "    count = effect_counts.get(interp, 0)\n",
        "    print(f\"   {interp}: {count} features\")\n",
        "\n",
        "print(f\"\\nMULTIVARIATE ANALYSIS\")\n",
        "print(f\"   PCA Centroid Distance: {centroid_distance:.4f}\")\n",
        "if not np.isnan(mahal_distance):\n",
        "    print(f\"   Mahalanobis Distance: {mahal_distance:.4f}\")\n",
        "print(f\"   t-SNE Centroid Distance: {tsne_centroid_distance:.4f}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"OVERALL ASSESSMENT (with Feature Importance Weighting)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nComponent Scores (weighted / unweighted):\")\n",
        "print(f\"  - Distribution (KS): {distribution_score_weighted:.1f} / {distribution_score_unweighted:.1f} (weight: {weights_components['distribution']*100:.0f}%)\")\n",
        "print(f\"  - Correlation Structure: {correlation_score:.1f} / {correlation_score:.1f} (weight: {weights_components['correlation']*100:.0f}%)\")\n",
        "print(f\"  - Effect Size: {effect_score_weighted:.1f} / {effect_score_unweighted:.1f} (weight: {weights_components['effect_size']*100:.0f}%)\")\n",
        "print(f\"  - Wasserstein: {wasserstein_score_weighted:.1f} / {wasserstein_score_unweighted:.1f} (weight: {weights_components['wasserstein']*100:.0f}%)\")\n",
        "print(f\"  - Multivariate (PCA): {pca_score:.1f} / {pca_score:.1f} (weight: {weights_components['multivariate']*100:.0f}%)\")\n",
        "print(f\"  -----------------------------------------------\")\n",
        "print(f\"  OVERALL (WEIGHTED): {overall_score_weighted:.1f}/100\")\n",
        "print(f\"  OVERALL (UNWEIGHTED): {overall_score_unweighted:.1f}/100\")\n",
        "print(f\"  Impact of Feature Weighting: {overall_score_weighted - overall_score_unweighted:+.1f} points\")\n",
        "\n",
        "if overall_score_weighted >= 80:\n",
        "    verdict = \"[EXCELLENT] Synthetic data closely matches real traffic\"\n",
        "elif overall_score_weighted >= 70:\n",
        "    verdict = \"[GOOD] Synthetic data reasonably represents real traffic\"\n",
        "elif overall_score_weighted >= 50:\n",
        "    verdict = \"[MODERATE] Some features need improvement\"\n",
        "else:\n",
        "    verdict = \"[POOR] Significant differences detected\"\n",
        "print(f\"\\n   Verdict: {verdict}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print features needing attention\n",
        "print(f\"\\nFEATURES NEEDING ATTENTION (Ranked by Weighted Impact):\")\n",
        "\n",
        "# Features with large effect size\n",
        "large_effect = effect_df[effect_df['interpretation'] == 'Large']['feature'].tolist()\n",
        "# Features with poor KS similarity  \n",
        "poor_ks = ks_df[ks_df['similarity_level'] == 'Poor']['feature'].tolist()\n",
        "# Features constant in synthetic only (GAN not learning variance)\n",
        "const_syn_features = [item['feature'] for item in problematic_features['constant_synthetic_only']]\n",
        "\n",
        "# Combine and rank by importance weight\n",
        "attention_features = list(set(large_effect + poor_ks + const_syn_features))\n",
        "if attention_features:\n",
        "    attention_with_weights = []\n",
        "    for f in attention_features:\n",
        "        weight = FEATURE_IMPORTANCE_WEIGHTS.get(f, FEATURE_IMPORTANCE_WEIGHTS['_default'])\n",
        "        reasons = []\n",
        "        if f in large_effect:\n",
        "            d_val = effect_df[effect_df['feature'] == f]['cohens_d'].values[0]\n",
        "            reasons.append(f\"Large effect (d={d_val:.3f})\")\n",
        "        if f in poor_ks:\n",
        "            ks_val = ks_df[ks_df['feature'] == f]['ks_statistic'].values[0]\n",
        "            reasons.append(f\"Poor KS ({ks_val:.3f})\")\n",
        "        if f in const_syn_features:\n",
        "            reasons.append(\"Constant in synthetic (GAN issue)\")\n",
        "        attention_with_weights.append((f, weight, reasons))\n",
        "    \n",
        "    # Sort by importance weight (highest first)\n",
        "    attention_with_weights.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    for f, weight, reasons in attention_with_weights:\n",
        "        print(f\"   - {f} (W={weight:.1f}): {', '.join(reasons)}\")\n",
        "else:\n",
        "    print(\"   None - all features have acceptable similarity\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results to CSV\n",
        "results_df = ks_df[['feature', 'ks_statistic', 'wasserstein_distance', 'similarity_level', 'importance_weight']].merge(\n",
        "    mw_df[['feature', 'mw_statistic', 'adjusted_p_value']], on='feature'\n",
        ")\n",
        "results_df = results_df.merge(\n",
        "    effect_df[['feature', 'cohens_d', 'interpretation', 'capped']], on='feature'\n",
        ")\n",
        "results_df = results_df.merge(\n",
        "    mean_diff.reset_index().rename(columns={'index': 'feature'})[['feature', 'synthetic_mean', 'real_mean', 'abs_diff', 'pct_diff']], \n",
        "    on='feature'\n",
        ")\n",
        "\n",
        "results_df.to_csv(f'{OUTPUT_DIR}/enhanced_v3_detailed_results.csv', index=False)\n",
        "print(f\"Detailed results saved to: {OUTPUT_DIR}/enhanced_v3_detailed_results.csv\")\n",
        "\n",
        "# Display final table\n",
        "print(\"\\nFinal Results Table:\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save summary statistics\n",
        "summary_stats = pd.DataFrame({\n",
        "    'Metric': [\n",
        "        'Synthetic Samples',\n",
        "        'Real Samples', \n",
        "        'Valid Features Compared',\n",
        "        'Constant in Both (Excluded)',\n",
        "        'Constant in Synthetic Only (GAN Issue)',\n",
        "        'Constant in Real Only',\n",
        "        'KS Excellent Features',\n",
        "        'KS Good or Better Features',\n",
        "        'Mean KS Statistic',\n",
        "        'Mean Wasserstein Distance',\n",
        "        'Weighted Wasserstein Distance',\n",
        "        'Correlation Similarity (Pearson)',\n",
        "        'Correlation Similarity (Spearman)',\n",
        "        'PCA Centroid Distance',\n",
        "        'Negligible Effect Features',\n",
        "        'Small Effect Features',\n",
        "        'Medium Effect Features',\n",
        "        'Large Effect Features',\n",
        "        'Distribution Score (Weighted)',\n",
        "        'Distribution Score (Unweighted)',\n",
        "        'Correlation Score',\n",
        "        'Effect Size Score (Weighted)',\n",
        "        'Effect Size Score (Unweighted)',\n",
        "        'Wasserstein Score (Weighted)',\n",
        "        'Multivariate (PCA) Score',\n",
        "        'Overall Similarity Score (WEIGHTED)',\n",
        "        'Overall Similarity Score (UNWEIGHTED)',\n",
        "        'Weighting Impact'\n",
        "    ],\n",
        "    'Value': [\n",
        "        len(synthetic_sampled),\n",
        "        len(real_sampled),\n",
        "        len(feature_cols),\n",
        "        n_constant_both,\n",
        "        n_constant_syn_only,\n",
        "        n_constant_real_only,\n",
        "        (ks_df['similarity_level'] == 'Excellent').sum(),\n",
        "        good_or_better,\n",
        "        f\"{mean_ks:.4f}\",\n",
        "        f\"{mean_wd:.4f}\",\n",
        "        f\"{wd_weighted:.4f}\",\n",
        "        f\"{structure_corr:.4f}\",\n",
        "        f\"{structure_spearman:.4f}\",\n",
        "        f\"{centroid_distance:.4f}\",\n",
        "        effect_counts.get('Negligible', 0),\n",
        "        effect_counts.get('Small', 0),\n",
        "        effect_counts.get('Medium', 0),\n",
        "        effect_counts.get('Large', 0),\n",
        "        f\"{distribution_score_weighted:.1f}/100\",\n",
        "        f\"{distribution_score_unweighted:.1f}/100\",\n",
        "        f\"{correlation_score:.1f}/100\",\n",
        "        f\"{effect_score_weighted:.1f}/100\",\n",
        "        f\"{effect_score_unweighted:.1f}/100\",\n",
        "        f\"{wasserstein_score_weighted:.1f}/100\",\n",
        "        f\"{pca_score:.1f}/100\",\n",
        "        f\"{overall_score_weighted:.1f}/100\",\n",
        "        f\"{overall_score_unweighted:.1f}/100\",\n",
        "        f\"{overall_score_weighted - overall_score_unweighted:+.1f} points\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "summary_stats.to_csv(f'{OUTPUT_DIR}/enhanced_v3_summary.csv', index=False)\n",
        "print(f\"Summary saved to: {OUTPUT_DIR}/enhanced_v3_summary.csv\")\n",
        "summary_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Quality Assessment Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive quality dashboard\n",
        "fig = plt.figure(figsize=(20, 16))\n",
        "\n",
        "# 1. Score breakdown (bar chart) - comparing weighted vs unweighted\n",
        "ax1 = fig.add_subplot(2, 3, 1)\n",
        "score_names = ['Distribution', 'Correlation', 'Effect Size', 'Wasserstein', 'Multivariate']\n",
        "scores_weighted = [distribution_score_weighted, correlation_score, effect_score_weighted, \n",
        "                   wasserstein_score_weighted, pca_score]\n",
        "scores_unweighted = [distribution_score_unweighted, correlation_score, effect_score_unweighted,\n",
        "                     wasserstein_score_unweighted, pca_score]\n",
        "\n",
        "x = np.arange(len(score_names))\n",
        "width = 0.35\n",
        "bars1 = ax1.bar(x - width/2, scores_weighted, width, label='Weighted', alpha=0.8, color='#3498db')\n",
        "bars2 = ax1.bar(x + width/2, scores_unweighted, width, label='Unweighted', alpha=0.8, color='#95a5a6')\n",
        "\n",
        "ax1.axhline(y=overall_score_weighted, color='blue', linestyle='--', linewidth=2, \n",
        "            label=f'Overall (W): {overall_score_weighted:.1f}')\n",
        "ax1.axhline(y=overall_score_unweighted, color='gray', linestyle=':', linewidth=2,\n",
        "            label=f'Overall (UW): {overall_score_unweighted:.1f}')\n",
        "ax1.set_ylabel('Score (out of 100)')\n",
        "ax1.set_title('Component Scores\\n(Weighted vs Unweighted)', fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels(score_names, rotation=45, ha='right')\n",
        "ax1.set_ylim(0, 100)\n",
        "ax1.legend(loc='lower right', fontsize=8)\n",
        "\n",
        "# 2. KS statistic distribution (pie chart)\n",
        "ax2 = fig.add_subplot(2, 3, 2)\n",
        "similarity_counts = ks_df['similarity_level'].value_counts()\n",
        "colors_sim = {'Excellent': 'darkgreen', 'Good': 'limegreen', 'Moderate': 'orange', 'Poor': 'red'}\n",
        "pie_colors = [colors_sim[level] for level in similarity_counts.index]\n",
        "ax2.pie(similarity_counts.values, labels=similarity_counts.index, colors=pie_colors,\n",
        "        autopct='%1.1f%%', startangle=90)\n",
        "ax2.set_title('Feature Distribution Similarity\\n(by KS statistic threshold)', fontweight='bold')\n",
        "\n",
        "# 3. Effect size distribution\n",
        "ax3 = fig.add_subplot(2, 3, 3)\n",
        "effect_order = ['Negligible', 'Small', 'Medium', 'Large']\n",
        "effect_colors = ['green', 'yellowgreen', 'orange', 'red']\n",
        "effect_values = [effect_counts.get(e, 0) for e in effect_order]\n",
        "ax3.bar(effect_order, effect_values, color=effect_colors, alpha=0.8, edgecolor='black')\n",
        "ax3.set_ylabel('Number of Features')\n",
        "ax3.set_title(\"Effect Size Distribution (Cohen's d)\", fontweight='bold')\n",
        "\n",
        "# 4. Feature importance heatmap (top features)\n",
        "ax4 = fig.add_subplot(2, 3, 4)\n",
        "top_features_for_heatmap = worst_features_ranked.head(10)[['feature', 'ks_statistic', 'abs_d', 'importance_weight']]\n",
        "heatmap_data = top_features_for_heatmap.set_index('feature')[['ks_statistic', 'abs_d', 'importance_weight']]\n",
        "# Normalize for visualization\n",
        "heatmap_norm = heatmap_data.copy()\n",
        "for col in heatmap_norm.columns:\n",
        "    heatmap_norm[col] = (heatmap_data[col] - heatmap_data[col].min()) / (heatmap_data[col].max() - heatmap_data[col].min() + 1e-10)\n",
        "sns.heatmap(heatmap_norm, ax=ax4, cmap='RdYlGn_r', annot=True, fmt='.2f', cbar_kws={'shrink': 0.5})\n",
        "ax4.set_title('Top-10 Worst Features\\n(Normalized KS, Effect, Weight)', fontweight='bold')\n",
        "ax4.set_xticklabels(['KS Stat', \"Cohen's d\", 'Importance'], rotation=45, ha='right')\n",
        "\n",
        "# 5. PCA centroid visualization (smaller version)\n",
        "ax5 = fig.add_subplot(2, 3, 5)\n",
        "for label, color in [('Synthetic', 'blue'), ('Real', 'red')]:\n",
        "    mask = np.array(labels) == label\n",
        "    ax5.scatter(X_pca_full[mask, 0], X_pca_full[mask, 1], c=color, alpha=0.1, s=3, label=label)\n",
        "ax5.scatter(centroid_syn[0], centroid_syn[1], c='blue', s=150, marker='*', edgecolors='black', linewidths=2, zorder=5)\n",
        "ax5.scatter(centroid_real[0], centroid_real[1], c='red', s=150, marker='*', edgecolors='black', linewidths=2, zorder=5)\n",
        "ax5.plot([centroid_syn[0], centroid_real[0]], [centroid_syn[1], centroid_real[1]], 'k--', linewidth=2, alpha=0.7)\n",
        "ax5.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
        "ax5.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
        "ax5.set_title(f'PCA Centroids\\n(Distance: {centroid_distance_2d:.2f})', fontweight='bold')\n",
        "ax5.legend(fontsize=8)\n",
        "\n",
        "# 6. Overall verdict panel\n",
        "ax6 = fig.add_subplot(2, 3, 6)\n",
        "ax6.axis('off')\n",
        "\n",
        "# Determine color based on score\n",
        "if overall_score_weighted >= 80:\n",
        "    verdict_color = 'darkgreen'\n",
        "    verdict_text = 'EXCELLENT'\n",
        "elif overall_score_weighted >= 70:\n",
        "    verdict_color = 'green'\n",
        "    verdict_text = 'GOOD'\n",
        "elif overall_score_weighted >= 50:\n",
        "    verdict_color = 'orange'\n",
        "    verdict_text = 'MODERATE'\n",
        "else:\n",
        "    verdict_color = 'red'\n",
        "    verdict_text = 'POOR'\n",
        "\n",
        "textstr = f\"\"\"OVERALL SIMILARITY SCORE\n",
        "(Feature-Importance Weighted)\n",
        "\n",
        "{overall_score_weighted:.1f} / 100\n",
        "\n",
        "Verdict: {verdict_text}\n",
        "\n",
        "================================\n",
        "Synthetic samples: {len(synthetic_sampled):,}\n",
        "Real samples: {len(real_sampled):,}\n",
        "Features analyzed: {len(feature_cols)}\n",
        "================================\n",
        "\n",
        "Key Enhancements in v3:\n",
        "- Feature importance weighting\n",
        "- Top-K worst features table\n",
        "- Calibration bias check\n",
        "- PCA centroid distance\n",
        "- t-SNE density overlay\n",
        "\n",
        "Weighting Impact: {overall_score_weighted - overall_score_unweighted:+.1f} pts\n",
        "\"\"\"\n",
        "\n",
        "ax6.text(0.5, 0.5, textstr, transform=ax6.transAxes, fontsize=11,\n",
        "         verticalalignment='center', horizontalalignment='center',\n",
        "         bbox=dict(boxstyle='round', facecolor=verdict_color, alpha=0.2),\n",
        "         family='monospace')\n",
        "\n",
        "plt.suptitle('BGP Phase 1: Synthetic vs Real Normal Traffic - Enhanced Quality Dashboard (v3)', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{OUTPUT_DIR}/enhanced_v3_quality_dashboard.png', dpi=200, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR}/enhanced_v3_quality_dashboard.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save problematic features report with enhanced information\n",
        "problematic_report = {\n",
        "    'constant_both': problematic_features['constant_both'],\n",
        "    'constant_synthetic_only': problematic_features['constant_synthetic_only'],\n",
        "    'constant_real_only': problematic_features['constant_real_only'],\n",
        "    'large_effect_size': large_effect,\n",
        "    'poor_ks_similarity': poor_ks,\n",
        "    'top_k_worst_features': top_k_worst['feature'].tolist(),\n",
        "    'calibration_issues': {\n",
        "        'features_with_mean_bias': calibration_df_raw[calibration_df_raw['potential_mean_bias']]['feature'].tolist(),\n",
        "        'features_with_std_bias': calibration_df_raw[calibration_df_raw['potential_std_bias']]['feature'].tolist()\n",
        "    },\n",
        "    'multivariate_metrics': {\n",
        "        'pca_centroid_distance': centroid_distance,\n",
        "        'pca_centroid_distance_2d': centroid_distance_2d,\n",
        "        'tsne_centroid_distance': tsne_centroid_distance\n",
        "    },\n",
        "    'feature_importance_weights': FEATURE_IMPORTANCE_WEIGHTS,\n",
        "    'overall_scores': {\n",
        "        'weighted': overall_score_weighted,\n",
        "        'unweighted': overall_score_unweighted,\n",
        "        'weighting_impact': overall_score_weighted - overall_score_unweighted\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(f'{OUTPUT_DIR}/enhanced_v3_problematic_features.json', 'w') as f:\n",
        "    json.dump(problematic_report, f, indent=2, default=str)\n",
        "\n",
        "print(f\"Problematic features report saved to: {OUTPUT_DIR}/enhanced_v3_problematic_features.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary for paper\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SUMMARY FOR PAPER\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\"\"\n",
        "This enhanced validation (v3) includes:\n",
        "\n",
        "1. PER-FEATURE IMPORTANCE WEIGHTING\n",
        "   - Critical BGP features (announcements, withdrawals, flaps, nadas, rare_ases) \n",
        "     weighted 1.3-1.5x\n",
        "   - Edit distance buckets weighted 0.8-0.9x\n",
        "   - Weighting impact on overall score: {overall_score_weighted - overall_score_unweighted:+.1f} points\n",
        "\n",
        "2. TOP-{TOP_K_WORST_FEATURES} WORST FEATURES TABLE\n",
        "   - Ranked by composite score (0.5*KS_weighted + 0.5*Effect_weighted)\n",
        "   - Worst feature: {top_k_worst.iloc[0]['feature']} (score: {top_k_worst.iloc[0]['composite_score']:.4f})\n",
        "   - Helps identify model weaknesses linked to BGP semantics\n",
        "\n",
        "3. CALIBRATION CHECK\n",
        "   - Features with potential mean bias: {calibration_df_raw['potential_mean_bias'].sum()}\n",
        "   - Features with potential std bias: {calibration_df_raw['potential_std_bias'].sum()}\n",
        "   - No significant preprocessing bias detected: {calibration_df_raw['potential_mean_bias'].sum() == 0 and calibration_df_raw['potential_std_bias'].sum() == 0}\n",
        "\n",
        "4. MULTIVARIATE ANALYSIS\n",
        "   - PCA centroid Euclidean distance: {centroid_distance:.4f}\n",
        "   - PCA centroid distance (2D): {centroid_distance_2d:.4f}\n",
        "   - t-SNE centroid distance: {tsne_centroid_distance:.4f}\n",
        "   - Mahalanobis distance: {mahal_distance:.4f if not np.isnan(mahal_distance) else 'N/A'}\n",
        "\n",
        "FINAL SCORES:\n",
        "   - Weighted Overall Score: {overall_score_weighted:.1f}/100\n",
        "   - Unweighted Overall Score: {overall_score_unweighted:.1f}/100\n",
        "   - Verdict: {verdict_text}\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "generator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}