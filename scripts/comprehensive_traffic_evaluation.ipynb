{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Traffic Generation Evaluation\n",
    "\n",
    "This notebook systematically compares all BGP traffic generation methods:\n",
    "\n",
    "**Generation Methods:**\n",
    "- SCAPY (direct packet generation)\n",
    "- GAN-default (LSTM, TimeGAN, DoppelGanger)\n",
    "- GAN-enhanced/tuned (LSTM, TimeGAN, DoppelGanger)\n",
    "- SMOTE variants (normal, borderline, kmeans, adasyn)\n",
    "- Hybrid (SMOTE + GAN)\n",
    "- Copula\n",
    "\n",
    "**Evaluation Datasets:**\n",
    "- Same dataset (rrc05): Training data evaluation\n",
    "- Different dataset (rrc04): Generalization evaluation\n",
    "\n",
    "**Metrics:**\n",
    "- KS Statistics (distribution similarity)\n",
    "- Wasserstein Distance (Earth Mover's Distance)\n",
    "- Cohen's d (effect size)\n",
    "- Correlation structure comparison\n",
    "- PCA centroid distance\n",
    "- t-SNE visualization\n",
    "- Feature-importance weighted scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "print(f\"Analysis timestamp: {TIMESTAMP}\")\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "**IMPORTANT:** Update the paths below to match your actual data locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - MODIFY THESE PATHS TO MATCH YOUR DATA\n",
    "# =============================================================================\n",
    "\n",
    "# Base path for results\n",
    "RESULTS_BASE_PATH = '/home/smotaali/BGP_Traffic_Generation/results'\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = f'{RESULTS_BASE_PATH}/comprehensive_evaluation/{TIMESTAMP}'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Real datasets for comparison\n",
    "REAL_DATASETS = {\n",
    "    'rrc05_same': f'{RESULTS_BASE_PATH}/final_label_results_HDBSCAN/rrc05_updates_20251216_extracted_discovered.csv',\n",
    "    'rrc04_different': f'{RESULTS_BASE_PATH}/final_label_results_HDBSCAN/rrc04_updates_20251116_extracted_discovered.csv'\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# SYNTHETIC DATASETS - UPDATE PATHS FOR YOUR DATA\n",
    "# =============================================================================\n",
    "# Structure: 'method_name': {'same_rrc05': path, 'diff_rrc04': path}\n",
    "\n",
    "SYNTHETIC_DATASETS = {\n",
    "    # SCAPY (direct generation - only one version)\n",
    "    'SCAPY': {\n",
    "        'generated': ''  # Path to SCAPY generated CSV\n",
    "    },\n",
    "    \n",
    "    # GAN Default Values\n",
    "    'GAN_LSTM_default': {\n",
    "        'same_rrc05': '',  # Evaluated on same rrc05 data\n",
    "        'diff_rrc04': ''   # Evaluated on different rrc04 data\n",
    "    },\n",
    "    'GAN_TimeGAN_default': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    'GAN_DoppelGanger_default': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    \n",
    "    # GAN Enhanced/Tuned Parameters\n",
    "    'GAN_LSTM_enhanced': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    'GAN_TimeGAN_enhanced': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    'GAN_DoppelGanger_enhanced': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    \n",
    "    # SMOTE Variants\n",
    "    'SMOTE_normal': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    'SMOTE_borderline': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    'SMOTE_kmeans': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    'SMOTE_adasyn': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    \n",
    "    # Hybrid (SMOTE + GAN)\n",
    "    'Hybrid_SMOTE_GAN': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    },\n",
    "    \n",
    "    # Copula\n",
    "    'Copula': {\n",
    "        'same_rrc05': '',\n",
    "        'diff_rrc04': ''\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluation parameters\n",
    "RANDOM_SEED = 42\n",
    "ALPHA = 0.05\n",
    "\n",
    "# KS statistic thresholds\n",
    "KS_EXCELLENT_THRESHOLD = 0.05\n",
    "KS_GOOD_THRESHOLD = 0.10\n",
    "KS_MODERATE_THRESHOLD = 0.15\n",
    "\n",
    "# Cohen's d cap\n",
    "COHENS_D_CAP = 10.0\n",
    "\n",
    "# Columns to exclude from analysis\n",
    "EXCLUDE_COLS = [\n",
    "    'timestamp', 'sequence_id', 'timestep', 'label', 'window_start', 'window_end',\n",
    "    'discovered_label', 'generation_method', 'log_transform_used', 'bgp_constraints_enforced'\n",
    "]\n",
    "\n",
    "# Feature importance weights for BGP semantics\n",
    "FEATURE_IMPORTANCE_WEIGHTS = {\n",
    "    # Core routing activity (highest importance)\n",
    "    'announcements_rate': 2.0,\n",
    "    'withdrawals_rate': 2.0,\n",
    "    'announcements_count': 2.0,\n",
    "    'withdrawals_count': 2.0,\n",
    "    \n",
    "    # Instability indicators\n",
    "    'flap_count': 1.8,\n",
    "    'flap_rate': 1.8,\n",
    "    \n",
    "    # Path dynamics\n",
    "    'edit_dist_mean': 1.5,\n",
    "    'edit_dist_max': 1.5,\n",
    "    'path_length_mean': 1.5,\n",
    "    'path_length_max': 1.5,\n",
    "    \n",
    "    # Prefix activity\n",
    "    'unique_prefixes': 1.3,\n",
    "    'prefix_announcements': 1.3,\n",
    "    \n",
    "    # AS-path related\n",
    "    'unique_origins': 1.2,\n",
    "    'avg_as_path_length': 1.2,\n",
    "    \n",
    "    # Default for other features\n",
    "    '_default': 1.0\n",
    "}\n",
    "\n",
    "# Top K worst features to highlight\n",
    "TOP_K_WORST = 10\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Configuration loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Get feature columns excluding metadata\"\"\"\n",
    "    return [col for col in df.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "\n",
    "def get_upper_triangle(matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Extract upper triangle elements from correlation matrix\"\"\"\n",
    "    return matrix[np.triu_indices(len(matrix), k=1)]\n",
    "\n",
    "\n",
    "def cohens_d(group1: pd.Series, group2: pd.Series, cap: float = COHENS_D_CAP) -> float:\n",
    "    \"\"\"Calculate Cohen's d effect size with capping\"\"\"\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1, var2 = group1.var(), group2.var()\n",
    "    \n",
    "    if n1 + n2 - 2 <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    \n",
    "    if pooled_std == 0:\n",
    "        if group1.mean() == group2.mean():\n",
    "            return 0.0\n",
    "        else:\n",
    "            return cap if group1.mean() > group2.mean() else -cap\n",
    "    \n",
    "    d = (group1.mean() - group2.mean()) / pooled_std\n",
    "    return np.clip(d, -cap, cap)\n",
    "\n",
    "\n",
    "def interpret_cohens_d(d: float) -> str:\n",
    "    \"\"\"Interpret Cohen's d value\"\"\"\n",
    "    abs_d = abs(d)\n",
    "    if abs_d < 0.2:\n",
    "        return 'Negligible'\n",
    "    elif abs_d < 0.5:\n",
    "        return 'Small'\n",
    "    elif abs_d < 0.8:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Large'\n",
    "\n",
    "\n",
    "def interpret_ks_statistic(ks_stat: float) -> str:\n",
    "    \"\"\"Interpret KS statistic threshold\"\"\"\n",
    "    if ks_stat < KS_EXCELLENT_THRESHOLD:\n",
    "        return 'Excellent'\n",
    "    elif ks_stat < KS_GOOD_THRESHOLD:\n",
    "        return 'Good'\n",
    "    elif ks_stat < KS_MODERATE_THRESHOLD:\n",
    "        return 'Moderate'\n",
    "    else:\n",
    "        return 'Poor'\n",
    "\n",
    "\n",
    "def get_weight(feature: str) -> float:\n",
    "    \"\"\"Get importance weight for a feature\"\"\"\n",
    "    return FEATURE_IMPORTANCE_WEIGHTS.get(feature, FEATURE_IMPORTANCE_WEIGHTS['_default'])\n",
    "\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Auto-Discover Datasets\n",
    "\n",
    "This section automatically discovers datasets in your results directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_discover_datasets(base_path: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Automatically discover synthetic datasets from the results directory.\n",
    "    \"\"\"\n",
    "    datasets = {}\n",
    "    base = Path(base_path)\n",
    "    \n",
    "    if not base.exists():\n",
    "        print(f\"Base path does not exist: {base_path}\")\n",
    "        return datasets\n",
    "    \n",
    "    # Search for method directories\n",
    "    for item in base.iterdir():\n",
    "        if item.is_dir():\n",
    "            method_name = item.name\n",
    "            datasets[method_name] = {}\n",
    "            \n",
    "            # Look for CSV files\n",
    "            for csv_file in item.glob('*.csv'):\n",
    "                filename = csv_file.stem.lower()\n",
    "                \n",
    "                if 'rrc05' in filename or 'same' in filename:\n",
    "                    datasets[method_name]['same_rrc05'] = str(csv_file)\n",
    "                elif 'rrc04' in filename or 'diff' in filename:\n",
    "                    datasets[method_name]['diff_rrc04'] = str(csv_file)\n",
    "                elif 'synthetic' in filename or 'generated' in filename:\n",
    "                    datasets[method_name]['generated'] = str(csv_file)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Auto-discover\n",
    "discovered = auto_discover_datasets(RESULTS_BASE_PATH)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"AUTO-DISCOVERED DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if discovered:\n",
    "    for method, paths in discovered.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        for comp_type, path in paths.items():\n",
    "            print(f\"  {comp_type}: {path}\")\n",
    "else:\n",
    "    print(\"No datasets discovered. Please update paths manually in SYNTHETIC_DATASETS.\")\n",
    "\n",
    "# Update SYNTHETIC_DATASETS with discovered paths\n",
    "for method, paths in discovered.items():\n",
    "    if method not in SYNTHETIC_DATASETS:\n",
    "        SYNTHETIC_DATASETS[method] = {}\n",
    "    SYNTHETIC_DATASETS[method].update(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Real Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load real datasets\n",
    "real_data = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING REAL DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, path in REAL_DATASETS.items():\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        # Filter only normal traffic\n",
    "        if 'discovered_label' in df.columns:\n",
    "            df = df[df['discovered_label'] == 'likely_normal'].copy()\n",
    "        real_data[name] = df\n",
    "        print(f\"  Loaded {name}: {len(df)} samples, {len(get_feature_columns(df))} features\")\n",
    "    else:\n",
    "        print(f\"  [WARNING] File not found: {path}\")\n",
    "\n",
    "print(f\"\\nTotal real datasets loaded: {len(real_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Core Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_comparison(\n",
    "    synthetic_df: pd.DataFrame,\n",
    "    real_df: pd.DataFrame,\n",
    "    method_name: str,\n",
    "    comparison_type: str\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of synthetic vs real data.\n",
    "    Returns a dictionary with all metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {\n",
    "        'method': method_name,\n",
    "        'comparison': comparison_type,\n",
    "        'feature_metrics': [],\n",
    "        'correlation_metrics': {},\n",
    "        'multivariate_metrics': {},\n",
    "        'overall_scores': {},\n",
    "        'problematic_features': {},\n",
    "        'constant_features': {}\n",
    "    }\n",
    "    \n",
    "    # Filter synthetic data if has label column\n",
    "    if 'label' in synthetic_df.columns:\n",
    "        syn_labels = synthetic_df['label'].unique()\n",
    "        if 'synthetic' in syn_labels:\n",
    "            synthetic_df = synthetic_df[synthetic_df['label'] == 'synthetic'].copy()\n",
    "    \n",
    "    # Get common feature columns\n",
    "    feature_cols = get_feature_columns(synthetic_df)\n",
    "    feature_cols = [c for c in feature_cols if c in real_df.columns]\n",
    "    \n",
    "    # Sample equal amounts\n",
    "    n_samples = min(len(synthetic_df), len(real_df))\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    synthetic_sampled = synthetic_df.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "    real_sampled = real_df.sample(n=n_samples, random_state=RANDOM_SEED)\n",
    "    \n",
    "    results['n_synthetic'] = n_samples\n",
    "    results['n_real'] = n_samples\n",
    "    \n",
    "    # Identify constant features\n",
    "    constant_both = []\n",
    "    constant_syn_only = []\n",
    "    constant_real_only = []\n",
    "    valid_features = []\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        syn_std = synthetic_sampled[col].std()\n",
    "        real_std = real_sampled[col].std()\n",
    "        \n",
    "        if syn_std == 0 and real_std == 0:\n",
    "            constant_both.append(col)\n",
    "        elif syn_std == 0 and real_std > 0:\n",
    "            constant_syn_only.append(col)\n",
    "        elif syn_std > 0 and real_std == 0:\n",
    "            constant_real_only.append(col)\n",
    "        else:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    results['constant_features'] = {\n",
    "        'constant_both': constant_both,\n",
    "        'constant_synthetic_only': constant_syn_only,\n",
    "        'constant_real_only': constant_real_only\n",
    "    }\n",
    "    results['n_features'] = len(valid_features)\n",
    "    \n",
    "    # Calculate per-feature metrics\n",
    "    ks_pvalues = []\n",
    "    mw_pvalues = []\n",
    "    \n",
    "    for col in valid_features:\n",
    "        syn_col = synthetic_sampled[col]\n",
    "        real_col = real_sampled[col]\n",
    "        \n",
    "        # KS Test\n",
    "        ks_stat, ks_pvalue = stats.ks_2samp(syn_col, real_col)\n",
    "        \n",
    "        # Wasserstein Distance (normalized)\n",
    "        syn_normalized = (syn_col - syn_col.mean()) / (syn_col.std() + 1e-10)\n",
    "        real_normalized = (real_col - real_col.mean()) / (real_col.std() + 1e-10)\n",
    "        wd = wasserstein_distance(syn_normalized, real_normalized)\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        mw_stat, mw_pvalue = stats.mannwhitneyu(syn_col, real_col, alternative='two-sided')\n",
    "        \n",
    "        # Effect size\n",
    "        d = cohens_d(syn_col, real_col)\n",
    "        d_interp = interpret_cohens_d(d)\n",
    "        \n",
    "        # Similarity level\n",
    "        sim_level = interpret_ks_statistic(ks_stat)\n",
    "        \n",
    "        # Mean difference\n",
    "        syn_mean = syn_col.mean()\n",
    "        real_mean = real_col.mean()\n",
    "        mean_diff = abs(syn_mean - real_mean)\n",
    "        pct_diff = (mean_diff / (abs(real_mean) + 1e-10)) * 100 if abs(real_mean) > 0.01 else 0\n",
    "        \n",
    "        weight = get_weight(col)\n",
    "        \n",
    "        ks_pvalues.append(ks_pvalue)\n",
    "        mw_pvalues.append(mw_pvalue)\n",
    "        \n",
    "        results['feature_metrics'].append({\n",
    "            'feature': col,\n",
    "            'ks_statistic': ks_stat,\n",
    "            'ks_pvalue': ks_pvalue,\n",
    "            'wasserstein_distance': wd,\n",
    "            'cohens_d': d,\n",
    "            'cohens_d_interpretation': d_interp,\n",
    "            'similarity_level': sim_level,\n",
    "            'mw_statistic': mw_stat,\n",
    "            'mw_pvalue': mw_pvalue,\n",
    "            'syn_mean': syn_mean,\n",
    "            'real_mean': real_mean,\n",
    "            'syn_std': syn_col.std(),\n",
    "            'real_std': real_col.std(),\n",
    "            'mean_diff': mean_diff,\n",
    "            'pct_diff': pct_diff,\n",
    "            'importance_weight': weight\n",
    "        })\n",
    "    \n",
    "    # Apply FDR correction\n",
    "    if ks_pvalues:\n",
    "        _, ks_adj, _, _ = multipletests(ks_pvalues, method='fdr_bh')\n",
    "        _, mw_adj, _, _ = multipletests(mw_pvalues, method='fdr_bh')\n",
    "        for i, fm in enumerate(results['feature_metrics']):\n",
    "            fm['ks_adjusted_pvalue'] = ks_adj[i]\n",
    "            fm['mw_adjusted_pvalue'] = mw_adj[i]\n",
    "    \n",
    "    # Correlation structure analysis\n",
    "    if len(valid_features) > 1:\n",
    "        corr_syn = synthetic_sampled[valid_features].corr()\n",
    "        corr_real = real_sampled[valid_features].corr()\n",
    "        corr_diff = abs(corr_syn - corr_real)\n",
    "        \n",
    "        corr_syn_flat = get_upper_triangle(corr_syn.values)\n",
    "        corr_real_flat = get_upper_triangle(corr_real.values)\n",
    "        \n",
    "        valid_mask = ~(np.isnan(corr_syn_flat) | np.isnan(corr_real_flat))\n",
    "        if valid_mask.sum() > 0:\n",
    "            pearson_corr, _ = stats.pearsonr(corr_syn_flat[valid_mask], corr_real_flat[valid_mask])\n",
    "            spearman_corr, _ = stats.spearmanr(corr_syn_flat[valid_mask], corr_real_flat[valid_mask])\n",
    "        else:\n",
    "            pearson_corr = 0.0\n",
    "            spearman_corr = 0.0\n",
    "        \n",
    "        results['correlation_metrics'] = {\n",
    "            'pearson_correlation': pearson_corr,\n",
    "            'spearman_correlation': spearman_corr,\n",
    "            'mean_abs_diff': np.nanmean(corr_diff.values),\n",
    "            'max_abs_diff': np.nanmax(corr_diff.values),\n",
    "            'median_abs_diff': np.nanmedian(corr_diff.values)\n",
    "        }\n",
    "        \n",
    "        # Store correlation matrices for visualization\n",
    "        results['corr_synthetic'] = corr_syn\n",
    "        results['corr_real'] = corr_real\n",
    "    \n",
    "    # Multivariate analysis (PCA)\n",
    "    X_syn = synthetic_sampled[valid_features].values\n",
    "    X_real = real_sampled[valid_features].values\n",
    "    X_combined = np.vstack([X_syn, X_real])\n",
    "    labels = np.array([0] * len(X_syn) + [1] * len(X_real))\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_combined)\n",
    "    X_syn_scaled = X_scaled[:len(X_syn)]\n",
    "    X_real_scaled = X_scaled[len(X_syn):]\n",
    "    \n",
    "    n_components = min(10, len(valid_features))\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    X_pca_syn = X_pca[:len(X_syn)]\n",
    "    X_pca_real = X_pca[len(X_syn):]\n",
    "    \n",
    "    centroid_syn = X_pca_syn.mean(axis=0)\n",
    "    centroid_real = X_pca_real.mean(axis=0)\n",
    "    \n",
    "    pca_centroid_distance = np.linalg.norm(centroid_syn - centroid_real)\n",
    "    pca_centroid_distance_2d = np.linalg.norm(centroid_syn[:2] - centroid_real[:2])\n",
    "    \n",
    "    try:\n",
    "        sil_score = silhouette_score(X_pca[:, :2], labels)\n",
    "    except:\n",
    "        sil_score = np.nan\n",
    "    \n",
    "    results['multivariate_metrics'] = {\n",
    "        'pca_centroid_distance': pca_centroid_distance,\n",
    "        'pca_centroid_distance_2d': pca_centroid_distance_2d,\n",
    "        'silhouette_score': sil_score,\n",
    "        'pca_explained_variance': pca.explained_variance_ratio_.tolist()\n",
    "    }\n",
    "    \n",
    "    # Store PCA results for visualization\n",
    "    results['X_pca'] = X_pca\n",
    "    results['pca_labels'] = labels\n",
    "    results['pca'] = pca\n",
    "    results['centroid_syn'] = centroid_syn\n",
    "    results['centroid_real'] = centroid_real\n",
    "    \n",
    "    # Calculate overall scores\n",
    "    fm_list = results['feature_metrics']\n",
    "    total_weight = sum(fm['importance_weight'] for fm in fm_list)\n",
    "    n_features = len(fm_list)\n",
    "    \n",
    "    # Distribution score (weighted)\n",
    "    good_or_better_weighted = sum(\n",
    "        fm['importance_weight'] for fm in fm_list\n",
    "        if fm['similarity_level'] in ['Excellent', 'Good']\n",
    "    )\n",
    "    distribution_score_weighted = (good_or_better_weighted / total_weight) * 100 if total_weight > 0 else 0\n",
    "    \n",
    "    # Distribution score (unweighted)\n",
    "    good_or_better = sum(1 for fm in fm_list if fm['similarity_level'] in ['Excellent', 'Good'])\n",
    "    distribution_score_unweighted = (good_or_better / n_features) * 100 if n_features > 0 else 0\n",
    "    \n",
    "    # Correlation score\n",
    "    correlation_score = ((results['correlation_metrics'].get('pearson_correlation', 0) + 1) / 2) * 100\n",
    "    \n",
    "    # Effect size score\n",
    "    effect_weights = {'Negligible': 4, 'Small': 3, 'Medium': 2, 'Large': 1}\n",
    "    effect_score_weighted = sum(\n",
    "        effect_weights.get(fm['cohens_d_interpretation'], 1) * fm['importance_weight']\n",
    "        for fm in fm_list\n",
    "    ) / (4 * total_weight) * 100 if total_weight > 0 else 0\n",
    "    \n",
    "    effect_score_unweighted = sum(\n",
    "        effect_weights.get(fm['cohens_d_interpretation'], 1)\n",
    "        for fm in fm_list\n",
    "    ) / (4 * n_features) * 100 if n_features > 0 else 0\n",
    "    \n",
    "    # Wasserstein score\n",
    "    mean_wd = np.mean([fm['wasserstein_distance'] for fm in fm_list]) if fm_list else 0\n",
    "    wasserstein_score = max(0, 100 - mean_wd * 50)\n",
    "    \n",
    "    # Multivariate score\n",
    "    pca_score = max(0, 100 - pca_centroid_distance * 10)\n",
    "    \n",
    "    # Overall scores\n",
    "    overall_weighted = (\n",
    "        distribution_score_weighted * 0.30 +\n",
    "        correlation_score * 0.20 +\n",
    "        effect_score_weighted * 0.25 +\n",
    "        wasserstein_score * 0.15 +\n",
    "        pca_score * 0.10\n",
    "    )\n",
    "    \n",
    "    overall_unweighted = (\n",
    "        distribution_score_unweighted * 0.30 +\n",
    "        correlation_score * 0.20 +\n",
    "        effect_score_unweighted * 0.25 +\n",
    "        wasserstein_score * 0.15 +\n",
    "        pca_score * 0.10\n",
    "    )\n",
    "    \n",
    "    results['overall_scores'] = {\n",
    "        'distribution_score_weighted': distribution_score_weighted,\n",
    "        'distribution_score_unweighted': distribution_score_unweighted,\n",
    "        'correlation_score': correlation_score,\n",
    "        'effect_size_score_weighted': effect_score_weighted,\n",
    "        'effect_size_score_unweighted': effect_score_unweighted,\n",
    "        'wasserstein_score': wasserstein_score,\n",
    "        'multivariate_score': pca_score,\n",
    "        'overall_score_weighted': overall_weighted,\n",
    "        'overall_score_unweighted': overall_unweighted\n",
    "    }\n",
    "    \n",
    "    # Identify problematic features\n",
    "    results['problematic_features'] = {\n",
    "        'large_effect': [fm['feature'] for fm in fm_list if fm['cohens_d_interpretation'] == 'Large'],\n",
    "        'poor_ks': [fm['feature'] for fm in fm_list if fm['similarity_level'] == 'Poor'],\n",
    "        'high_wasserstein': sorted(\n",
    "            [(fm['feature'], fm['wasserstein_distance']) for fm in fm_list],\n",
    "            key=lambda x: x[1], reverse=True\n",
    "        )[:TOP_K_WORST]\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all results\n",
    "all_results = {}\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for method_name, syn_paths in SYNTHETIC_DATASETS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Method: {method_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_results[method_name] = {}\n",
    "    \n",
    "    for comp_type, syn_path in syn_paths.items():\n",
    "        if not syn_path or not os.path.exists(syn_path):\n",
    "            print(f\"  [SKIP] {comp_type}: Path not found or empty\")\n",
    "            continue\n",
    "        \n",
    "        # Determine which real dataset to use\n",
    "        if 'rrc05' in comp_type or 'same' in comp_type or 'generated' in comp_type:\n",
    "            real_key = 'rrc05_same'\n",
    "        else:\n",
    "            real_key = 'rrc04_different'\n",
    "        \n",
    "        if real_key not in real_data:\n",
    "            print(f\"  [SKIP] {comp_type}: Real data {real_key} not loaded\")\n",
    "            continue\n",
    "        \n",
    "        # Load synthetic data\n",
    "        try:\n",
    "            synthetic_df = pd.read_csv(syn_path)\n",
    "            print(f\"  Loading {comp_type}: {len(synthetic_df)} samples\")\n",
    "        except Exception as e:\n",
    "            print(f\"  [ERROR] Loading {syn_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Evaluate\n",
    "        result = evaluate_single_comparison(\n",
    "            synthetic_df,\n",
    "            real_data[real_key],\n",
    "            method_name,\n",
    "            comp_type\n",
    "        )\n",
    "        all_results[method_name][comp_type] = result\n",
    "        \n",
    "        print(f\"    -> Overall Score (Weighted): {result['overall_scores']['overall_score_weighted']:.2f}\")\n",
    "        print(f\"    -> Features compared: {result['n_features']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Comparison Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comparison table\n",
    "comparison_rows = []\n",
    "\n",
    "for method_name, method_results in all_results.items():\n",
    "    for comp_type, result in method_results.items():\n",
    "        comparison_rows.append({\n",
    "            'Method': method_name,\n",
    "            'Comparison': comp_type,\n",
    "            'N_Samples': result['n_synthetic'],\n",
    "            'N_Features': result['n_features'],\n",
    "            'Overall_W': round(result['overall_scores']['overall_score_weighted'], 2),\n",
    "            'Overall_UW': round(result['overall_scores']['overall_score_unweighted'], 2),\n",
    "            'Distribution_W': round(result['overall_scores']['distribution_score_weighted'], 2),\n",
    "            'Correlation': round(result['overall_scores']['correlation_score'], 2),\n",
    "            'Effect_W': round(result['overall_scores']['effect_size_score_weighted'], 2),\n",
    "            'Wasserstein': round(result['overall_scores']['wasserstein_score'], 2),\n",
    "            'PCA': round(result['overall_scores']['multivariate_score'], 2),\n",
    "            'N_Large_Effect': len(result['problematic_features']['large_effect']),\n",
    "            'N_Poor_KS': len(result['problematic_features']['poor_ks']),\n",
    "            'N_Const_Syn': len(result['constant_features']['constant_synthetic_only'])\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "comparison_df = comparison_df.sort_values('Overall_W', ascending=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON TABLE (Sorted by Overall Weighted Score)\")\n",
    "print(\"=\" * 80)\n",
    "display(comparison_df)\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv(f'{OUTPUT_DIR}/comparison_table.csv', index=False)\n",
    "print(f\"\\nSaved: {OUTPUT_DIR}/comparison_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization: Overall Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not comparison_df.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "    \n",
    "    # 1. Overall scores bar chart\n",
    "    ax1 = axes[0, 0]\n",
    "    methods = comparison_df['Method'] + '\\n(' + comparison_df['Comparison'] + ')'\n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x - width/2, comparison_df['Overall_W'], width, label='Weighted', alpha=0.8, color='#3498db')\n",
    "    bars2 = ax1.bar(x + width/2, comparison_df['Overall_UW'], width, label='Unweighted', alpha=0.8, color='#95a5a6')\n",
    "    \n",
    "    ax1.set_xlabel('Method', fontsize=10)\n",
    "    ax1.set_ylabel('Overall Score', fontsize=10)\n",
    "    ax1.set_title('Overall Similarity Scores by Method', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(methods, rotation=45, ha='right', fontsize=8)\n",
    "    ax1.legend()\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax1.axhline(y=70, color='g', linestyle='--', alpha=0.5, label='Good threshold')\n",
    "    ax1.axhline(y=50, color='orange', linestyle='--', alpha=0.5, label='Moderate threshold')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.annotate(f'{height:.0f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    # 2. Component scores heatmap\n",
    "    ax2 = axes[0, 1]\n",
    "    score_cols = ['Distribution_W', 'Correlation', 'Effect_W', 'Wasserstein', 'PCA']\n",
    "    score_data = comparison_df[score_cols].values\n",
    "    \n",
    "    im = ax2.imshow(score_data, cmap='RdYlGn', aspect='auto', vmin=0, vmax=100)\n",
    "    ax2.set_xticks(np.arange(len(score_cols)))\n",
    "    ax2.set_xticklabels(score_cols, rotation=45, ha='right')\n",
    "    ax2.set_yticks(np.arange(len(methods)))\n",
    "    ax2.set_yticklabels(methods, fontsize=8)\n",
    "    ax2.set_title('Component Scores Heatmap', fontsize=12, fontweight='bold')\n",
    "    plt.colorbar(im, ax=ax2, label='Score')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(methods)):\n",
    "        for j in range(len(score_cols)):\n",
    "            text = ax2.text(j, i, f'{score_data[i, j]:.0f}',\n",
    "                           ha='center', va='center', fontsize=7,\n",
    "                           color='white' if score_data[i, j] < 50 else 'black')\n",
    "    \n",
    "    # 3. Problematic features count\n",
    "    ax3 = axes[1, 0]\n",
    "    issue_cols = ['N_Large_Effect', 'N_Poor_KS', 'N_Const_Syn']\n",
    "    issue_labels = ['Large Effect Size', 'Poor KS Score', 'Constant in Synthetic']\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    colors = ['#e74c3c', '#f39c12', '#9b59b6']\n",
    "    \n",
    "    for i, (col, label, color) in enumerate(zip(issue_cols, issue_labels, colors)):\n",
    "        ax3.bar(x + i * width, comparison_df[col], width, label=label, alpha=0.8, color=color)\n",
    "    \n",
    "    ax3.set_xlabel('Method', fontsize=10)\n",
    "    ax3.set_ylabel('Number of Features', fontsize=10)\n",
    "    ax3.set_title('Problematic Features Count', fontsize=12, fontweight='bold')\n",
    "    ax3.set_xticks(x + width)\n",
    "    ax3.set_xticklabels(methods, rotation=45, ha='right', fontsize=8)\n",
    "    ax3.legend(fontsize=8)\n",
    "    \n",
    "    # 4. Same vs Different dataset comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    same_scores = []\n",
    "    diff_scores = []\n",
    "    method_names = []\n",
    "    \n",
    "    for method in comparison_df['Method'].unique():\n",
    "        method_data = comparison_df[comparison_df['Method'] == method]\n",
    "        same_data = method_data[method_data['Comparison'].str.contains('same|rrc05|generated', case=False)]\n",
    "        diff_data = method_data[method_data['Comparison'].str.contains('diff|rrc04', case=False)]\n",
    "        \n",
    "        if not same_data.empty:\n",
    "            same_scores.append(same_data['Overall_W'].values[0])\n",
    "        else:\n",
    "            same_scores.append(0)\n",
    "        \n",
    "        if not diff_data.empty:\n",
    "            diff_scores.append(diff_data['Overall_W'].values[0])\n",
    "        else:\n",
    "            diff_scores.append(0)\n",
    "        \n",
    "        method_names.append(method)\n",
    "    \n",
    "    x = np.arange(len(method_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax4.bar(x - width/2, same_scores, width, label='Same Dataset (rrc05)', alpha=0.8, color='#2ecc71')\n",
    "    ax4.bar(x + width/2, diff_scores, width, label='Different Dataset (rrc04)', alpha=0.8, color='#e74c3c')\n",
    "    \n",
    "    ax4.set_xlabel('Method', fontsize=10)\n",
    "    ax4.set_ylabel('Overall Score (Weighted)', fontsize=10)\n",
    "    ax4.set_title('Generalization: Same vs Different Dataset', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(method_names, rotation=45, ha='right', fontsize=8)\n",
    "    ax4.legend()\n",
    "    ax4.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/overall_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {OUTPUT_DIR}/overall_comparison.png\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization: Radar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not comparison_df.empty and len(comparison_df) > 0:\n",
    "    # Get top methods for radar chart\n",
    "    top_n = min(8, len(comparison_df))\n",
    "    top_methods = comparison_df.head(top_n)\n",
    "    \n",
    "    # Metrics for radar chart\n",
    "    metrics = ['Distribution_W', 'Correlation', 'Effect_W', 'Wasserstein', 'PCA']\n",
    "    num_vars = len(metrics)\n",
    "    \n",
    "    # Create angles\n",
    "    angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 12), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(top_methods)))\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_methods.iterrows()):\n",
    "        values = row[metrics].values.flatten().tolist()\n",
    "        values += values[:1]\n",
    "        \n",
    "        label = f\"{row['Method']}\\n({row['Comparison']})\"\n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=label, color=colors[idx])\n",
    "        ax.fill(angles, values, alpha=0.1, color=colors[idx])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics, fontsize=11)\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.set_title(f'Top {len(top_methods)} Methods Comparison\\n(Higher = Better)', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/radar_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {OUTPUT_DIR}/radar_comparison.png\")\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature-Level Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate detailed feature comparison table\n",
    "feature_rows = []\n",
    "\n",
    "for method_name, method_results in all_results.items():\n",
    "    for comp_type, result in method_results.items():\n",
    "        for fm in result['feature_metrics']:\n",
    "            feature_rows.append({\n",
    "                'Method': method_name,\n",
    "                'Comparison': comp_type,\n",
    "                'Feature': fm['feature'],\n",
    "                'KS_Statistic': round(fm['ks_statistic'], 4),\n",
    "                'Wasserstein': round(fm['wasserstein_distance'], 4),\n",
    "                'Cohens_d': round(fm['cohens_d'], 4),\n",
    "                'Effect_Interp': fm['cohens_d_interpretation'],\n",
    "                'Similarity_Level': fm['similarity_level'],\n",
    "                'Mean_Diff_Pct': round(fm['pct_diff'], 2),\n",
    "                'Weight': fm['importance_weight']\n",
    "            })\n",
    "\n",
    "feature_df = pd.DataFrame(feature_rows)\n",
    "\n",
    "if not feature_df.empty:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"FEATURE-LEVEL COMPARISON (Sample)\")\n",
    "    print(\"=\" * 80)\n",
    "    display(feature_df.head(20))\n",
    "    \n",
    "    # Save to CSV\n",
    "    feature_df.to_csv(f'{OUTPUT_DIR}/feature_comparison.csv', index=False)\n",
    "    print(f\"\\nSaved: {OUTPUT_DIR}/feature_comparison.csv\")\n",
    "else:\n",
    "    print(\"No feature data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Feature Heatmap by Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not feature_df.empty:\n",
    "    # Create pivot table for KS statistic\n",
    "    pivot_ks = feature_df.pivot_table(\n",
    "        index='Feature',\n",
    "        columns=['Method', 'Comparison'],\n",
    "        values='KS_Statistic',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    if not pivot_ks.empty and len(pivot_ks.columns) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(max(14, len(pivot_ks.columns) * 1.2), \n",
    "                                        max(10, len(pivot_ks) * 0.35)))\n",
    "        \n",
    "        sns.heatmap(pivot_ks, ax=ax, cmap='RdYlGn_r', center=0.1, \n",
    "                    annot=True, fmt='.2f', annot_kws={'size': 7},\n",
    "                    cbar_kws={'label': 'KS Statistic (lower = better)'})\n",
    "        \n",
    "        ax.set_title('Feature-wise KS Statistic Comparison\\n(Green = Similar, Red = Different)', \n",
    "                     fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Method & Comparison', fontsize=10)\n",
    "        ax.set_ylabel('Feature', fontsize=10)\n",
    "        \n",
    "        plt.xticks(rotation=45, ha='right', fontsize=8)\n",
    "        plt.yticks(fontsize=8)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{OUTPUT_DIR}/feature_ks_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        print(f\"Saved: {OUTPUT_DIR}/feature_ks_heatmap.png\")\n",
    "    else:\n",
    "        print(\"Not enough data for heatmap\")\n",
    "else:\n",
    "    print(\"No feature data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. PCA Visualization for Best Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 4 methods for PCA visualization\n",
    "top_methods_for_pca = comparison_df.head(4)\n",
    "\n",
    "if len(top_methods_for_pca) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 14))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (_, row) in enumerate(top_methods_for_pca.iterrows()):\n",
    "        method = row['Method']\n",
    "        comp = row['Comparison']\n",
    "        \n",
    "        if method in all_results and comp in all_results[method]:\n",
    "            result = all_results[method][comp]\n",
    "            \n",
    "            if 'X_pca' in result:\n",
    "                ax = axes[idx]\n",
    "                X_pca = result['X_pca']\n",
    "                labels = result['pca_labels']\n",
    "                centroid_syn = result['centroid_syn']\n",
    "                centroid_real = result['centroid_real']\n",
    "                pca = result['pca']\n",
    "                \n",
    "                # Plot points\n",
    "                mask_syn = labels == 0\n",
    "                mask_real = labels == 1\n",
    "                \n",
    "                ax.scatter(X_pca[mask_syn, 0], X_pca[mask_syn, 1], \n",
    "                          c='blue', alpha=0.3, s=10, label='Synthetic')\n",
    "                ax.scatter(X_pca[mask_real, 0], X_pca[mask_real, 1], \n",
    "                          c='red', alpha=0.3, s=10, label='Real')\n",
    "                \n",
    "                # Plot centroids\n",
    "                ax.scatter(centroid_syn[0], centroid_syn[1], c='blue', s=200, \n",
    "                          marker='*', edgecolors='black', linewidths=2, \n",
    "                          label='Syn Centroid', zorder=5)\n",
    "                ax.scatter(centroid_real[0], centroid_real[1], c='red', s=200, \n",
    "                          marker='*', edgecolors='black', linewidths=2, \n",
    "                          label='Real Centroid', zorder=5)\n",
    "                \n",
    "                # Draw line between centroids\n",
    "                ax.plot([centroid_syn[0], centroid_real[0]], \n",
    "                       [centroid_syn[1], centroid_real[1]], \n",
    "                       'k--', linewidth=2, alpha=0.7)\n",
    "                \n",
    "                dist = np.linalg.norm(centroid_syn[:2] - centroid_real[:2])\n",
    "                ax.annotate(f'd={dist:.2f}', \n",
    "                           xy=((centroid_syn[0]+centroid_real[0])/2, \n",
    "                               (centroid_syn[1]+centroid_real[1])/2),\n",
    "                           fontsize=10, fontweight='bold',\n",
    "                           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "                \n",
    "                ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "                ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "                ax.set_title(f'{method}\\n({comp})\\nScore: {row[\"Overall_W\"]:.1f}', fontsize=10)\n",
    "                ax.legend(fontsize=8, loc='upper right')\n",
    "    \n",
    "    plt.suptitle('PCA Visualization: Top 4 Methods', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{OUTPUT_DIR}/pca_top_methods.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Saved: {OUTPUT_DIR}/pca_top_methods.png\")\n",
    "else:\n",
    "    print(\"No results available for PCA visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE TRAFFIC GENERATION EVALUATION - SUMMARY REPORT\")\n",
    "print(f\"Generated: {TIMESTAMP}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"RANKING BY OVERALL SCORE (WEIGHTED)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        print(f\"{i+1}. {row['Method']} ({row['Comparison']}): {row['Overall_W']:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"BEST METHOD BY CATEGORY\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    categories = {\n",
    "        'Distribution Similarity': 'Distribution_W',\n",
    "        'Correlation Structure': 'Correlation',\n",
    "        'Effect Size': 'Effect_W',\n",
    "        'Wasserstein Distance': 'Wasserstein',\n",
    "        'Multivariate (PCA)': 'PCA'\n",
    "    }\n",
    "    \n",
    "    for cat_name, col in categories.items():\n",
    "        best_idx = comparison_df[col].idxmax()\n",
    "        best_row = comparison_df.loc[best_idx]\n",
    "        print(f\"  {cat_name}: {best_row['Method']} ({best_row['Comparison']}) - {best_row[col]:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"GENERALIZATION ANALYSIS (Same vs Different Dataset)\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for method in comparison_df['Method'].unique():\n",
    "        method_data = comparison_df[comparison_df['Method'] == method]\n",
    "        same_data = method_data[method_data['Comparison'].str.contains('same|rrc05|generated', case=False)]\n",
    "        diff_data = method_data[method_data['Comparison'].str.contains('diff|rrc04', case=False)]\n",
    "        \n",
    "        if not same_data.empty and not diff_data.empty:\n",
    "            same_score = same_data['Overall_W'].values[0]\n",
    "            diff_score = diff_data['Overall_W'].values[0]\n",
    "            drop = same_score - diff_score\n",
    "            print(f\"  {method}: Same={same_score:.2f}, Diff={diff_score:.2f}, Drop={drop:.2f}\")\n",
    "        elif not same_data.empty:\n",
    "            print(f\"  {method}: Same={same_data['Overall_W'].values[0]:.2f}, Diff=N/A\")\n",
    "        elif not diff_data.empty:\n",
    "            print(f\"  {method}: Same=N/A, Diff={diff_data['Overall_W'].values[0]:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 40)\n",
    "    print(\"KEY FINDINGS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Best overall method\n",
    "    best = comparison_df.iloc[0]\n",
    "    print(f\"\\n  BEST OVERALL: {best['Method']} ({best['Comparison']})\")\n",
    "    print(f\"    Score: {best['Overall_W']:.2f}\")\n",
    "    \n",
    "    # Method with least issues\n",
    "    comparison_df['total_issues'] = comparison_df['N_Large_Effect'] + comparison_df['N_Poor_KS'] + comparison_df['N_Const_Syn']\n",
    "    least_issues = comparison_df.loc[comparison_df['total_issues'].idxmin()]\n",
    "    print(f\"\\n  LEAST PROBLEMATIC FEATURES: {least_issues['Method']} ({least_issues['Comparison']})\")\n",
    "    print(f\"    Total issues: {least_issues['total_issues']}\")\n",
    "    \n",
    "    # Best generalization (smallest drop)\n",
    "    gen_scores = []\n",
    "    for method in comparison_df['Method'].unique():\n",
    "        method_data = comparison_df[comparison_df['Method'] == method]\n",
    "        same_data = method_data[method_data['Comparison'].str.contains('same|rrc05|generated', case=False)]\n",
    "        diff_data = method_data[method_data['Comparison'].str.contains('diff|rrc04', case=False)]\n",
    "        \n",
    "        if not same_data.empty and not diff_data.empty:\n",
    "            drop = same_data['Overall_W'].values[0] - diff_data['Overall_W'].values[0]\n",
    "            gen_scores.append((method, drop))\n",
    "    \n",
    "    if gen_scores:\n",
    "        best_gen = min(gen_scores, key=lambda x: x[1])\n",
    "        print(f\"\\n  BEST GENERALIZATION: {best_gen[0]}\")\n",
    "        print(f\"    Score drop: {best_gen[1]:.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo evaluation results available.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"END OF REPORT\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary report as text file\n",
    "report_text = f\"\"\"\n",
    "{'='*80}\n",
    "COMPREHENSIVE TRAFFIC GENERATION EVALUATION - SUMMARY REPORT\n",
    "Generated: {TIMESTAMP}\n",
    "{'='*80}\n",
    "\n",
    "METHODS EVALUATED:\n",
    "\"\"\"\n",
    "\n",
    "if not comparison_df.empty:\n",
    "    for i, (_, row) in enumerate(comparison_df.iterrows()):\n",
    "        report_text += f\"\\n{i+1}. {row['Method']} ({row['Comparison']}): {row['Overall_W']:.2f}\"\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/summary_report.txt', 'w') as f:\n",
    "    f.write(report_text)\n",
    "print(f\"Saved: {OUTPUT_DIR}/summary_report.txt\")\n",
    "\n",
    "# Save raw results as JSON (simplified version without numpy arrays)\n",
    "results_json = {}\n",
    "for method_name, method_results in all_results.items():\n",
    "    results_json[method_name] = {}\n",
    "    for comp_type, result in method_results.items():\n",
    "        results_json[method_name][comp_type] = {\n",
    "            'n_synthetic': result['n_synthetic'],\n",
    "            'n_real': result['n_real'],\n",
    "            'n_features': result['n_features'],\n",
    "            'overall_scores': result['overall_scores'],\n",
    "            'correlation_metrics': result['correlation_metrics'],\n",
    "            'multivariate_metrics': {\n",
    "                k: v for k, v in result['multivariate_metrics'].items() \n",
    "                if k != 'pca_explained_variance' or isinstance(v, (list, float, int))\n",
    "            },\n",
    "            'problematic_features': {\n",
    "                k: v if isinstance(v, list) else [(x[0], float(x[1])) for x in v]\n",
    "                for k, v in result['problematic_features'].items()\n",
    "            },\n",
    "            'constant_features': result['constant_features']\n",
    "        }\n",
    "\n",
    "with open(f'{OUTPUT_DIR}/results.json', 'w') as f:\n",
    "    json.dump(results_json, f, indent=2, default=str)\n",
    "print(f\"Saved: {OUTPUT_DIR}/results.json\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Quick Reference: How to Use This Notebook\n",
    "\n",
    "### Setup Instructions:\n",
    "\n",
    "1. **Update paths in Section 1 (Configuration)**:\n",
    "   - Set `RESULTS_BASE_PATH` to your results directory\n",
    "   - Update `REAL_DATASETS` with paths to your real BGP data\n",
    "   - Fill in `SYNTHETIC_DATASETS` with paths to your generated data\n",
    "\n",
    "2. **Auto-discovery (Section 3)**:\n",
    "   - The notebook will try to auto-discover datasets in your results directory\n",
    "   - Check the output and manually add any missing paths\n",
    "\n",
    "3. **Run all cells** to generate the comprehensive evaluation\n",
    "\n",
    "### Output Files:\n",
    "- `comparison_table.csv`: Overall scores for all methods\n",
    "- `feature_comparison.csv`: Detailed per-feature metrics\n",
    "- `summary_report.txt`: Text summary of results\n",
    "- `results.json`: Raw results in JSON format\n",
    "- `*.png`: Visualization plots\n",
    "\n",
    "### Metrics Explained:\n",
    "- **KS Statistic**: Distribution similarity (lower = better)\n",
    "- **Wasserstein Distance**: Earth Mover's Distance (lower = better)\n",
    "- **Cohen's d**: Effect size (smaller = better)\n",
    "- **Correlation Score**: How well correlation structure is preserved\n",
    "- **PCA Score**: Multivariate similarity based on centroid distance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
