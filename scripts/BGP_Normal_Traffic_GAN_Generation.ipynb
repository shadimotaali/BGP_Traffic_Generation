{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Normal Traffic Generation using GAN Algorithms\n",
    "\n",
    "## Overview\n",
    "This notebook implements a comprehensive pipeline for generating synthetic BGP normal traffic using various GAN architectures:\n",
    "- **TimeGAN**: Temporal generative adversarial network for time-series\n",
    "- **DoppelGANger**: High-fidelity time-series generation\n",
    "- **LSTM-GAN**: LSTM-based generative adversarial network\n",
    "\n",
    "## Workflow\n",
    "1. Load and preprocess BGP traffic data\n",
    "2. Filter to normal traffic only\n",
    "3. Feature selection and normalization\n",
    "4. Sequence building for time-series models\n",
    "5. Train multiple GAN architectures\n",
    "6. Evaluate and compare models\n",
    "7. Generate synthetic normal traffic\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install torch torchvision pandas numpy scikit-learn matplotlib seaborn tqdm statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from scipy import stats\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Output directory: /home/smotaali/BGP_Traffic_Generation/results/gan_outputs_opus/\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters as needed\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    'data_path': '/home/smotaali/BGP_Traffic_Generation/results/rrc04_20251116_exctracted_1s_FIXED.csv',\n",
    "    'output_dir': '/home/smotaali/BGP_Traffic_Generation/results/gan_outputs_opus/',\n",
    "    \n",
    "    # Feature configuration\n",
    "    'selected_features': [\n",
    "        'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "        'origin_0', 'origin_2', 'origin_changes',\n",
    "        'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "        'as_path_max', 'unique_as_path_max',\n",
    "        'edit_distance_avg', 'edit_distance_max',\n",
    "        'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "        'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "        'edit_distance_dict_6',\n",
    "        'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "        'number_rare_ases', 'rare_ases_avg',\n",
    "        'nadas', 'flaps'\n",
    "    ],\n",
    "    \n",
    "    # Columns to drop\n",
    "    'drop_columns': ['label', 'window_start', 'window_end'],\n",
    "    \n",
    "    # Sequence parameters\n",
    "    'sequence_length': 30,  # T: window length in seconds (adjustable: 10-60)\n",
    "    'stride': 1,  # Sliding window stride\n",
    "    \n",
    "    # Train/test split\n",
    "    'test_size': 0.2,\n",
    "    'validation_size': 0.1,  # From training set\n",
    "    \n",
    "    # Training parameters\n",
    "    'batch_size': 64,\n",
    "    'epochs': 200,\n",
    "    'learning_rate': 0.0002,\n",
    "    'beta1': 0.5,  # Adam optimizer beta1\n",
    "    'beta2': 0.999,  # Adam optimizer beta2\n",
    "    \n",
    "    # Model architecture\n",
    "    'latent_dim': 32,\n",
    "    'hidden_dim': 128,\n",
    "    'num_layers': 3,\n",
    "    \n",
    "    # Evaluation\n",
    "    'n_synthetic_samples': 1000,  # Number of synthetic sequences to generate\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "print(f\"Configuration loaded. Output directory: {CONFIG['output_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Original data shape: (86387, 30)\n",
      "\n",
      "Columns: ['announcements', 'withdrawals', 'nlri_ann', 'dups', 'origin_0', 'origin_2', 'origin_changes', 'imp_wd', 'imp_wd_spath', 'imp_wd_dpath', 'as_path_max', 'unique_as_path_max', 'edit_distance_avg', 'edit_distance_max', 'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2', 'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5', 'edit_distance_dict_6', 'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1', 'number_rare_ases', 'rare_ases_avg', 'nadas', 'flaps', 'label', 'window_start', 'window_end']\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "normal    86387\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(CONFIG['data_path'])\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Filtering to NORMAL traffic only...\n",
      "============================================================\n",
      "Normal traffic rows: 86387 (100.00% of total)\n"
     ]
    }
   ],
   "source": [
    "# Filter to normal traffic only\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Filtering to NORMAL traffic only...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "df_normal = df[df['label'] == 'normal'].copy()\n",
    "print(f\"Normal traffic rows: {len(df_normal)} ({100*len(df_normal)/len(df):.2f}% of total)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Feature Selection\n",
      "============================================================\n",
      "Using 27 features: ['announcements', 'withdrawals', 'nlri_ann', 'dups', 'origin_0', 'origin_2', 'origin_changes', 'imp_wd', 'imp_wd_spath', 'imp_wd_dpath', 'as_path_max', 'unique_as_path_max', 'edit_distance_avg', 'edit_distance_max', 'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2', 'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5', 'edit_distance_dict_6', 'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1', 'number_rare_ases', 'rare_ases_avg', 'nadas', 'flaps']\n",
      "\n",
      "Missing values before cleaning: 0\n",
      "Data shape after feature selection: (86387, 27)\n"
     ]
    }
   ],
   "source": [
    "# Select only the specified features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Feature Selection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check which features exist in the data\n",
    "available_features = [f for f in CONFIG['selected_features'] if f in df_normal.columns]\n",
    "missing_features = [f for f in CONFIG['selected_features'] if f not in df_normal.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Warning: Missing features: {missing_features}\")\n",
    "\n",
    "print(f\"Using {len(available_features)} features: {available_features}\")\n",
    "\n",
    "# Select features\n",
    "df_features = df_normal[available_features].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "print(f\"\\nMissing values before cleaning: {df_features.isnull().sum().sum()}\")\n",
    "df_features = df_features.fillna(0)  # Fill NaN with 0 for BGP features\n",
    "print(f\"Data shape after feature selection: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Feature Statistics (Normal Traffic)\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>announcements</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>182.400141</td>\n",
       "      <td>313.207475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>12661.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>withdrawals</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>15.683124</td>\n",
       "      <td>60.897714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2454.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nlri_ann</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>146.069629</td>\n",
       "      <td>272.502888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>12632.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dups</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>7.523181</td>\n",
       "      <td>26.891729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>947.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin_0</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin_2</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>origin_changes</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_wd</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>12.660632</td>\n",
       "      <td>40.066748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1435.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_wd_spath</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>2.739868</td>\n",
       "      <td>19.699433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>601.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imp_wd_dpath</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>9.920764</td>\n",
       "      <td>32.776439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1435.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>as_path_max</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>13.678088</td>\n",
       "      <td>7.933387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>577.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_as_path_max</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>3.319712</td>\n",
       "      <td>1.490487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>14.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_avg</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>1.706129</td>\n",
       "      <td>1.450060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>2.313859</td>\n",
       "      <td>27.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_max</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>2.725028</td>\n",
       "      <td>2.872595</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>56.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_0</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_1</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>4.716080</td>\n",
       "      <td>23.710777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>964.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_2</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>2.666269</td>\n",
       "      <td>12.545233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1072.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_3</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>1.147731</td>\n",
       "      <td>5.397419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>379.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_4</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.506037</td>\n",
       "      <td>2.855452</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>256.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_5</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.304652</td>\n",
       "      <td>2.142376</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>169.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_dict_6</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.178592</td>\n",
       "      <td>1.369539</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>113.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_unique_dict_0</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>edit_distance_unique_dict_1</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>3.553023</td>\n",
       "      <td>19.727656</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>961.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_rare_ases</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>43.289094</td>\n",
       "      <td>39.834912</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1230.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rare_ases_avg</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.126945</td>\n",
       "      <td>0.116204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.044776</td>\n",
       "      <td>0.100629</td>\n",
       "      <td>0.165179</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nadas</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>2.897739</td>\n",
       "      <td>18.754418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1173.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flaps</th>\n",
       "      <td>86387.0</td>\n",
       "      <td>0.414646</td>\n",
       "      <td>4.036637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>311.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count        mean         std  min        25%  \\\n",
       "announcements                86387.0  182.400141  313.207475  0.0  24.000000   \n",
       "withdrawals                  86387.0   15.683124   60.897714  0.0   1.000000   \n",
       "nlri_ann                     86387.0  146.069629  272.502888  0.0  16.000000   \n",
       "dups                         86387.0    7.523181   26.891729  0.0   0.000000   \n",
       "origin_0                     86387.0    0.000000    0.000000  0.0   0.000000   \n",
       "origin_2                     86387.0    0.000000    0.000000  0.0   0.000000   \n",
       "origin_changes               86387.0    0.000000    0.000000  0.0   0.000000   \n",
       "imp_wd                       86387.0   12.660632   40.066748  0.0   1.000000   \n",
       "imp_wd_spath                 86387.0    2.739868   19.699433  0.0   0.000000   \n",
       "imp_wd_dpath                 86387.0    9.920764   32.776439  0.0   1.000000   \n",
       "as_path_max                  86387.0   13.678088    7.933387  0.0   8.000000   \n",
       "unique_as_path_max           86387.0    3.319712    1.490487  0.0   2.000000   \n",
       "edit_distance_avg            86387.0    1.706129    1.450060  0.0   1.000000   \n",
       "edit_distance_max            86387.0    2.725028    2.872595  0.0   1.000000   \n",
       "edit_distance_dict_0         86387.0    0.000000    0.000000  0.0   0.000000   \n",
       "edit_distance_dict_1         86387.0    4.716080   23.710777  0.0   0.000000   \n",
       "edit_distance_dict_2         86387.0    2.666269   12.545233  0.0   0.000000   \n",
       "edit_distance_dict_3         86387.0    1.147731    5.397419  0.0   0.000000   \n",
       "edit_distance_dict_4         86387.0    0.506037    2.855452  0.0   0.000000   \n",
       "edit_distance_dict_5         86387.0    0.304652    2.142376  0.0   0.000000   \n",
       "edit_distance_dict_6         86387.0    0.178592    1.369539  0.0   0.000000   \n",
       "edit_distance_unique_dict_0  86387.0    0.000000    0.000000  0.0   0.000000   \n",
       "edit_distance_unique_dict_1  86387.0    3.553023   19.727656  0.0   0.000000   \n",
       "number_rare_ases             86387.0   43.289094   39.834912  0.0  16.000000   \n",
       "rare_ases_avg                86387.0    0.126945    0.116204  0.0   0.044776   \n",
       "nadas                        86387.0    2.897739   18.754418  0.0   0.000000   \n",
       "flaps                        86387.0    0.414646    4.036637  0.0   0.000000   \n",
       "\n",
       "                                   50%         75%       max  \n",
       "announcements                61.000000  179.000000  12661.00  \n",
       "withdrawals                   3.000000    9.000000   2454.00  \n",
       "nlri_ann                     43.000000  128.000000  12632.00  \n",
       "dups                          1.000000    4.000000    947.00  \n",
       "origin_0                      0.000000    0.000000      0.00  \n",
       "origin_2                      0.000000    0.000000      0.00  \n",
       "origin_changes                0.000000    0.000000      0.00  \n",
       "imp_wd                        3.000000    8.000000   1435.00  \n",
       "imp_wd_spath                  0.000000    0.000000    601.00  \n",
       "imp_wd_dpath                  3.000000    7.000000   1435.00  \n",
       "as_path_max                  11.000000   16.000000    577.00  \n",
       "unique_as_path_max            3.000000    4.000000     14.00  \n",
       "edit_distance_avg             1.500000    2.313859     27.75  \n",
       "edit_distance_max             2.000000    4.000000     56.00  \n",
       "edit_distance_dict_0          0.000000    0.000000      0.00  \n",
       "edit_distance_dict_1          1.000000    3.000000    964.00  \n",
       "edit_distance_dict_2          0.000000    2.000000   1072.00  \n",
       "edit_distance_dict_3          0.000000    1.000000    379.00  \n",
       "edit_distance_dict_4          0.000000    0.000000    256.00  \n",
       "edit_distance_dict_5          0.000000    0.000000    169.00  \n",
       "edit_distance_dict_6          0.000000    0.000000    113.00  \n",
       "edit_distance_unique_dict_0   0.000000    0.000000      0.00  \n",
       "edit_distance_unique_dict_1   1.000000    2.000000    961.00  \n",
       "number_rare_ases             27.000000   62.000000   1230.00  \n",
       "rare_ases_avg                 0.100629    0.165179      1.00  \n",
       "nadas                         0.000000    1.000000   1173.00  \n",
       "flaps                         0.000000    0.000000    311.00  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display statistics of selected features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Feature Statistics (Normal Traffic)\")\n",
    "print(\"=\"*60)\n",
    "df_features.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Normalization and Sequence Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Normalizing data with StandardScaler...\n",
      "============================================================\n",
      "Normalized data shape: (86387, 27)\n",
      "Normalized data range: [-2.2273, 89.4763]\n",
      "Normalized data mean: 0.000000\n",
      "Normalized data std: 0.902671\n"
     ]
    }
   ],
   "source": [
    "# Normalize the data using StandardScaler\n",
    "print(\"=\"*60)\n",
    "print(\"Normalizing data with StandardScaler...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(df_features.values)\n",
    "\n",
    "print(f\"Normalized data shape: {data_normalized.shape}\")\n",
    "print(f\"Normalized data range: [{data_normalized.min():.4f}, {data_normalized.max():.4f}]\")\n",
    "print(f\"Normalized data mean: {data_normalized.mean():.6f}\")\n",
    "print(f\"Normalized data std: {data_normalized.std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Building sequences (T=30, stride=1)...\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences shape: (86358, 30, 27)\n",
      "  - Number of samples: 86358\n",
      "  - Sequence length (T): 30\n",
      "  - Number of features: 27\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, seq_length, stride=1):\n",
    "    \"\"\"\n",
    "    Create sliding window sequences from time-series data.\n",
    "    \n",
    "    Args:\n",
    "        data: numpy array of shape (num_timesteps, num_features)\n",
    "        seq_length: length of each sequence (T)\n",
    "        stride: step size between sequences\n",
    "        \n",
    "    Returns:\n",
    "        sequences: numpy array of shape (num_samples, seq_length, num_features)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - seq_length + 1, stride):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Create sequences\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Building sequences (T={CONFIG['sequence_length']}, stride={CONFIG['stride']})...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sequences = create_sequences(data_normalized, CONFIG['sequence_length'], CONFIG['stride'])\n",
    "print(f\"Sequences shape: {sequences.shape}\")\n",
    "print(f\"  - Number of samples: {sequences.shape[0]}\")\n",
    "print(f\"  - Sequence length (T): {sequences.shape[1]}\")\n",
    "print(f\"  - Number of features: {sequences.shape[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Train/Test/Validation Split\n",
      "============================================================\n",
      "Training set: (62177, 30, 27)\n",
      "Validation set: (6909, 30, 27)\n",
      "Test set: (17272, 30, 27)\n"
     ]
    }
   ],
   "source": [
    "# Train/Test/Validation Split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Train/Test/Validation Split\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# First split: train+val vs test\n",
    "X_train_val, X_test = train_test_split(\n",
    "    sequences, \n",
    "    test_size=CONFIG['test_size'], \n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Second split: train vs val\n",
    "X_train, X_val = train_test_split(\n",
    "    X_train_val, \n",
    "    test_size=CONFIG['validation_size'], \n",
    "    random_state=SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 971\n",
      "Validation batches: 108\n",
      "Test batches: 270\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors and create DataLoaders\n",
    "train_tensor = torch.FloatTensor(X_train)\n",
    "val_tensor = torch.FloatTensor(X_val)\n",
    "test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "train_dataset = TensorDataset(train_tensor)\n",
    "val_dataset = TensorDataset(val_tensor)\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: GAN Model Definitions\n",
    "\n",
    "### 5.1 TimeGAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeGAN model defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# TimeGAN Components\n",
    "# Based on: \"Time-series Generative Adversarial Networks\" (Yoon et al., NeurIPS 2019)\n",
    "# ============================================================================\n",
    "\n",
    "class TimeGAN_Embedder(nn.Module):\n",
    "    \"\"\"Embedding network: maps original feature space to latent space.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h, _ = self.rnn(x)\n",
    "        h = self.fc(h)\n",
    "        return self.activation(h)\n",
    "\n",
    "\n",
    "class TimeGAN_Recovery(nn.Module):\n",
    "    \"\"\"Recovery network: maps from latent space back to original feature space.\"\"\"\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        h_hat, _ = self.rnn(h)\n",
    "        x_hat = self.fc(h_hat)\n",
    "        return x_hat\n",
    "\n",
    "\n",
    "class TimeGAN_Generator(nn.Module):\n",
    "    \"\"\"Generator: generates synthetic latent representations from noise.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(latent_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, z):\n",
    "        e_hat, _ = self.rnn(z)\n",
    "        e_hat = self.fc(e_hat)\n",
    "        return self.activation(e_hat)\n",
    "\n",
    "\n",
    "class TimeGAN_Supervisor(nn.Module):\n",
    "    \"\"\"Supervisor: captures temporal dynamics in latent space.\"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers - 1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, h):\n",
    "        s, _ = self.rnn(h)\n",
    "        s = self.fc(s)\n",
    "        return self.activation(s)\n",
    "\n",
    "\n",
    "class TimeGAN_Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator: distinguishes real from synthetic sequences.\"\"\"\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, h):\n",
    "        d_out, _ = self.rnn(h)\n",
    "        y_hat = self.fc(d_out)\n",
    "        return y_hat\n",
    "\n",
    "\n",
    "class TimeGAN:\n",
    "    \"\"\"Complete TimeGAN model.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, device):\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Initialize networks\n",
    "        self.embedder = TimeGAN_Embedder(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.recovery = TimeGAN_Recovery(hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.generator = TimeGAN_Generator(latent_dim, hidden_dim, num_layers).to(device)\n",
    "        self.supervisor = TimeGAN_Supervisor(hidden_dim, num_layers).to(device)\n",
    "        self.discriminator = TimeGAN_Discriminator(hidden_dim, num_layers).to(device)\n",
    "        \n",
    "        # Loss function\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def _get_optimizers(self, lr):\n",
    "        \"\"\"Create optimizers for each training phase.\"\"\"\n",
    "        # Embedding optimizer\n",
    "        e_params = list(self.embedder.parameters()) + list(self.recovery.parameters())\n",
    "        self.opt_e = optim.Adam(e_params, lr=lr)\n",
    "        \n",
    "        # Supervised optimizer\n",
    "        self.opt_s = optim.Adam(self.supervisor.parameters(), lr=lr)\n",
    "        \n",
    "        # Generator optimizer\n",
    "        g_params = list(self.generator.parameters()) + list(self.supervisor.parameters())\n",
    "        self.opt_g = optim.Adam(g_params, lr=lr)\n",
    "        \n",
    "        # Discriminator optimizer\n",
    "        self.opt_d = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
    "        \n",
    "    def train(self, train_loader, epochs, lr=0.001, gamma=1.0):\n",
    "        \"\"\"Train TimeGAN in four phases.\"\"\"\n",
    "        self._get_optimizers(lr)\n",
    "        history = {'e_loss': [], 's_loss': [], 'g_loss': [], 'd_loss': []}\n",
    "        \n",
    "        # Phase 1: Embedding network training\n",
    "        print(\"Phase 1: Training Embedding Network...\")\n",
    "        for epoch in tqdm(range(epochs // 4)):\n",
    "            e_losses = []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0].to(self.device)\n",
    "                \n",
    "                self.opt_e.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                x_tilde = self.recovery(h)\n",
    "                e_loss = self.mse_loss(x, x_tilde)\n",
    "                e_loss.backward()\n",
    "                self.opt_e.step()\n",
    "                e_losses.append(e_loss.item())\n",
    "            history['e_loss'].append(np.mean(e_losses))\n",
    "        \n",
    "        # Phase 2: Supervised network training\n",
    "        print(\"Phase 2: Training Supervised Network...\")\n",
    "        for epoch in tqdm(range(epochs // 4)):\n",
    "            s_losses = []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0].to(self.device)\n",
    "                \n",
    "                self.opt_s.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                h_hat_supervise = self.supervisor(h)\n",
    "                s_loss = self.mse_loss(h[:, 1:, :], h_hat_supervise[:, :-1, :])\n",
    "                s_loss.backward()\n",
    "                self.opt_s.step()\n",
    "                s_losses.append(s_loss.item())\n",
    "            history['s_loss'].append(np.mean(s_losses))\n",
    "        \n",
    "        # Phase 3 & 4: Joint training\n",
    "        print(\"Phase 3 & 4: Joint Training...\")\n",
    "        for epoch in tqdm(range(epochs // 2)):\n",
    "            g_losses, d_losses = [], []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0].to(self.device)\n",
    "                batch_size, seq_len, _ = x.shape\n",
    "                \n",
    "                # Random noise\n",
    "                z = torch.randn(batch_size, seq_len, self.latent_dim).to(self.device)\n",
    "                \n",
    "                # Train Generator (2 steps per discriminator step)\n",
    "                for _ in range(2):\n",
    "                    self.opt_g.zero_grad()\n",
    "                    \n",
    "                    # Real embeddings\n",
    "                    h = self.embedder(x)\n",
    "                    h_hat_supervise = self.supervisor(h)\n",
    "                    \n",
    "                    # Synthetic embeddings\n",
    "                    e_hat = self.generator(z)\n",
    "                    h_hat = self.supervisor(e_hat)\n",
    "                    \n",
    "                    # Synthetic data recovery\n",
    "                    x_hat = self.recovery(h_hat)\n",
    "                    \n",
    "                    # Discriminator output for fake\n",
    "                    y_fake = self.discriminator(h_hat)\n",
    "                    y_fake_e = self.discriminator(e_hat)\n",
    "                    \n",
    "                    # Generator losses\n",
    "                    g_loss_u = self.bce_loss(y_fake, torch.ones_like(y_fake))\n",
    "                    g_loss_u_e = self.bce_loss(y_fake_e, torch.ones_like(y_fake_e))\n",
    "                    g_loss_s = self.mse_loss(h[:, 1:, :], h_hat_supervise[:, :-1, :])\n",
    "                    \n",
    "                    # Moment matching\n",
    "                    g_loss_v1 = torch.mean(torch.abs(torch.sqrt(x_hat.var(dim=0, unbiased=False) + 1e-6) - \n",
    "                                                     torch.sqrt(x.var(dim=0, unbiased=False) + 1e-6)))\n",
    "                    g_loss_v2 = torch.mean(torch.abs(x_hat.mean(dim=0) - x.mean(dim=0)))\n",
    "                    \n",
    "                    g_loss = g_loss_u + g_loss_u_e + gamma * g_loss_s + 100 * (g_loss_v1 + g_loss_v2)\n",
    "                    g_loss.backward()\n",
    "                    self.opt_g.step()\n",
    "                \n",
    "                # Train Discriminator\n",
    "                self.opt_d.zero_grad()\n",
    "                \n",
    "                h = self.embedder(x)\n",
    "                e_hat = self.generator(z)\n",
    "                h_hat = self.supervisor(e_hat)\n",
    "                \n",
    "                y_real = self.discriminator(h)\n",
    "                y_fake = self.discriminator(h_hat)\n",
    "                y_fake_e = self.discriminator(e_hat)\n",
    "                \n",
    "                d_loss_real = self.bce_loss(y_real, torch.ones_like(y_real))\n",
    "                d_loss_fake = self.bce_loss(y_fake, torch.zeros_like(y_fake))\n",
    "                d_loss_fake_e = self.bce_loss(y_fake_e, torch.zeros_like(y_fake_e))\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_fake + d_loss_fake_e\n",
    "                \n",
    "                if d_loss > 0.15:  # Only update if discriminator is not too strong\n",
    "                    d_loss.backward()\n",
    "                    self.opt_d.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples, seq_len):\n",
    "        \"\"\"Generate synthetic sequences.\"\"\"\n",
    "        self.generator.eval()\n",
    "        self.supervisor.eval()\n",
    "        self.recovery.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, seq_len, self.latent_dim).to(self.device)\n",
    "            e_hat = self.generator(z)\n",
    "            h_hat = self.supervisor(e_hat)\n",
    "            x_hat = self.recovery(h_hat)\n",
    "        \n",
    "        return x_hat.cpu().numpy()\n",
    "\n",
    "print(\"TimeGAN model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LSTM-GAN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM-GAN model defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LSTM-GAN Components\n",
    "# ============================================================================\n",
    "\n",
    "class LSTMGAN_Generator(nn.Module):\n",
    "    \"\"\"LSTM-based Generator for time-series.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Tanh()  # Output in [-1, 1] range for normalized data\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        # z: (batch, seq_len, latent_dim)\n",
    "        lstm_out, _ = self.lstm(z)\n",
    "        output = self.fc(lstm_out)\n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTMGAN_Discriminator(nn.Module):\n",
    "    \"\"\"LSTM-based Discriminator for time-series.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Use the last timestep for classification\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        return output\n",
    "\n",
    "\n",
    "class LSTMGAN:\n",
    "    \"\"\"Complete LSTM-GAN model.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, seq_len, device):\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.generator = LSTMGAN_Generator(\n",
    "            latent_dim, hidden_dim, input_dim, num_layers, seq_len\n",
    "        ).to(device)\n",
    "        \n",
    "        self.discriminator = LSTMGAN_Discriminator(\n",
    "            input_dim, hidden_dim, num_layers\n",
    "        ).to(device)\n",
    "        \n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def train(self, train_loader, epochs, lr=0.0002, beta1=0.5, beta2=0.999):\n",
    "        \"\"\"Train LSTM-GAN.\"\"\"\n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': []}\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            g_losses, d_losses = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                real_data = batch[0].to(self.device)\n",
    "                batch_size = real_data.size(0)\n",
    "                \n",
    "                # Labels\n",
    "                real_labels = torch.ones(batch_size, 1).to(self.device)\n",
    "                fake_labels = torch.zeros(batch_size, 1).to(self.device)\n",
    "                \n",
    "                # ---------------------\n",
    "                # Train Discriminator\n",
    "                # ---------------------\n",
    "                opt_d.zero_grad()\n",
    "                \n",
    "                # Real samples\n",
    "                real_output = self.discriminator(real_data)\n",
    "                d_loss_real = self.bce_loss(real_output, real_labels)\n",
    "                \n",
    "                # Fake samples\n",
    "                z = torch.randn(batch_size, self.seq_len, self.latent_dim).to(self.device)\n",
    "                fake_data = self.generator(z)\n",
    "                fake_output = self.discriminator(fake_data.detach())\n",
    "                d_loss_fake = self.bce_loss(fake_output, fake_labels)\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "                d_loss.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                # ---------------------\n",
    "                # Train Generator\n",
    "                # ---------------------\n",
    "                opt_g.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size, self.seq_len, self.latent_dim).to(self.device)\n",
    "                fake_data = self.generator(z)\n",
    "                fake_output = self.discriminator(fake_data)\n",
    "                g_loss = self.bce_loss(fake_output, real_labels)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] D_loss: {history['d_loss'][-1]:.4f}, G_loss: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate synthetic sequences.\"\"\"\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.seq_len, self.latent_dim).to(self.device)\n",
    "            fake_data = self.generator(z)\n",
    "        return fake_data.cpu().numpy()\n",
    "\n",
    "print(\"LSTM-GAN model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 DoppelGANger Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoppelGANger model defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DoppelGANger Components\n",
    "# Based on: \"Using GANs for Sharing Networked Time Series Data\" (Lin et al., IMC 2020)\n",
    "# Simplified implementation focused on temporal features\n",
    "# ============================================================================\n",
    "\n",
    "class DoppelGANger_AttrGenerator(nn.Module):\n",
    "    \"\"\"Attribute generator for metadata/static features.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, attr_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, attr_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "class DoppelGANger_FeatureGenerator(nn.Module):\n",
    "    \"\"\"Feature generator for time-series with attention mechanism.\"\"\"\n",
    "    def __init__(self, latent_dim, hidden_dim, feature_dim, num_layers, seq_len):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Initial hidden state generator\n",
    "        self.init_hidden = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim * num_layers),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # LSTM for temporal generation\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Self-attention for capturing long-range dependencies\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4, batch_first=True)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, feature_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "    def forward(self, z_seq, z_attr=None):\n",
    "        batch_size = z_seq.size(0)\n",
    "        \n",
    "        # Generate initial hidden state\n",
    "        if z_attr is not None:\n",
    "            h0 = self.init_hidden(z_attr)\n",
    "            h0 = h0.view(self.num_layers, batch_size, self.hidden_dim)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            lstm_out, _ = self.lstm(z_seq, (h0, c0))\n",
    "        else:\n",
    "            lstm_out, _ = self.lstm(z_seq)\n",
    "        \n",
    "        # Apply self-attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Residual connection\n",
    "        combined = lstm_out + attn_out\n",
    "        \n",
    "        # Output projection\n",
    "        output = self.output(combined)\n",
    "        return output\n",
    "\n",
    "\n",
    "class DoppelGANger_Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator with auxiliary classifier for attributes.\"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Feature encoder\n",
    "        self.lstm = nn.LSTM(feature_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention pooling\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Real/fake classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, feature_dim)\n",
    "        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        # Attention-weighted pooling\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context = torch.sum(attn_weights * lstm_out, dim=1)  # (batch, hidden_dim * 2)\n",
    "        \n",
    "        # Classification\n",
    "        validity = self.classifier(context)\n",
    "        return validity\n",
    "\n",
    "\n",
    "class DoppelGANger:\n",
    "    \"\"\"Complete DoppelGANger model.\"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, latent_dim, num_layers, seq_len, device):\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.feature_gen = DoppelGANger_FeatureGenerator(\n",
    "            latent_dim, hidden_dim, feature_dim, num_layers, seq_len\n",
    "        ).to(device)\n",
    "        \n",
    "        self.attr_gen = DoppelGANger_AttrGenerator(\n",
    "            latent_dim, hidden_dim, latent_dim  # Attr dim = latent dim for conditioning\n",
    "        ).to(device)\n",
    "        \n",
    "        self.discriminator = DoppelGANger_Discriminator(\n",
    "            feature_dim, hidden_dim, num_layers\n",
    "        ).to(device)\n",
    "        \n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def train(self, train_loader, epochs, lr=0.0002, beta1=0.5, beta2=0.999, n_critic=5):\n",
    "        \"\"\"Train DoppelGANger with WGAN-style training.\"\"\"\n",
    "        g_params = list(self.feature_gen.parameters()) + list(self.attr_gen.parameters())\n",
    "        opt_g = optim.Adam(g_params, lr=lr, betas=(beta1, beta2))\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': []}\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            g_losses, d_losses = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                real_data = batch[0].to(self.device)\n",
    "                batch_size = real_data.size(0)\n",
    "                \n",
    "                real_labels = torch.ones(batch_size, 1).to(self.device)\n",
    "                fake_labels = torch.zeros(batch_size, 1).to(self.device)\n",
    "                \n",
    "                # ---------------------\n",
    "                # Train Discriminator\n",
    "                # ---------------------\n",
    "                for _ in range(n_critic):\n",
    "                    opt_d.zero_grad()\n",
    "                    \n",
    "                    # Real samples\n",
    "                    real_output = self.discriminator(real_data)\n",
    "                    d_loss_real = self.bce_loss(real_output, real_labels)\n",
    "                    \n",
    "                    # Fake samples\n",
    "                    z_attr = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "                    z_seq = torch.randn(batch_size, self.seq_len, self.latent_dim).to(self.device)\n",
    "                    \n",
    "                    attr_fake = self.attr_gen(z_attr)\n",
    "                    fake_data = self.feature_gen(z_seq, attr_fake)\n",
    "                    \n",
    "                    fake_output = self.discriminator(fake_data.detach())\n",
    "                    d_loss_fake = self.bce_loss(fake_output, fake_labels)\n",
    "                    \n",
    "                    d_loss = d_loss_real + d_loss_fake\n",
    "                    d_loss.backward()\n",
    "                    opt_d.step()\n",
    "                \n",
    "                # ---------------------\n",
    "                # Train Generator\n",
    "                # ---------------------\n",
    "                opt_g.zero_grad()\n",
    "                \n",
    "                z_attr = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
    "                z_seq = torch.randn(batch_size, self.seq_len, self.latent_dim).to(self.device)\n",
    "                \n",
    "                attr_fake = self.attr_gen(z_attr)\n",
    "                fake_data = self.feature_gen(z_seq, attr_fake)\n",
    "                \n",
    "                fake_output = self.discriminator(fake_data)\n",
    "                g_loss = self.bce_loss(fake_output, real_labels)\n",
    "                \n",
    "                # Add feature matching loss\n",
    "                real_mean = real_data.mean(dim=[0, 1])\n",
    "                fake_mean = fake_data.mean(dim=[0, 1])\n",
    "                g_loss += 10 * self.mse_loss(fake_mean, real_mean)\n",
    "                \n",
    "                g_loss.backward()\n",
    "                opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}] D_loss: {history['d_loss'][-1]:.4f}, G_loss: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate synthetic sequences.\"\"\"\n",
    "        self.feature_gen.eval()\n",
    "        self.attr_gen.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z_attr = torch.randn(n_samples, self.latent_dim).to(self.device)\n",
    "            z_seq = torch.randn(n_samples, self.seq_len, self.latent_dim).to(self.device)\n",
    "            \n",
    "            attr_fake = self.attr_gen(z_attr)\n",
    "            fake_data = self.feature_gen(z_seq, attr_fake)\n",
    "        \n",
    "        return fake_data.cpu().numpy()\n",
    "\n",
    "print(\"DoppelGANger model defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Evaluation Metrics for Time-Series GANs\n",
    "# ============================================================================\n",
    "\n",
    "class TimeSeriesEvaluator:\n",
    "    \"\"\"Comprehensive evaluation metrics for synthetic time-series.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "    def compute_all_metrics(self, real_data, synthetic_data):\n",
    "        \"\"\"\n",
    "        Compute all evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            real_data: numpy array (n_samples, seq_len, n_features)\n",
    "            synthetic_data: numpy array (n_samples, seq_len, n_features)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Distribution metrics\n",
    "        metrics['distribution'] = self._compute_distribution_metrics(real_data, synthetic_data)\n",
    "        \n",
    "        # Temporal metrics\n",
    "        metrics['temporal'] = self._compute_temporal_metrics(real_data, synthetic_data)\n",
    "        \n",
    "        # Correlation metrics\n",
    "        metrics['correlation'] = self._compute_correlation_metrics(real_data, synthetic_data)\n",
    "        \n",
    "        # Compute overall score\n",
    "        metrics['overall_score'] = self._compute_overall_score(metrics)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_distribution_metrics(self, real_data, synthetic_data):\n",
    "        \"\"\"Compare marginal distributions using various metrics.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Flatten temporal dimension for marginal comparison\n",
    "        real_flat = real_data.reshape(-1, real_data.shape[-1])\n",
    "        syn_flat = synthetic_data.reshape(-1, synthetic_data.shape[-1])\n",
    "        \n",
    "        # KS statistic for each feature\n",
    "        ks_stats = []\n",
    "        wasserstein_dists = []\n",
    "        \n",
    "        for i in range(real_flat.shape[1]):\n",
    "            ks_stat, _ = stats.ks_2samp(real_flat[:, i], syn_flat[:, i])\n",
    "            ks_stats.append(ks_stat)\n",
    "            \n",
    "            # Wasserstein distance\n",
    "            wd = stats.wasserstein_distance(real_flat[:, i], syn_flat[:, i])\n",
    "            wasserstein_dists.append(wd)\n",
    "        \n",
    "        metrics['ks_stats'] = dict(zip(self.feature_names, ks_stats))\n",
    "        metrics['ks_mean'] = np.mean(ks_stats)\n",
    "        metrics['wasserstein_dists'] = dict(zip(self.feature_names, wasserstein_dists))\n",
    "        metrics['wasserstein_mean'] = np.mean(wasserstein_dists)\n",
    "        \n",
    "        # Mean and std comparison\n",
    "        real_mean = real_flat.mean(axis=0)\n",
    "        syn_mean = syn_flat.mean(axis=0)\n",
    "        real_std = real_flat.std(axis=0)\n",
    "        syn_std = syn_flat.std(axis=0)\n",
    "        \n",
    "        metrics['mean_error'] = np.mean(np.abs(real_mean - syn_mean))\n",
    "        metrics['std_error'] = np.mean(np.abs(real_std - syn_std))\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_temporal_metrics(self, real_data, synthetic_data):\n",
    "        \"\"\"Compare temporal structure.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Autocorrelation comparison\n",
    "        acf_errors = []\n",
    "        max_lag = min(20, real_data.shape[1] - 1)\n",
    "        \n",
    "        for i in range(real_data.shape[-1]):\n",
    "            # Average ACF across samples\n",
    "            real_acfs = []\n",
    "            syn_acfs = []\n",
    "            \n",
    "            for j in range(min(100, real_data.shape[0])):\n",
    "                try:\n",
    "                    r_acf = acf(real_data[j, :, i], nlags=max_lag, fft=True)\n",
    "                    real_acfs.append(r_acf)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "            for j in range(min(100, synthetic_data.shape[0])):\n",
    "                try:\n",
    "                    s_acf = acf(synthetic_data[j, :, i], nlags=max_lag, fft=True)\n",
    "                    syn_acfs.append(s_acf)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if real_acfs and syn_acfs:\n",
    "                real_acf_mean = np.mean(real_acfs, axis=0)\n",
    "                syn_acf_mean = np.mean(syn_acfs, axis=0)\n",
    "                acf_error = np.mean(np.abs(real_acf_mean - syn_acf_mean))\n",
    "                acf_errors.append(acf_error)\n",
    "        \n",
    "        metrics['acf_error'] = np.mean(acf_errors) if acf_errors else float('inf')\n",
    "        \n",
    "        # Burstiness comparison\n",
    "        real_burst = self._compute_burstiness(real_data)\n",
    "        syn_burst = self._compute_burstiness(synthetic_data)\n",
    "        metrics['burstiness_error'] = np.mean(np.abs(real_burst - syn_burst))\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_burstiness(self, data):\n",
    "        \"\"\"Compute burstiness measure for each feature.\"\"\"\n",
    "        # Burstiness = (std - mean) / (std + mean)\n",
    "        flat = data.reshape(-1, data.shape[-1])\n",
    "        mean = flat.mean(axis=0)\n",
    "        std = flat.std(axis=0)\n",
    "        burstiness = (std - mean) / (std + mean + 1e-10)\n",
    "        return burstiness\n",
    "    \n",
    "    def _compute_correlation_metrics(self, real_data, synthetic_data):\n",
    "        \"\"\"Compare feature correlations.\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # Flatten for correlation matrix\n",
    "        real_flat = real_data.reshape(-1, real_data.shape[-1])\n",
    "        syn_flat = synthetic_data.reshape(-1, synthetic_data.shape[-1])\n",
    "        \n",
    "        # Feature correlation matrices\n",
    "        real_corr = np.corrcoef(real_flat.T)\n",
    "        syn_corr = np.corrcoef(syn_flat.T)\n",
    "        \n",
    "        # Handle NaN correlations\n",
    "        real_corr = np.nan_to_num(real_corr)\n",
    "        syn_corr = np.nan_to_num(syn_corr)\n",
    "        \n",
    "        # Frobenius norm of difference\n",
    "        metrics['corr_matrix_error'] = np.linalg.norm(real_corr - syn_corr, 'fro')\n",
    "        metrics['corr_matrix_error_normalized'] = metrics['corr_matrix_error'] / np.sqrt(real_corr.size)\n",
    "        \n",
    "        # Cross-correlation over time\n",
    "        cross_corr_errors = []\n",
    "        for i in range(real_data.shape[-1]):\n",
    "            for j in range(i + 1, real_data.shape[-1]):\n",
    "                real_xcorr = np.corrcoef(real_flat[:, i], real_flat[:, j])[0, 1]\n",
    "                syn_xcorr = np.corrcoef(syn_flat[:, i], syn_flat[:, j])[0, 1]\n",
    "                if not np.isnan(real_xcorr) and not np.isnan(syn_xcorr):\n",
    "                    cross_corr_errors.append(abs(real_xcorr - syn_xcorr))\n",
    "        \n",
    "        metrics['cross_corr_error'] = np.mean(cross_corr_errors) if cross_corr_errors else float('inf')\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_overall_score(self, metrics):\n",
    "        \"\"\"Compute overall quality score (lower is better).\"\"\"\n",
    "        # Weighted combination of metrics\n",
    "        score = (\n",
    "            0.3 * metrics['distribution']['ks_mean'] +\n",
    "            0.2 * metrics['distribution']['wasserstein_mean'] +\n",
    "            0.2 * metrics['temporal']['acf_error'] +\n",
    "            0.15 * metrics['temporal']['burstiness_error'] +\n",
    "            0.15 * metrics['correlation']['corr_matrix_error_normalized']\n",
    "        )\n",
    "        return score\n",
    "\n",
    "print(\"Evaluation metrics defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization functions defined.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Visualization Functions\n",
    "# ============================================================================\n",
    "\n",
    "def plot_training_history(histories, model_names, save_path=None):\n",
    "    \"\"\"Plot training histories for all models.\"\"\"\n",
    "    fig, axes = plt.subplots(len(model_names), 2, figsize=(14, 4*len(model_names)))\n",
    "    \n",
    "    for idx, (name, history) in enumerate(zip(model_names, histories)):\n",
    "        ax_g = axes[idx, 0] if len(model_names) > 1 else axes[0]\n",
    "        ax_d = axes[idx, 1] if len(model_names) > 1 else axes[1]\n",
    "        \n",
    "        ax_g.plot(history['g_loss'], label='Generator Loss', color='blue')\n",
    "        ax_g.set_title(f'{name} - Generator Loss')\n",
    "        ax_g.set_xlabel('Epoch')\n",
    "        ax_g.set_ylabel('Loss')\n",
    "        ax_g.legend()\n",
    "        ax_g.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax_d.plot(history['d_loss'], label='Discriminator Loss', color='red')\n",
    "        ax_d.set_title(f'{name} - Discriminator Loss')\n",
    "        ax_d.set_xlabel('Epoch')\n",
    "        ax_d.set_ylabel('Loss')\n",
    "        ax_d.legend()\n",
    "        ax_d.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_distribution_comparison(real_data, synthetic_data, feature_names, model_name, n_features=6, save_path=None):\n",
    "    \"\"\"Plot distribution comparison histograms.\"\"\"\n",
    "    real_flat = real_data.reshape(-1, real_data.shape[-1])\n",
    "    syn_flat = synthetic_data.reshape(-1, synthetic_data.shape[-1])\n",
    "    \n",
    "    n_cols = 3\n",
    "    n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(min(n_features, len(feature_names))):\n",
    "        ax = axes[i]\n",
    "        ax.hist(real_flat[:, i], bins=50, alpha=0.5, label='Real', density=True, color='blue')\n",
    "        ax.hist(syn_flat[:, i], bins=50, alpha=0.5, label='Synthetic', density=True, color='red')\n",
    "        ax.set_title(f'{feature_names[i]}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Distribution Comparison', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_comparison(real_data, synthetic_data, feature_names, model_name, save_path=None):\n",
    "    \"\"\"Plot correlation matrix comparison.\"\"\"\n",
    "    real_flat = real_data.reshape(-1, real_data.shape[-1])\n",
    "    syn_flat = synthetic_data.reshape(-1, synthetic_data.shape[-1])\n",
    "    \n",
    "    real_corr = np.corrcoef(real_flat.T)\n",
    "    syn_corr = np.corrcoef(syn_flat.T)\n",
    "    \n",
    "    # Handle NaN\n",
    "    real_corr = np.nan_to_num(real_corr)\n",
    "    syn_corr = np.nan_to_num(syn_corr)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Truncate feature names for display\n",
    "    short_names = [f[:10] for f in feature_names]\n",
    "    \n",
    "    # Real correlation\n",
    "    sns.heatmap(real_corr, ax=axes[0], cmap='coolwarm', center=0, \n",
    "                xticklabels=short_names, yticklabels=short_names, vmin=-1, vmax=1)\n",
    "    axes[0].set_title('Real Data Correlation')\n",
    "    axes[0].tick_params(axis='x', rotation=90, labelsize=6)\n",
    "    axes[0].tick_params(axis='y', rotation=0, labelsize=6)\n",
    "    \n",
    "    # Synthetic correlation\n",
    "    sns.heatmap(syn_corr, ax=axes[1], cmap='coolwarm', center=0,\n",
    "                xticklabels=short_names, yticklabels=short_names, vmin=-1, vmax=1)\n",
    "    axes[1].set_title(f'{model_name} Synthetic Correlation')\n",
    "    axes[1].tick_params(axis='x', rotation=90, labelsize=6)\n",
    "    axes[1].tick_params(axis='y', rotation=0, labelsize=6)\n",
    "    \n",
    "    # Difference\n",
    "    diff = real_corr - syn_corr\n",
    "    sns.heatmap(diff, ax=axes[2], cmap='RdBu', center=0,\n",
    "                xticklabels=short_names, yticklabels=short_names, vmin=-1, vmax=1)\n",
    "    axes[2].set_title('Difference (Real - Synthetic)')\n",
    "    axes[2].tick_params(axis='x', rotation=90, labelsize=6)\n",
    "    axes[2].tick_params(axis='y', rotation=0, labelsize=6)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_temporal_comparison(real_data, synthetic_data, feature_names, model_name, n_features=4, save_path=None):\n",
    "    \"\"\"Plot temporal patterns comparison.\"\"\"\n",
    "    fig, axes = plt.subplots(n_features, 2, figsize=(14, 3*n_features))\n",
    "    \n",
    "    for i in range(min(n_features, len(feature_names))):\n",
    "        # Sample sequences\n",
    "        real_sample = real_data[np.random.randint(0, real_data.shape[0]), :, i]\n",
    "        syn_sample = synthetic_data[np.random.randint(0, synthetic_data.shape[0]), :, i]\n",
    "        \n",
    "        # Time series plot\n",
    "        axes[i, 0].plot(real_sample, label='Real', color='blue', alpha=0.7)\n",
    "        axes[i, 0].plot(syn_sample, label='Synthetic', color='red', alpha=0.7)\n",
    "        axes[i, 0].set_title(f'{feature_names[i]} - Sample Sequence')\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Autocorrelation plot\n",
    "        max_lag = min(20, real_data.shape[1] - 1)\n",
    "        try:\n",
    "            real_acfs = [acf(real_data[j, :, i], nlags=max_lag, fft=True) \n",
    "                         for j in range(min(50, real_data.shape[0]))]\n",
    "            syn_acfs = [acf(synthetic_data[j, :, i], nlags=max_lag, fft=True) \n",
    "                        for j in range(min(50, synthetic_data.shape[0]))]\n",
    "            \n",
    "            real_acf_mean = np.mean(real_acfs, axis=0)\n",
    "            syn_acf_mean = np.mean(syn_acfs, axis=0)\n",
    "            \n",
    "            axes[i, 1].plot(real_acf_mean, label='Real', color='blue')\n",
    "            axes[i, 1].plot(syn_acf_mean, label='Synthetic', color='red')\n",
    "            axes[i, 1].set_title(f'{feature_names[i]} - Autocorrelation')\n",
    "            axes[i, 1].legend()\n",
    "            axes[i, 1].grid(True, alpha=0.3)\n",
    "        except:\n",
    "            axes[i, 1].text(0.5, 0.5, 'ACF computation failed', ha='center', va='center')\n",
    "    \n",
    "    plt.suptitle(f'{model_name} - Temporal Comparison', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_model_comparison(all_metrics, model_names, save_path=None):\n",
    "    \"\"\"Plot comparison of all models.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Extract metrics\n",
    "    ks_means = [m['distribution']['ks_mean'] for m in all_metrics]\n",
    "    wasserstein_means = [m['distribution']['wasserstein_mean'] for m in all_metrics]\n",
    "    acf_errors = [m['temporal']['acf_error'] for m in all_metrics]\n",
    "    corr_errors = [m['correlation']['corr_matrix_error_normalized'] for m in all_metrics]\n",
    "    overall_scores = [m['overall_score'] for m in all_metrics]\n",
    "    \n",
    "    x = np.arange(len(model_names))\n",
    "    \n",
    "    # KS Statistics\n",
    "    axes[0, 0].bar(x, ks_means, color=['blue', 'green', 'red'][:len(model_names)])\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(model_names)\n",
    "    axes[0, 0].set_title('KS Statistic (lower is better)')\n",
    "    axes[0, 0].set_ylabel('Mean KS Statistic')\n",
    "    \n",
    "    # Wasserstein Distance\n",
    "    axes[0, 1].bar(x, wasserstein_means, color=['blue', 'green', 'red'][:len(model_names)])\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(model_names)\n",
    "    axes[0, 1].set_title('Wasserstein Distance (lower is better)')\n",
    "    axes[0, 1].set_ylabel('Mean Wasserstein Distance')\n",
    "    \n",
    "    # ACF Error\n",
    "    axes[1, 0].bar(x, acf_errors, color=['blue', 'green', 'red'][:len(model_names)])\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(model_names)\n",
    "    axes[1, 0].set_title('Autocorrelation Error (lower is better)')\n",
    "    axes[1, 0].set_ylabel('Mean ACF Error')\n",
    "    \n",
    "    # Overall Score\n",
    "    colors = ['blue', 'green', 'red'][:len(model_names)]\n",
    "    bars = axes[1, 1].bar(x, overall_scores, color=colors)\n",
    "    axes[1, 1].set_xticks(x)\n",
    "    axes[1, 1].set_xticklabels(model_names)\n",
    "    axes[1, 1].set_title('Overall Score (lower is better)')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    \n",
    "    # Highlight best model\n",
    "    best_idx = np.argmin(overall_scores)\n",
    "    bars[best_idx].set_edgecolor('gold')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "    \n",
    "    plt.suptitle('Model Comparison Summary', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return model_names[best_idx]\n",
    "\n",
    "print(\"Visualization functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Model Configuration\n",
      "============================================================\n",
      "Number of features: 27\n",
      "Sequence length: 30\n",
      "Hidden dimension: 128\n",
      "Latent dimension: 32\n",
      "Number of layers: 3\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Initialize Models\n",
    "# ============================================================================\n",
    "\n",
    "num_features = len(available_features)\n",
    "seq_len = CONFIG['sequence_length']\n",
    "hidden_dim = CONFIG['hidden_dim']\n",
    "latent_dim = CONFIG['latent_dim']\n",
    "num_layers = CONFIG['num_layers']\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Model Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of features: {num_features}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Hidden dimension: {hidden_dim}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Number of layers: {num_layers}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Initializing TimeGAN...\n",
      "============================================================\n",
      "TimeGAN initialized.\n"
     ]
    }
   ],
   "source": [
    "# Initialize TimeGAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing TimeGAN...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timegan = TimeGAN(\n",
    "    input_dim=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_layers=num_layers,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"TimeGAN initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training TimeGAN...\n",
      "============================================================\n",
      "Phase 1: Training Embedding Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [2:35:01<00:00, 186.03s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 2: Training Supervised Network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [1:57:38<00:00, 141.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 3 & 4: Joint Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|      | 41/100 [16:05:08<18:37:12, 1136.15s/it]"
     ]
    }
   ],
   "source": [
    "# Train TimeGAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training TimeGAN...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timegan_history = timegan.train(\n",
    "    train_loader=train_loader,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    lr=CONFIG['learning_rate']\n",
    ")\n",
    "\n",
    "print(\"TimeGAN training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LSTM-GAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing LSTM-GAN...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstmgan = LSTMGAN(\n",
    "    input_dim=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_layers=num_layers,\n",
    "    seq_len=seq_len,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"LSTM-GAN initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM-GAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training LSTM-GAN...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstmgan_history = lstmgan.train(\n",
    "    train_loader=train_loader,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    beta1=CONFIG['beta1'],\n",
    "    beta2=CONFIG['beta2']\n",
    ")\n",
    "\n",
    "print(\"LSTM-GAN training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DoppelGANger\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Initializing DoppelGANger...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doppelganger = DoppelGANger(\n",
    "    feature_dim=num_features,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_layers=num_layers,\n",
    "    seq_len=seq_len,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"DoppelGANger initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DoppelGANger\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training DoppelGANger...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doppelganger_history = doppelganger.train(\n",
    "    train_loader=train_loader,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    beta1=CONFIG['beta1'],\n",
    "    beta2=CONFIG['beta2']\n",
    ")\n",
    "\n",
    "print(\"DoppelGANger training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Plotting Training Histories\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "histories = [timegan_history, lstmgan_history, doppelganger_history]\n",
    "model_names = ['TimeGAN', 'LSTM-GAN', 'DoppelGANger']\n",
    "\n",
    "plot_training_history(\n",
    "    histories=histories,\n",
    "    model_names=model_names,\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'training_history.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Generate Synthetic Data and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from each model\n",
    "print(\"=\"*60)\n",
    "print(\"Generating Synthetic Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_synthetic = CONFIG['n_synthetic_samples']\n",
    "\n",
    "# TimeGAN\n",
    "print(f\"Generating {n_synthetic} samples from TimeGAN...\")\n",
    "timegan_synthetic = timegan.generate(n_synthetic, seq_len)\n",
    "print(f\"TimeGAN synthetic shape: {timegan_synthetic.shape}\")\n",
    "\n",
    "# LSTM-GAN\n",
    "print(f\"Generating {n_synthetic} samples from LSTM-GAN...\")\n",
    "lstmgan_synthetic = lstmgan.generate(n_synthetic)\n",
    "print(f\"LSTM-GAN synthetic shape: {lstmgan_synthetic.shape}\")\n",
    "\n",
    "# DoppelGANger\n",
    "print(f\"Generating {n_synthetic} samples from DoppelGANger...\")\n",
    "doppelganger_synthetic = doppelganger.generate(n_synthetic)\n",
    "print(f\"DoppelGANger synthetic shape: {doppelganger_synthetic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = TimeSeriesEvaluator(available_features)\n",
    "\n",
    "# Evaluate each model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluating Models\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use test set for evaluation\n",
    "real_test_data = X_test\n",
    "\n",
    "print(\"\\nEvaluating TimeGAN...\")\n",
    "timegan_metrics = evaluator.compute_all_metrics(real_test_data, timegan_synthetic)\n",
    "\n",
    "print(\"\\nEvaluating LSTM-GAN...\")\n",
    "lstmgan_metrics = evaluator.compute_all_metrics(real_test_data, lstmgan_synthetic)\n",
    "\n",
    "print(\"\\nEvaluating DoppelGANger...\")\n",
    "doppelganger_metrics = evaluator.compute_all_metrics(real_test_data, doppelganger_synthetic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed metrics\n",
    "def print_metrics(model_name, metrics):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{model_name} Evaluation Results\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    print(\"\\n--- Distribution Metrics ---\")\n",
    "    print(f\"  Mean KS Statistic: {metrics['distribution']['ks_mean']:.4f}\")\n",
    "    print(f\"  Mean Wasserstein Distance: {metrics['distribution']['wasserstein_mean']:.4f}\")\n",
    "    print(f\"  Mean Error: {metrics['distribution']['mean_error']:.4f}\")\n",
    "    print(f\"  Std Error: {metrics['distribution']['std_error']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Temporal Metrics ---\")\n",
    "    print(f\"  ACF Error: {metrics['temporal']['acf_error']:.4f}\")\n",
    "    print(f\"  Burstiness Error: {metrics['temporal']['burstiness_error']:.4f}\")\n",
    "    \n",
    "    print(\"\\n--- Correlation Metrics ---\")\n",
    "    print(f\"  Correlation Matrix Error: {metrics['correlation']['corr_matrix_error']:.4f}\")\n",
    "    print(f\"  Normalized Corr Error: {metrics['correlation']['corr_matrix_error_normalized']:.4f}\")\n",
    "    print(f\"  Cross-Correlation Error: {metrics['correlation']['cross_corr_error']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n>>> OVERALL SCORE: {metrics['overall_score']:.4f} (lower is better)\")\n",
    "\n",
    "print_metrics('TimeGAN', timegan_metrics)\n",
    "print_metrics('LSTM-GAN', lstmgan_metrics)\n",
    "print_metrics('DoppelGANger', doppelganger_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison plots for each model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Plotting Distribution Comparisons\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_distribution_comparison(\n",
    "    real_test_data, timegan_synthetic, available_features, 'TimeGAN',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'timegan_distributions.png')\n",
    ")\n",
    "\n",
    "plot_distribution_comparison(\n",
    "    real_test_data, lstmgan_synthetic, available_features, 'LSTM-GAN',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'lstmgan_distributions.png')\n",
    ")\n",
    "\n",
    "plot_distribution_comparison(\n",
    "    real_test_data, doppelganger_synthetic, available_features, 'DoppelGANger',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'doppelganger_distributions.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation comparison plots\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Plotting Correlation Comparisons\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_correlation_comparison(\n",
    "    real_test_data, timegan_synthetic, available_features, 'TimeGAN',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'timegan_correlation.png')\n",
    ")\n",
    "\n",
    "plot_correlation_comparison(\n",
    "    real_test_data, lstmgan_synthetic, available_features, 'LSTM-GAN',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'lstmgan_correlation.png')\n",
    ")\n",
    "\n",
    "plot_correlation_comparison(\n",
    "    real_test_data, doppelganger_synthetic, available_features, 'DoppelGANger',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'doppelganger_correlation.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal comparison plots\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Plotting Temporal Comparisons\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "plot_temporal_comparison(\n",
    "    real_test_data, timegan_synthetic, available_features, 'TimeGAN',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'timegan_temporal.png')\n",
    ")\n",
    "\n",
    "plot_temporal_comparison(\n",
    "    real_test_data, lstmgan_synthetic, available_features, 'LSTM-GAN',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'lstmgan_temporal.png')\n",
    ")\n",
    "\n",
    "plot_temporal_comparison(\n",
    "    real_test_data, doppelganger_synthetic, available_features, 'DoppelGANger',\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'doppelganger_temporal.png')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Model Selection and Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"=\"*60)\n",
    "print(\"Model Comparison and Selection\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_metrics = [timegan_metrics, lstmgan_metrics, doppelganger_metrics]\n",
    "model_names = ['TimeGAN', 'LSTM-GAN', 'DoppelGANger']\n",
    "\n",
    "best_model_name = plot_model_comparison(\n",
    "    all_metrics=all_metrics,\n",
    "    model_names=model_names,\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'model_comparison.png')\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_data = {\n",
    "    'Model': model_names,\n",
    "    'KS Statistic': [m['distribution']['ks_mean'] for m in all_metrics],\n",
    "    'Wasserstein': [m['distribution']['wasserstein_mean'] for m in all_metrics],\n",
    "    'ACF Error': [m['temporal']['acf_error'] for m in all_metrics],\n",
    "    'Burstiness Error': [m['temporal']['burstiness_error'] for m in all_metrics],\n",
    "    'Correlation Error': [m['correlation']['corr_matrix_error_normalized'] for m in all_metrics],\n",
    "    'Overall Score': [m['overall_score'] for m in all_metrics]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.round(4)\n",
    "summary_df.to_csv(os.path.join(CONFIG['output_dir'], 'model_comparison_summary.csv'), index=False)\n",
    "\n",
    "print(\"\\nModel Comparison Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Generate Final Synthetic Normal Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model\n",
    "best_model_map = {\n",
    "    'TimeGAN': timegan,\n",
    "    'LSTM-GAN': lstmgan,\n",
    "    'DoppelGANger': doppelganger\n",
    "}\n",
    "best_synthetic_map = {\n",
    "    'TimeGAN': timegan_synthetic,\n",
    "    'LSTM-GAN': lstmgan_synthetic,\n",
    "    'DoppelGANger': doppelganger_synthetic\n",
    "}\n",
    "\n",
    "best_model = best_model_map[best_model_name]\n",
    "best_synthetic = best_synthetic_map[best_model_name]\n",
    "\n",
    "print(f\"Using {best_model_name} for final synthetic traffic generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate larger set of synthetic normal traffic\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating Final Synthetic Normal Traffic\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_final_samples = 5000  # Adjust as needed\n",
    "\n",
    "if best_model_name == 'TimeGAN':\n",
    "    final_synthetic = best_model.generate(n_final_samples, seq_len)\n",
    "else:\n",
    "    final_synthetic = best_model.generate(n_final_samples)\n",
    "\n",
    "print(f\"Generated synthetic data shape: {final_synthetic.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse transform to original scale\n",
    "print(\"\\nInverse transforming to original scale...\")\n",
    "\n",
    "# Reshape for inverse transform\n",
    "final_synthetic_flat = final_synthetic.reshape(-1, num_features)\n",
    "final_synthetic_original = scaler.inverse_transform(final_synthetic_flat)\n",
    "final_synthetic_original = final_synthetic_original.reshape(final_synthetic.shape)\n",
    "\n",
    "print(f\"Final synthetic data shape (original scale): {final_synthetic_original.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synthetic data to CSV\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving Synthetic Normal Traffic\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Flatten sequences for saving (each row = one timestep)\n",
    "n_samples, seq_len_out, n_features = final_synthetic_original.shape\n",
    "synthetic_records = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    for t in range(seq_len_out):\n",
    "        record = {'sequence_id': i, 'timestep': t, 'label': 'normal'}\n",
    "        for j, feat in enumerate(available_features):\n",
    "            record[feat] = final_synthetic_original[i, t, j]\n",
    "        synthetic_records.append(record)\n",
    "\n",
    "synthetic_df = pd.DataFrame(synthetic_records)\n",
    "synthetic_output_path = os.path.join(CONFIG['output_dir'], 'synthetic_normal_traffic.csv')\n",
    "synthetic_df.to_csv(synthetic_output_path, index=False)\n",
    "\n",
    "print(f\"Saved synthetic normal traffic to: {synthetic_output_path}\")\n",
    "print(f\"Total records: {len(synthetic_df)}\")\n",
    "print(f\"Unique sequences: {synthetic_df['sequence_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model checkpoints\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Saving Model Checkpoints\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save TimeGAN components\n",
    "timegan_path = os.path.join(CONFIG['output_dir'], 'timegan_checkpoint.pt')\n",
    "torch.save({\n",
    "    'embedder': timegan.embedder.state_dict(),\n",
    "    'recovery': timegan.recovery.state_dict(),\n",
    "    'generator': timegan.generator.state_dict(),\n",
    "    'supervisor': timegan.supervisor.state_dict(),\n",
    "    'discriminator': timegan.discriminator.state_dict(),\n",
    "}, timegan_path)\n",
    "print(f\"Saved TimeGAN checkpoint to: {timegan_path}\")\n",
    "\n",
    "# Save LSTM-GAN\n",
    "lstmgan_path = os.path.join(CONFIG['output_dir'], 'lstmgan_checkpoint.pt')\n",
    "torch.save({\n",
    "    'generator': lstmgan.generator.state_dict(),\n",
    "    'discriminator': lstmgan.discriminator.state_dict(),\n",
    "}, lstmgan_path)\n",
    "print(f\"Saved LSTM-GAN checkpoint to: {lstmgan_path}\")\n",
    "\n",
    "# Save DoppelGANger\n",
    "doppelganger_path = os.path.join(CONFIG['output_dir'], 'doppelganger_checkpoint.pt')\n",
    "torch.save({\n",
    "    'feature_gen': doppelganger.feature_gen.state_dict(),\n",
    "    'attr_gen': doppelganger.attr_gen.state_dict(),\n",
    "    'discriminator': doppelganger.discriminator.state_dict(),\n",
    "}, doppelganger_path)\n",
    "print(f\"Saved DoppelGANger checkpoint to: {doppelganger_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler for future use\n",
    "import pickle\n",
    "\n",
    "scaler_path = os.path.join(CONFIG['output_dir'], 'scaler.pkl')\n",
    "with open(scaler_path, 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(f\"Saved scaler to: {scaler_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration and results\n",
    "results_summary = {\n",
    "    'config': CONFIG,\n",
    "    'best_model': best_model_name,\n",
    "    'metrics': {\n",
    "        'TimeGAN': {\n",
    "            'overall_score': timegan_metrics['overall_score'],\n",
    "            'ks_mean': timegan_metrics['distribution']['ks_mean'],\n",
    "            'acf_error': timegan_metrics['temporal']['acf_error']\n",
    "        },\n",
    "        'LSTM-GAN': {\n",
    "            'overall_score': lstmgan_metrics['overall_score'],\n",
    "            'ks_mean': lstmgan_metrics['distribution']['ks_mean'],\n",
    "            'acf_error': lstmgan_metrics['temporal']['acf_error']\n",
    "        },\n",
    "        'DoppelGANger': {\n",
    "            'overall_score': doppelganger_metrics['overall_score'],\n",
    "            'ks_mean': doppelganger_metrics['distribution']['ks_mean'],\n",
    "            'acf_error': doppelganger_metrics['temporal']['acf_error']\n",
    "        }\n",
    "    },\n",
    "    'data_info': {\n",
    "        'original_samples': len(df),\n",
    "        'normal_samples': len(df_normal),\n",
    "        'features': available_features,\n",
    "        'sequence_length': seq_len,\n",
    "        'train_sequences': len(X_train),\n",
    "        'val_sequences': len(X_val),\n",
    "        'test_sequences': len(X_test),\n",
    "        'synthetic_sequences': n_final_samples\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "results_path = os.path.join(CONFIG['output_dir'], 'training_results.json')\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "print(f\"Saved results summary to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BGP NORMAL TRAFFIC GAN GENERATION - COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "SUMMARY\n",
    "-------\n",
    "1. Data Processing:\n",
    "   - Original samples: {len(df)}\n",
    "   - Normal traffic samples: {len(df_normal)}\n",
    "   - Features used: {len(available_features)}\n",
    "   - Sequence length: {seq_len} timesteps\n",
    "\n",
    "2. Train/Test Split:\n",
    "   - Training sequences: {len(X_train)}\n",
    "   - Validation sequences: {len(X_val)}\n",
    "   - Test sequences: {len(X_test)}\n",
    "\n",
    "3. Models Trained:\n",
    "   - TimeGAN (Overall Score: {timegan_metrics['overall_score']:.4f})\n",
    "   - LSTM-GAN (Overall Score: {lstmgan_metrics['overall_score']:.4f})\n",
    "   - DoppelGANger (Overall Score: {doppelganger_metrics['overall_score']:.4f})\n",
    "\n",
    "4. Best Model: {best_model_name}\n",
    "\n",
    "5. Generated Synthetic Normal Traffic:\n",
    "   - Sequences: {n_final_samples}\n",
    "   - Total records: {n_final_samples * seq_len}\n",
    "\n",
    "OUTPUT FILES (in {CONFIG['output_dir']}):\n",
    "------------------------------------------\n",
    "- synthetic_normal_traffic.csv : Generated synthetic data\n",
    "- timegan_checkpoint.pt        : TimeGAN model weights\n",
    "- lstmgan_checkpoint.pt        : LSTM-GAN model weights\n",
    "- doppelganger_checkpoint.pt   : DoppelGANger model weights\n",
    "- scaler.pkl                   : StandardScaler for transformation\n",
    "- training_results.json        : Training configuration and metrics\n",
    "- model_comparison_summary.csv : Model comparison table\n",
    "- Various .png visualization files\n",
    "\n",
    "NEXT STEPS:\n",
    "-----------\n",
    "1. Use synthetic normal traffic for:\n",
    "   - Training anomaly detectors\n",
    "   - Data augmentation\n",
    "   - Normal-only baseline comparisons\n",
    "\n",
    "2. Extend to anomaly phase:\n",
    "   - Add conditional labels (attack types)\n",
    "   - Train conditional GAN (TimeGAN-c or LSTM-cGAN)\n",
    "   - Generate synthetic attack traffic\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all output files\n",
    "print(\"\\nGenerated files:\")\n",
    "for f in os.listdir(CONFIG['output_dir']):\n",
    "    filepath = os.path.join(CONFIG['output_dir'], f)\n",
    "    size = os.path.getsize(filepath) / 1024  # KB\n",
    "    print(f\"  {f}: {size:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
