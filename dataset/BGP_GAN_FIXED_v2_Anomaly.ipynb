{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Setup\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max', 'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6', 'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "CONTINUOUS_FEATURES = ['edit_distance_avg', 'rare_ases_avg']\n",
    "\n",
    "ALL_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_avg', 'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6', 'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'rare_ases_avg', 'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Sparse features - apply log1p transform\n",
    "SPARSE_FEATURES = [\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6', 'imp_wd_spath', 'unique_as_path_max',\n",
    "    'origin_changes', 'flaps', 'nadas'\n",
    "]\n",
    "\n",
    "print(f\"Total features: {len(ALL_FEATURES)}, Sparse: {len(SPARSE_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform functions\n",
    "def apply_log_transform(data, feature_names, sparse_features):\n",
    "    transformed = data.copy()\n",
    "    indices = []\n",
    "    for feat in sparse_features:\n",
    "        if feat in feature_names:\n",
    "            idx = feature_names.index(feat)\n",
    "            indices.append(idx)\n",
    "            transformed[:, idx] = np.log1p(data[:, idx])\n",
    "    return transformed, indices\n",
    "\n",
    "def inverse_log_transform(data, feature_names, sparse_features):\n",
    "    restored = data.copy()\n",
    "    if len(data.shape) == 3:\n",
    "        n, s, f = data.shape\n",
    "        restored = restored.reshape(-1, f)\n",
    "        reshape = True\n",
    "    else:\n",
    "        reshape = False\n",
    "    \n",
    "    for feat in sparse_features:\n",
    "        if feat in feature_names:\n",
    "            idx = feature_names.index(feat)\n",
    "            restored[:, idx] = np.expm1(restored[:, idx])\n",
    "    \n",
    "    return restored.reshape(n, s, f) if reshape else restored\n",
    "\n",
    "def post_process_synthetic_data(synthetic_data, feature_names, scaler, sparse_features):\n",
    "    n, s, f = synthetic_data.shape\n",
    "    flat = scaler.inverse_transform(synthetic_data.reshape(-1, f))\n",
    "    flat = inverse_log_transform(flat.reshape(n, s, f), feature_names, sparse_features).reshape(-1, f)\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        flat[:, i] = np.maximum(flat[:, i], 0)\n",
    "        if feat in INTEGER_FEATURES:\n",
    "            flat[:, i] = np.round(flat[:, i])\n",
    "    \n",
    "    return flat.reshape(n, s, f)\n",
    "\n",
    "print(\"Transform functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths - ANOMALY DATA\n",
    "DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv'\n",
    "OUTPUT_DIR = '/home/smotaali/BGP_Traffic_Generation/results/gan_anomaly_outputs_improved/'\n",
    "\n",
    "# High confidence anomaly labels to filter\n",
    "HIGH_CONFIDENCE_LABELS = ['medium_confidence', 'high_confidence', 'very_high_confidence']\n",
    "\n",
    "# Sequence\n",
    "SEQ_LEN = 30\n",
    "STRIDE = 1\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 250\n",
    "\n",
    "# Learning rates\n",
    "LR_G = 0.0002\n",
    "LR_D = 0.0001\n",
    "BETA1, BETA2 = 0.5, 0.999\n",
    "\n",
    "# Model capacity (INCREASED)\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 64\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "# Training\n",
    "LABEL_SMOOTHING = 0.1\n",
    "NOISE_STD = 0.05\n",
    "CLIP_VALUE = 1.0\n",
    "\n",
    "# Loss weights\n",
    "SPARSE_WEIGHT = 2.0      # Weight for sparse features in MSE\n",
    "HIST_LOSS_WEIGHT = 0.5   # Histogram loss weight\n",
    "CORR_LOSS_WEIGHT = 0.3   # Correlation loss weight\n",
    "\n",
    "# Data split\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "N_SYNTHETIC = 2000\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Configuration for ANOMALY traffic generation:\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"High confidence labels: {HIGH_CONFIDENCE_LABELS}\")\n",
    "print(f\"Config: H={HIDDEN_DIM}, L={LATENT_DIM}, Layers={NUM_LAYERS}, Epochs={EPOCHS}\")\n",
    "print(f\"Loss weights: Sparse={SPARSE_WEIGHT}, Hist={HIST_LOSS_WEIGHT}, Corr={CORR_LOSS_WEIGHT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Loading with Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading anomaly data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "\n",
    "# Check confidence_label distribution\n",
    "if 'confidence_label' in df.columns:\n",
    "    print(f\"\\nConfidence labels distribution:\")\n",
    "    print(df['confidence_label'].value_counts())\n",
    "\n",
    "# Filter high confidence anomalies\n",
    "df_anomaly = df[df['confidence_label'].isin(HIGH_CONFIDENCE_LABELS)].copy()\n",
    "print(f\"\\nHigh confidence anomaly rows: {len(df_anomaly):,}\")\n",
    "\n",
    "available_features = [f for f in ALL_FEATURES if f in df_anomaly.columns]\n",
    "sparse_indices = [available_features.index(f) for f in SPARSE_FEATURES if f in available_features]\n",
    "data = df_anomaly[available_features].fillna(0).values\n",
    "NUM_FEATURES = len(available_features)\n",
    "\n",
    "print(f\"\\nSparse features BEFORE log transform:\")\n",
    "for feat in SPARSE_FEATURES[:4]:\n",
    "    if feat in available_features:\n",
    "        idx = available_features.index(feat)\n",
    "        print(f\"  {feat}: range=[{data[:, idx].min():.0f}, {data[:, idx].max():.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log transform\n",
    "data_log, transform_indices = apply_log_transform(data, available_features, SPARSE_FEATURES)\n",
    "\n",
    "print(\"Sparse features AFTER log transform:\")\n",
    "for feat in SPARSE_FEATURES[:4]:\n",
    "    if feat in available_features:\n",
    "        idx = available_features.index(feat)\n",
    "        print(f\"  {feat}: range=[{data_log[:, idx].min():.2f}, {data_log[:, idx].max():.2f}]\")\n",
    "\n",
    "# Normalize\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_norm = scaler.fit_transform(data_log)\n",
    "print(f\"\\nNormalized range: [{data_norm.min():.4f}, {data_norm.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "sequences = np.array([data_norm[i:i+SEQ_LEN] for i in range(0, len(data_norm)-SEQ_LEN+1, STRIDE)])\n",
    "X_train_val, X_test = train_test_split(sequences, test_size=TEST_SIZE, random_state=SEED)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=VAL_SIZE, random_state=SEED)\n",
    "\n",
    "print(f\"Sequences: {sequences.shape}\")\n",
    "print(f\"Train: {X_train.shape[0]:,}, Val: {X_val.shape[0]:,}, Test: {X_test.shape[0]:,}\")\n",
    "\n",
    "train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Loss Functions (Histogram + Correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributionLosses:\n",
    "    \"\"\"Additional losses for distribution matching.\"\"\"\n",
    "    \n",
    "    def __init__(self, sparse_indices, sparse_weight=2.0, n_bins=50):\n",
    "        self.sparse_indices = sparse_indices\n",
    "        self.sparse_weight = sparse_weight\n",
    "        self.n_bins = n_bins\n",
    "    \n",
    "    def weighted_mse(self, pred, target):\n",
    "        \"\"\"MSE with higher weight for sparse features.\"\"\"\n",
    "        mse = (pred - target) ** 2\n",
    "        if self.sparse_indices:\n",
    "            weights = torch.ones_like(mse)\n",
    "            for idx in self.sparse_indices:\n",
    "                weights[:, :, idx] = self.sparse_weight\n",
    "            mse = mse * weights\n",
    "        return mse.mean()\n",
    "    \n",
    "    def histogram_loss(self, real, fake):\n",
    "        \"\"\"\n",
    "        Soft histogram loss using kernel density estimation.\n",
    "        Penalizes distribution mismatch per feature.\n",
    "        \"\"\"\n",
    "        # Flatten to (batch * seq_len, features)\n",
    "        real_flat = real.reshape(-1, real.shape[-1])\n",
    "        fake_flat = fake.reshape(-1, fake.shape[-1])\n",
    "        \n",
    "        loss = 0.0\n",
    "        n_features = real_flat.shape[1]\n",
    "        \n",
    "        for i in range(n_features):\n",
    "            # Compute histogram-like statistics\n",
    "            real_col = real_flat[:, i]\n",
    "            fake_col = fake_flat[:, i]\n",
    "            \n",
    "            # Mean and std matching\n",
    "            mean_diff = (real_col.mean() - fake_col.mean()) ** 2\n",
    "            std_diff = (real_col.std() - fake_col.std()) ** 2\n",
    "            \n",
    "            # Quantile matching (approximates histogram)\n",
    "            quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]\n",
    "            for q in quantiles:\n",
    "                real_q = torch.quantile(real_col, q)\n",
    "                fake_q = torch.quantile(fake_col, q)\n",
    "                mean_diff += (real_q - fake_q) ** 2\n",
    "            \n",
    "            # Higher weight for sparse features\n",
    "            weight = self.sparse_weight if i in self.sparse_indices else 1.0\n",
    "            loss += weight * (mean_diff + std_diff)\n",
    "        \n",
    "        return loss / n_features\n",
    "    \n",
    "    def correlation_loss(self, real, fake):\n",
    "        \"\"\"\n",
    "        Penalizes difference in correlation structure.\n",
    "        \"\"\"\n",
    "        # Flatten\n",
    "        real_flat = real.reshape(-1, real.shape[-1])\n",
    "        fake_flat = fake.reshape(-1, fake.shape[-1])\n",
    "        \n",
    "        # Compute correlation matrices\n",
    "        def corrcoef(x):\n",
    "            # Center\n",
    "            x_centered = x - x.mean(dim=0, keepdim=True)\n",
    "            # Covariance\n",
    "            cov = torch.mm(x_centered.T, x_centered) / (x.shape[0] - 1)\n",
    "            # Correlation\n",
    "            std = torch.sqrt(torch.diag(cov) + 1e-8)\n",
    "            corr = cov / torch.outer(std, std)\n",
    "            return torch.clamp(corr, -1, 1)\n",
    "        \n",
    "        real_corr = corrcoef(real_flat)\n",
    "        fake_corr = corrcoef(fake_flat)\n",
    "        \n",
    "        # Frobenius norm of difference\n",
    "        corr_diff = torch.mean((real_corr - fake_corr) ** 2)\n",
    "        \n",
    "        return corr_diff\n",
    "\n",
    "# Initialize\n",
    "dist_losses = DistributionLosses(sparse_indices, SPARSE_WEIGHT)\n",
    "print(\"Distribution losses defined (weighted MSE, histogram, correlation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Improved LSTM-GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                           dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        bs, seq, _ = z.shape\n",
    "        out, _ = self.lstm(z)\n",
    "        out = out.contiguous().view(-1, out.size(-1))\n",
    "        out = self.fc(out)\n",
    "        return out.view(bs, seq, -1)\n",
    "\n",
    "class LSTM_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                           dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class ImprovedLSTMGAN:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, seq_len, device, dist_losses):\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.dist_losses = dist_losses\n",
    "        \n",
    "        self.G = LSTM_Generator(latent_dim, hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.D = LSTM_Discriminator(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train(self, train_loader, epochs, lr_g=0.0002, lr_d=0.0001, label_smoothing=0.1):\n",
    "        opt_g = optim.Adam(self.G.parameters(), lr=lr_g, betas=(BETA1, BETA2))\n",
    "        opt_d = optim.Adam(self.D.parameters(), lr=lr_d, betas=(BETA1, BETA2))\n",
    "        \n",
    "        sched_g = optim.lr_scheduler.CosineAnnealingLR(opt_g, T_max=epochs, eta_min=lr_g/10)\n",
    "        sched_d = optim.lr_scheduler.CosineAnnealingLR(opt_d, T_max=epochs, eta_min=lr_d/10)\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': []}\n",
    "        start = time.time()\n",
    "        \n",
    "        real_label = 1.0 - label_smoothing\n",
    "        fake_label = label_smoothing\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            g_losses, d_losses = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                real = batch[0]\n",
    "                bs = real.size(0)\n",
    "                \n",
    "                real_labels = torch.full((bs, 1), real_label).to(self.device)\n",
    "                fake_labels = torch.full((bs, 1), fake_label).to(self.device)\n",
    "                \n",
    "                # Train D\n",
    "                opt_d.zero_grad()\n",
    "                real_noisy = torch.clamp(real + NOISE_STD * torch.randn_like(real), 0, 1)\n",
    "                d_real = self.D(real_noisy)\n",
    "                loss_real = self.criterion(d_real, real_labels)\n",
    "                \n",
    "                z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                fake = self.G(z)\n",
    "                d_fake = self.D(fake.detach())\n",
    "                loss_fake = self.criterion(d_fake, fake_labels)\n",
    "                \n",
    "                d_loss = loss_real + loss_fake\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.D.parameters(), CLIP_VALUE)\n",
    "                opt_d.step()\n",
    "                \n",
    "                # Train G (2x when D is strong)\n",
    "                n_g = 2 if d_loss.item() < 0.5 else 1\n",
    "                for _ in range(n_g):\n",
    "                    opt_g.zero_grad()\n",
    "                    z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                    fake = self.G(z)\n",
    "                    d_fake = self.D(fake)\n",
    "                    \n",
    "                    # Adversarial loss\n",
    "                    g_loss = self.criterion(d_fake, real_labels)\n",
    "                    \n",
    "                    # Feature matching (weighted MSE)\n",
    "                    g_loss += self.dist_losses.weighted_mse(fake, real) * 10\n",
    "                    \n",
    "                    # Histogram loss\n",
    "                    g_loss += self.dist_losses.histogram_loss(real, fake) * HIST_LOSS_WEIGHT\n",
    "                    \n",
    "                    # Correlation loss\n",
    "                    g_loss += self.dist_losses.correlation_loss(real, fake) * CORR_LOSS_WEIGHT\n",
    "                    \n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.G.parameters(), CLIP_VALUE)\n",
    "                    opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            sched_g.step()\n",
    "            sched_d.step()\n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 25 == 0:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{epochs} | D: {history['d_loss'][-1]:.4f} | G: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        print(f\"  Time: {(time.time()-start)/60:.1f} min\")\n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.seq_len, self.latent_dim).to(self.device)\n",
    "            return self.G(z).cpu().numpy()\n",
    "\n",
    "print(\"Improved LSTM-GAN defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Improved TimeGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeGAN_Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                         dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, x):\n",
    "        h, _ = self.rnn(x)\n",
    "        return torch.sigmoid(self.fc(h))\n",
    "\n",
    "class TimeGAN_Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                         dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(0.2),\n",
    "                                nn.Linear(hidden_dim, output_dim), nn.Sigmoid())\n",
    "    def forward(self, h):\n",
    "        r, _ = self.rnn(h)\n",
    "        return self.fc(r)\n",
    "\n",
    "class TimeGAN_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(latent_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                         dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, z):\n",
    "        e, _ = self.rnn(z)\n",
    "        return torch.sigmoid(self.fc(e))\n",
    "\n",
    "class TimeGAN_Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, max(1, num_layers-1), batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "    def forward(self, h):\n",
    "        s, _ = self.rnn(h)\n",
    "        return torch.sigmoid(self.fc(s))\n",
    "\n",
    "class TimeGAN_Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                         dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim//2), nn.LeakyReLU(0.2),\n",
    "                                nn.Linear(hidden_dim//2, 1))\n",
    "    def forward(self, h):\n",
    "        d, _ = self.rnn(h)\n",
    "        return self.fc(d)\n",
    "\n",
    "print(\"TimeGAN components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTimeGAN:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, device, dist_losses):\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.dist_losses = dist_losses\n",
    "        \n",
    "        self.embedder = TimeGAN_Embedder(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.recovery = TimeGAN_Recovery(hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.generator = TimeGAN_Generator(latent_dim, hidden_dim, num_layers).to(device)\n",
    "        self.supervisor = TimeGAN_Supervisor(hidden_dim, num_layers).to(device)\n",
    "        self.discriminator = TimeGAN_Discriminator(hidden_dim, num_layers).to(device)\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train(self, train_loader, epochs, lr_g=0.0002, lr_d=0.0001, label_smoothing=0.1):\n",
    "        opt_e = optim.Adam(list(self.embedder.parameters()) + list(self.recovery.parameters()), lr=lr_g)\n",
    "        opt_s = optim.Adam(self.supervisor.parameters(), lr=lr_g)\n",
    "        opt_g = optim.Adam(list(self.generator.parameters()) + list(self.supervisor.parameters()), lr=lr_g)\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=lr_d)\n",
    "        \n",
    "        history = {'e_loss': [], 's_loss': [], 'g_loss': [], 'd_loss': []}\n",
    "        start = time.time()\n",
    "        \n",
    "        real_label = 1.0 - label_smoothing\n",
    "        fake_label = label_smoothing\n",
    "        \n",
    "        # Phase 1: Embedding\n",
    "        print(\"  Phase 1: Embedding...\")\n",
    "        for epoch in range(epochs // 3):\n",
    "            e_losses = []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0]\n",
    "                opt_e.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                x_tilde = self.recovery(h)\n",
    "                loss = self.dist_losses.weighted_mse(x_tilde, x)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.embedder.parameters(), CLIP_VALUE)\n",
    "                torch.nn.utils.clip_grad_norm_(self.recovery.parameters(), CLIP_VALUE)\n",
    "                opt_e.step()\n",
    "                e_losses.append(loss.item())\n",
    "            history['e_loss'].append(np.mean(e_losses))\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs//3} | E: {history['e_loss'][-1]:.4f}\")\n",
    "        \n",
    "        # Phase 2: Supervised\n",
    "        print(\"  Phase 2: Supervised...\")\n",
    "        for epoch in range(epochs // 3):\n",
    "            s_losses = []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0]\n",
    "                opt_s.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                h_s = self.supervisor(h)\n",
    "                loss = nn.MSELoss()(h[:, 1:, :], h_s[:, :-1, :])\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.supervisor.parameters(), CLIP_VALUE)\n",
    "                opt_s.step()\n",
    "                s_losses.append(loss.item())\n",
    "            history['s_loss'].append(np.mean(s_losses))\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs//3} | S: {history['s_loss'][-1]:.4f}\")\n",
    "        \n",
    "        # Phase 3: Joint\n",
    "        print(\"  Phase 3: Joint...\")\n",
    "        for epoch in range(epochs // 3):\n",
    "            g_losses, d_losses = [], []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0]\n",
    "                bs, seq_len, _ = x.shape\n",
    "                z = torch.randn(bs, seq_len, self.latent_dim).to(self.device)\n",
    "                \n",
    "                real_labels = torch.full((bs, seq_len, 1), real_label).to(self.device)\n",
    "                fake_labels = torch.full((bs, seq_len, 1), fake_label).to(self.device)\n",
    "                \n",
    "                # Generator (2x)\n",
    "                for _ in range(2):\n",
    "                    opt_g.zero_grad()\n",
    "                    h = self.embedder(x)\n",
    "                    h_s = self.supervisor(h)\n",
    "                    e_hat = self.generator(z)\n",
    "                    h_hat = self.supervisor(e_hat)\n",
    "                    x_hat = self.recovery(h_hat)\n",
    "                    \n",
    "                    y_fake = self.discriminator(h_hat)\n",
    "                    g_loss = self.bce(y_fake, real_labels)\n",
    "                    g_loss += nn.MSELoss()(h[:, 1:, :], h_s[:, :-1, :]) * 10\n",
    "                    \n",
    "                    # Distribution losses\n",
    "                    g_loss += self.dist_losses.weighted_mse(x_hat, x) * 100\n",
    "                    g_loss += self.dist_losses.histogram_loss(x, x_hat) * HIST_LOSS_WEIGHT\n",
    "                    g_loss += self.dist_losses.correlation_loss(x, x_hat) * CORR_LOSS_WEIGHT\n",
    "                    \n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.generator.parameters(), CLIP_VALUE)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.supervisor.parameters(), CLIP_VALUE)\n",
    "                    opt_g.step()\n",
    "                \n",
    "                # Discriminator\n",
    "                opt_d.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                e_hat = self.generator(z)\n",
    "                h_hat = self.supervisor(e_hat)\n",
    "                y_real = self.discriminator(h)\n",
    "                y_fake = self.discriminator(h_hat.detach())\n",
    "                d_loss = self.bce(y_real, real_labels) + self.bce(y_fake, fake_labels)\n",
    "                \n",
    "                if d_loss.item() > 0.3:\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), CLIP_VALUE)\n",
    "                    opt_d.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs//3} | D: {history['d_loss'][-1]:.4f} | G: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        print(f\"  Time: {(time.time()-start)/60:.1f} min\")\n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples, seq_len):\n",
    "        self.generator.eval()\n",
    "        self.supervisor.eval()\n",
    "        self.recovery.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, seq_len, self.latent_dim).to(self.device)\n",
    "            e_hat = self.generator(z)\n",
    "            h_hat = self.supervisor(e_hat)\n",
    "            return self.recovery(h_hat).cpu().numpy()\n",
    "\n",
    "print(\"Improved TimeGAN defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Improved DoppelGANger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DG_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                           dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.LeakyReLU(0.2),\n",
    "                                nn.Linear(hidden_dim, output_dim), nn.Sigmoid())\n",
    "    def forward(self, z):\n",
    "        out, _ = self.lstm(z)\n",
    "        return self.fc(out)\n",
    "\n",
    "class DG_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True,\n",
    "                           bidirectional=True, dropout=0.2 if num_layers > 1 else 0)\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim), nn.LeakyReLU(0.2),\n",
    "                                nn.Dropout(0.4), nn.Linear(hidden_dim, 1))\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "class ImprovedDoppelGANger:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, seq_len, device, dist_losses):\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.dist_losses = dist_losses\n",
    "        \n",
    "        self.G = DG_Generator(latent_dim, hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.D = DG_Discriminator(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train(self, train_loader, epochs, lr_g=0.0002, lr_d=0.0001, label_smoothing=0.1):\n",
    "        opt_g = optim.Adam(self.G.parameters(), lr=lr_g, betas=(BETA1, BETA2))\n",
    "        opt_d = optim.Adam(self.D.parameters(), lr=lr_d, betas=(BETA1, BETA2))\n",
    "        \n",
    "        sched_g = optim.lr_scheduler.CosineAnnealingLR(opt_g, T_max=epochs, eta_min=lr_g/10)\n",
    "        sched_d = optim.lr_scheduler.CosineAnnealingLR(opt_d, T_max=epochs, eta_min=lr_d/10)\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': []}\n",
    "        start = time.time()\n",
    "        \n",
    "        real_label = 1.0 - label_smoothing\n",
    "        fake_label = label_smoothing\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            g_losses, d_losses = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                real = batch[0]\n",
    "                bs = real.size(0)\n",
    "                \n",
    "                real_labels = torch.full((bs, 1), real_label).to(self.device)\n",
    "                fake_labels = torch.full((bs, 1), fake_label).to(self.device)\n",
    "                \n",
    "                # Train D\n",
    "                opt_d.zero_grad()\n",
    "                d_real = self.D(real)\n",
    "                loss_real = self.criterion(d_real, real_labels)\n",
    "                \n",
    "                z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                fake = self.G(z)\n",
    "                d_fake = self.D(fake.detach())\n",
    "                loss_fake = self.criterion(d_fake, fake_labels)\n",
    "                \n",
    "                d_loss = loss_real + loss_fake\n",
    "                d_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.D.parameters(), CLIP_VALUE)\n",
    "                opt_d.step()\n",
    "                \n",
    "                # Train G (2x)\n",
    "                for _ in range(2):\n",
    "                    opt_g.zero_grad()\n",
    "                    z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                    fake = self.G(z)\n",
    "                    d_fake = self.D(fake)\n",
    "                    \n",
    "                    g_loss = self.criterion(d_fake, real_labels)\n",
    "                    g_loss += self.dist_losses.weighted_mse(fake, real) * 10\n",
    "                    g_loss += self.dist_losses.histogram_loss(real, fake) * HIST_LOSS_WEIGHT\n",
    "                    g_loss += self.dist_losses.correlation_loss(real, fake) * CORR_LOSS_WEIGHT\n",
    "                    \n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.G.parameters(), CLIP_VALUE)\n",
    "                    opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            sched_g.step()\n",
    "            sched_d.step()\n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 25 == 0:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{epochs} | D: {history['d_loss'][-1]:.4f} | G: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        print(f\"  Time: {(time.time()-start)/60:.1f} min\")\n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.seq_len, self.latent_dim).to(self.device)\n",
    "            return self.G(z).cpu().numpy()\n",
    "\n",
    "print(\"Improved DoppelGANger defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Training All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "histories = {}\n",
    "synthetic_raw = {}\n",
    "synthetic_processed = {}\n",
    "\n",
    "total_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM-GAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Improved LSTM-GAN on Anomaly Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstm_gan = ImprovedLSTMGAN(NUM_FEATURES, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, SEQ_LEN, device, dist_losses)\n",
    "histories['LSTM-GAN'] = lstm_gan.train(train_loader, EPOCHS, lr_g=LR_G, lr_d=LR_D, label_smoothing=LABEL_SMOOTHING)\n",
    "models['LSTM-GAN'] = lstm_gan\n",
    "synthetic_raw['LSTM-GAN'] = lstm_gan.generate(N_SYNTHETIC)\n",
    "print(f\"Generated: {synthetic_raw['LSTM-GAN'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TimeGAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Improved TimeGAN on Anomaly Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timegan = ImprovedTimeGAN(NUM_FEATURES, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, device, dist_losses)\n",
    "histories['TimeGAN'] = timegan.train(train_loader, EPOCHS, lr_g=LR_G, lr_d=LR_D, label_smoothing=LABEL_SMOOTHING)\n",
    "models['TimeGAN'] = timegan\n",
    "synthetic_raw['TimeGAN'] = timegan.generate(N_SYNTHETIC, SEQ_LEN)\n",
    "print(f\"Generated: {synthetic_raw['TimeGAN'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DoppelGANger\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Improved DoppelGANger on Anomaly Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doppelganger = ImprovedDoppelGANger(NUM_FEATURES, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, SEQ_LEN, device, dist_losses)\n",
    "histories['DoppelGANger'] = doppelganger.train(train_loader, EPOCHS, lr_g=LR_G, lr_d=LR_D, label_smoothing=LABEL_SMOOTHING)\n",
    "models['DoppelGANger'] = doppelganger\n",
    "synthetic_raw['DoppelGANger'] = doppelganger.generate(N_SYNTHETIC)\n",
    "print(f\"Generated: {synthetic_raw['DoppelGANger'].shape}\")\n",
    "\n",
    "print(f\"\\nTotal training time: {(time.time()-total_start)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"POST-PROCESSING: Denormalize -> Inverse Log -> Clip -> Round\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, raw in synthetic_raw.items():\n",
    "    processed = post_process_synthetic_data(raw, available_features, scaler, SPARSE_FEATURES)\n",
    "    synthetic_processed[name] = processed\n",
    "    print(f\"{name}: [{raw.min():.3f}, {raw.max():.3f}] -> [{processed.min():.0f}, {processed.max():.0f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 11: Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(real_data, syn_data, feature_names, sparse_features):\n",
    "    real_flat = real_data.reshape(-1, len(feature_names))\n",
    "    syn_flat = syn_data.reshape(-1, len(feature_names))\n",
    "    \n",
    "    metrics = {'per_feature': {}, 'ks_stats': [], 'mae_values': [], 'sparse_ks': [], 'poor_ks': []}\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        real_col, syn_col = real_flat[:, i], syn_flat[:, i]\n",
    "        if real_col.std() < 1e-10 or syn_col.std() < 1e-10:\n",
    "            continue\n",
    "        \n",
    "        ks_stat, _ = stats.ks_2samp(real_col, syn_col)\n",
    "        mae = np.abs(real_col.mean() - syn_col.mean())\n",
    "        \n",
    "        if not np.isnan(ks_stat):\n",
    "            metrics['ks_stats'].append(ks_stat)\n",
    "            if feat in sparse_features:\n",
    "                metrics['sparse_ks'].append((feat, ks_stat))\n",
    "            if ks_stat > 0.2:\n",
    "                metrics['poor_ks'].append((feat, ks_stat))\n",
    "        if not np.isnan(mae):\n",
    "            metrics['mae_values'].append(mae)\n",
    "        \n",
    "        metrics['per_feature'][feat] = {'ks': ks_stat, 'mae': mae}\n",
    "    \n",
    "    metrics['ks_mean'] = np.mean(metrics['ks_stats']) if metrics['ks_stats'] else np.nan\n",
    "    metrics['mae_mean'] = np.mean(metrics['mae_values']) if metrics['mae_values'] else np.nan\n",
    "    metrics['sparse_ks_mean'] = np.mean([x[1] for x in metrics['sparse_ks']]) if metrics['sparse_ks'] else np.nan\n",
    "    \n",
    "    real_corr = np.nan_to_num(np.corrcoef(real_flat.T), nan=0)\n",
    "    syn_corr = np.nan_to_num(np.corrcoef(syn_flat.T), nan=0)\n",
    "    metrics['corr_error'] = np.mean(np.abs(real_corr - syn_corr))\n",
    "    \n",
    "    metrics['overall_score'] = (0.4 * metrics['ks_mean'] + 0.3 * metrics['sparse_ks_mean'] + \n",
    "                                0.2 * min(metrics['mae_mean']/100, 1) + 0.1 * metrics['corr_error'])\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS - ANOMALY DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_test_original = post_process_synthetic_data(X_test, available_features, scaler, SPARSE_FEATURES)\n",
    "\n",
    "all_metrics = {}\n",
    "for name, syn_data in synthetic_processed.items():\n",
    "    metrics = calculate_metrics(X_test_original, syn_data, available_features, SPARSE_FEATURES)\n",
    "    all_metrics[name] = metrics\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  KS (all):      {metrics['ks_mean']:.4f}\")\n",
    "    print(f\"  KS (sparse):   {metrics['sparse_ks_mean']:.4f}\")\n",
    "    print(f\"  MAE:           {metrics['mae_mean']:.4f}\")\n",
    "    print(f\"  Corr Error:    {metrics['corr_error']:.4f}\")\n",
    "    print(f\"  Overall Score: {metrics['overall_score']:.4f}\")\n",
    "    if metrics['poor_ks']:\n",
    "        print(f\"  Poor KS (>0.2): {[f[0] for f in metrics['poor_ks']]}\")\n",
    "    else:\n",
    "        print(f\"  Poor KS (>0.2): None - ALL FEATURES BELOW THRESHOLD!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed sparse feature analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SPARSE FEATURE KS STATISTICS (Target: < 0.2)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original problematic features\n",
    "ORIGINAL_POOR = ['edit_distance_dict_3', 'imp_wd_spath', 'unique_as_path_max', 'origin_changes']\n",
    "\n",
    "for name, metrics in all_metrics.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(\"-\" * 55)\n",
    "    for feat, ks in sorted(metrics['sparse_ks'], key=lambda x: x[1], reverse=True):\n",
    "        status = \"✓\" if ks < 0.15 else \"~\" if ks < 0.2 else \"✗\"\n",
    "        highlight = \" <-- ORIGINAL PROBLEM\" if feat in ORIGINAL_POOR else \"\"\n",
    "        print(f\"  [{status}] {feat:25s}: KS={ks:.4f}{highlight}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model\n",
    "best_model = min(all_metrics.keys(), key=lambda k: all_metrics[k]['overall_score'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Overall Score: {all_metrics[best_model]['overall_score']:.4f}\")\n",
    "print(f\"  KS (all):      {all_metrics[best_model]['ks_mean']:.4f}\")\n",
    "print(f\"  KS (sparse):   {all_metrics[best_model]['sparse_ks_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 12: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for idx, (name, hist) in enumerate(histories.items()):\n",
    "    ax = axes[idx]\n",
    "    if 'g_loss' in hist:\n",
    "        ax.plot(hist['g_loss'], label='G', alpha=0.7)\n",
    "    if 'd_loss' in hist:\n",
    "        ax.plot(hist['d_loss'], label='D', alpha=0.7)\n",
    "    ax.set_title(f'{name} (Anomaly)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves_anomaly.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "models_list = list(all_metrics.keys())\n",
    "x = np.arange(len(models_list))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - width, [all_metrics[m]['ks_mean'] for m in models_list], width, label='KS (All)', alpha=0.8)\n",
    "ax.bar(x, [all_metrics[m]['sparse_ks_mean'] for m in models_list], width, label='KS (Sparse)', alpha=0.8)\n",
    "ax.bar(x + width, [all_metrics[m]['overall_score'] for m in models_list], width, label='Overall', alpha=0.8)\n",
    "\n",
    "ax.axhline(y=0.2, color='r', linestyle='--', label='Threshold (0.2)')\n",
    "ax.set_ylabel('Score (lower is better)')\n",
    "ax.set_title('Model Comparison - Anomaly Traffic Generation')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models_list)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison_anomaly.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparse feature distribution comparison (best model)\n",
    "real_flat = X_test_original.reshape(-1, NUM_FEATURES)\n",
    "syn_flat = synthetic_processed[best_model].reshape(-1, NUM_FEATURES)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "for i, feat in enumerate(SPARSE_FEATURES[:9]):\n",
    "    if feat in available_features:\n",
    "        idx = available_features.index(feat)\n",
    "        ax = axes[i//3, i%3]\n",
    "        ax.hist(real_flat[:, idx], bins=50, alpha=0.5, label='Real', density=True)\n",
    "        ax.hist(syn_flat[:, idx], bins=50, alpha=0.5, label=best_model, density=True)\n",
    "        ks = all_metrics[best_model]['per_feature'].get(feat, {}).get('ks', 0)\n",
    "        status = \"✓\" if ks < 0.2 else \"✗\"\n",
    "        ax.set_title(f'{feat}\\nKS={ks:.3f} {status}')\n",
    "        ax.legend(fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Sparse Features: Real vs {best_model} (Anomaly)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'sparse_features_anomaly.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 13: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save synthetic data for all models\n",
    "for name, syn_data in synthetic_processed.items():\n",
    "    n, s, f = syn_data.shape\n",
    "    rows = []\n",
    "    for seq_idx in range(n):\n",
    "        for t in range(s):\n",
    "            row = {'sequence_id': seq_idx, 'timestep': t, 'label': 'anomaly'}\n",
    "            for f_idx, feat in enumerate(available_features):\n",
    "                row[feat] = syn_data[seq_idx, t, f_idx]\n",
    "            rows.append(row)\n",
    "    \n",
    "    df_syn = pd.DataFrame(rows)\n",
    "    for feat in INTEGER_FEATURES:\n",
    "        if feat in df_syn.columns:\n",
    "            df_syn[feat] = df_syn[feat].astype(int)\n",
    "    \n",
    "    filename = f\"synthetic_{name.lower().replace('-', '_')}_anomaly_improved.csv\"\n",
    "    df_syn.to_csv(os.path.join(OUTPUT_DIR, filename), index=False)\n",
    "    print(f\"Saved {filename}: {len(df_syn):,} rows\")\n",
    "\n",
    "# Copy best to main output\n",
    "import shutil\n",
    "best_src = os.path.join(OUTPUT_DIR, f\"synthetic_{best_model.lower().replace('-', '_')}_anomaly_improved.csv\")\n",
    "best_dst = os.path.join(OUTPUT_DIR, 'synthetic_anomaly_traffic_best.csv')\n",
    "shutil.copy(best_src, best_dst)\n",
    "print(f\"\\nBest model saved as: synthetic_anomaly_traffic_best.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config and scaler\n",
    "with open(os.path.join(OUTPUT_DIR, 'scaler_anomaly.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "config = {\n",
    "    'version': 'improved_complete_v1_anomaly',\n",
    "    'data_type': 'anomaly',\n",
    "    'high_confidence_labels': HIGH_CONFIDENCE_LABELS,\n",
    "    'improvements': [\n",
    "        'log1p transform for sparse features',\n",
    "        'increased capacity (LATENT=64, LAYERS=3)',\n",
    "        'histogram loss',\n",
    "        'correlation loss',\n",
    "        'feature-weighted MSE',\n",
    "        'all three models improved'\n",
    "    ],\n",
    "    'sparse_features': SPARSE_FEATURES,\n",
    "    'loss_weights': {'sparse': SPARSE_WEIGHT, 'hist': HIST_LOSS_WEIGHT, 'corr': CORR_LOSS_WEIGHT},\n",
    "    'epochs': EPOCHS,\n",
    "    'model_params': {'hidden': HIDDEN_DIM, 'latent': LATENT_DIM, 'layers': NUM_LAYERS},\n",
    "    'best_model': best_model,\n",
    "    'metrics': {k: {kk: vv for kk, vv in v.items() if kk not in ['per_feature', 'sparse_ks', 'poor_ks']} \n",
    "                for k, v in all_metrics.items()},\n",
    "    'sparse_ks': {k: dict(v['sparse_ks']) for k, v in all_metrics.items()},\n",
    "    'poor_ks_features': {k: [f[0] for f in v['poor_ks']] for k, v in all_metrics.items()},\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'config_anomaly.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=str)\n",
    "print(\"Saved config_anomaly.json and scaler_anomaly.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE - IMPROVED ANOMALY TRAFFIC GENERATION (ALL 3 MODELS)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Data Configuration:\n",
    "  Data path: {DATA_PATH}\n",
    "  High confidence labels: {HIGH_CONFIDENCE_LABELS}\n",
    "  Training samples: {len(X_train):,}\n",
    "\n",
    "Improvements Applied to ALL models:\n",
    "  ✓ Log1p transform for sparse features\n",
    "  ✓ Increased capacity: HIDDEN={HIDDEN_DIM}, LATENT={LATENT_DIM}, LAYERS={NUM_LAYERS}\n",
    "  ✓ Histogram loss (weight={HIST_LOSS_WEIGHT})\n",
    "  ✓ Correlation loss (weight={CORR_LOSS_WEIGHT})\n",
    "  ✓ Feature-weighted MSE (sparse_weight={SPARSE_WEIGHT})\n",
    "  ✓ Extended training: {EPOCHS} epochs\n",
    "\n",
    "Results Summary:\n",
    "\"\"\")\n",
    "\n",
    "print(f\"{'Model':<15} {'KS (all)':<12} {'KS (sparse)':<12} {'Poor KS':<10} {'Overall':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for name, m in all_metrics.items():\n",
    "    poor = len(m['poor_ks'])\n",
    "    best_mark = \" <-- BEST\" if name == best_model else \"\"\n",
    "    print(f\"{name:<15} {m['ks_mean']:<12.4f} {m['sparse_ks_mean']:<12.4f} {poor:<10} {m['overall_score']:<10.4f}{best_mark}\")\n",
    "\n",
    "print(f\"\\nOriginal problem features: {ORIGINAL_POOR}\")\n",
    "print(f\"Best model poor KS: {[f[0] for f in all_metrics[best_model]['poor_ks']] if all_metrics[best_model]['poor_ks'] else 'NONE'}\")\n",
    "print(f\"\\nOutput: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "print(\"\\nFiles:\")\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1024\n",
    "    print(f\"  {f}: {size:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
