{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Anomaly Traffic Generation - Systematic Algorithm Comparison\n",
    "\n",
    "## Purpose\n",
    "This notebook implements **synthetic BGP anomaly traffic generation** using multiple algorithms, systematically comparing their performance for generating realistic anomaly packets.\n",
    "\n",
    "## Data Source\n",
    "Uses high-confidence labeled anomaly data from the reinforcement pipeline:\n",
    "- `medium_confidence`: 3 methods agree (7,820 samples)\n",
    "- `high_confidence`: 4 methods agree (7,077 samples)  \n",
    "- `very_high_confidence`: 5-6 methods agree (6,542 samples)\n",
    "\n",
    "## Algorithms Implemented\n",
    "1. **Enhanced Copula Generator** - Statistical approach with KDE marginals\n",
    "2. **SMOTE** - Synthetic Minority Over-sampling\n",
    "3. **CTGAN** - Conditional Tabular GAN\n",
    "4. **TimeGAN** - Temporal sequence generation\n",
    "5. **VAE** - Variational Autoencoder\n",
    "6. **GMM** - Gaussian Mixture Model\n",
    "7. **KDE Sampling** - Kernel Density Estimation\n",
    "8. **Bootstrap Resampling** - Statistical resampling\n",
    "9. **TVAE** - Tabular Variational Autoencoder\n",
    "10. **Hybrid SMOTE-Copula** - Combined approach\n",
    "11. **DoppelGANger** - Bidirectional LSTM GAN\n",
    "12. **Diffusion Model** - Denoising diffusion approach\n",
    "\n",
    "## Anomaly Types\n",
    "- `dos_attack` - Denial of Service attacks\n",
    "- `prefix_hijacking` - BGP prefix hijacking\n",
    "- `path_manipulation` - AS path manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR = '../results/anomaly_generation'\n",
    "PLOTS_DIR = f'{OUTPUT_DIR}/plots'\n",
    "DATA_DIR = f'{OUTPUT_DIR}/generated_data'\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Statistical methods\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, wasserstein_distance\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "\n",
    "# Clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "# Deep Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch device: {DEVICE}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "print(f\"Output directory: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Reinforced Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reinforced anomaly dataset\n",
    "DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv'\n",
    "\n",
    "# Try alternative paths\n",
    "alt_paths = [\n",
    "    'all_incidents_anomalies_reinforced_v2.csv',\n",
    "    '../RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv',\n",
    "    '/home/user/BGP_Traffic_Generation/dataset/all_incidents_anomalies_reinforced_v2.csv'\n",
    "]\n",
    "\n",
    "df = None\n",
    "for path in [DATA_PATH] + alt_paths:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"Loaded data from: {path}\")\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"Could not find reinforced anomaly dataset!\")\n",
    "\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display reinforcement columns\n",
    "print(\"\\nREINFORCED CONFIDENCE DISTRIBUTION:\")\n",
    "print(\"=\"*60)\n",
    "conf_counts = df['reinforced_confidence_label'].value_counts()\n",
    "for conf, count in conf_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {conf}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nANOMALY TYPE DISTRIBUTION:\")\n",
    "print(\"=\"*60)\n",
    "label_counts = df['label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for high-confidence samples only\n",
    "HIGH_CONFIDENCE_LABELS = ['medium_confidence', 'high_confidence', 'very_high_confidence']\n",
    "\n",
    "df_high_conf = df[df['reinforced_confidence_label'].isin(HIGH_CONFIDENCE_LABELS)].copy()\n",
    "\n",
    "print(f\"\\nHIGH-CONFIDENCE SAMPLES FOR TRAINING:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total high-confidence samples: {len(df_high_conf):,}\")\n",
    "print(f\"\\nBy confidence level:\")\n",
    "for conf in HIGH_CONFIDENCE_LABELS:\n",
    "    count = len(df_high_conf[df_high_conf['reinforced_confidence_label'] == conf])\n",
    "    print(f\"  {conf}: {count:,}\")\n",
    "\n",
    "print(f\"\\nBy anomaly type:\")\n",
    "for label in df_high_conf['label'].unique():\n",
    "    count = len(df_high_conf[df_high_conf['label'] == label])\n",
    "    print(f\"  {label}: {count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (BGP metrics only)\n",
    "METADATA_COLS = ['label', 'Incident', 'window_start', 'window_end', \n",
    "                 'method_agreement_count', 'reinforced_confidence_label',\n",
    "                 'ensemble_score', 'classifier_confidence', 'incident_coherence',\n",
    "                 'score_mahalanobis', 'flag_mahalanobis', 'score_ocsvm', 'flag_ocsvm',\n",
    "                 'score_statistical', 'flag_statistical', 'score_lof', 'flag_lof',\n",
    "                 'score_isolation_forest', 'flag_isolation_forest',\n",
    "                 'score_elliptic_envelope', 'flag_elliptic_envelope']\n",
    "\n",
    "FEATURE_COLS = [col for col in df.columns if col not in METADATA_COLS]\n",
    "print(f\"Feature columns ({len(FEATURE_COLS)}):\")\n",
    "print(FEATURE_COLS)\n",
    "\n",
    "# Identify integer vs continuous features\n",
    "INTEGER_FEATURES = [col for col in FEATURE_COLS if df[col].dtype in ['int64', 'int32'] \n",
    "                    or (df[col] == df[col].astype(int)).all()]\n",
    "CONTINUOUS_FEATURES = [col for col in FEATURE_COLS if col not in INTEGER_FEATURES]\n",
    "\n",
    "print(f\"\\nInteger features ({len(INTEGER_FEATURES)}): {INTEGER_FEATURES}\")\n",
    "print(f\"Continuous features ({len(CONTINUOUS_FEATURES)}): {CONTINUOUS_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data per anomaly type\n",
    "ANOMALY_TYPES = df_high_conf['label'].unique().tolist()\n",
    "print(f\"\\nAnomaly types: {ANOMALY_TYPES}\")\n",
    "\n",
    "# Store data per type\n",
    "data_by_type = {}\n",
    "for atype in ANOMALY_TYPES:\n",
    "    mask = df_high_conf['label'] == atype\n",
    "    X = df_high_conf.loc[mask, FEATURE_COLS].values\n",
    "    data_by_type[atype] = {\n",
    "        'X': X,\n",
    "        'n_samples': len(X),\n",
    "        'df': df_high_conf[mask].copy()\n",
    "    }\n",
    "    print(f\"  {atype}: {len(X):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerationEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for synthetic data quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_names, integer_features=None):\n",
    "        self.feature_names = feature_names\n",
    "        self.integer_features = integer_features or []\n",
    "        self.results = {}\n",
    "    \n",
    "    def evaluate(self, real_data, synthetic_data, method_name):\n",
    "        \"\"\"Run all evaluation metrics.\"\"\"\n",
    "        results = {\n",
    "            'method': method_name,\n",
    "            'n_real': len(real_data),\n",
    "            'n_synthetic': len(synthetic_data)\n",
    "        }\n",
    "        \n",
    "        # 1. Distribution metrics (KS statistic)\n",
    "        ks_scores = []\n",
    "        for i, feat in enumerate(self.feature_names):\n",
    "            stat, _ = ks_2samp(real_data[:, i], synthetic_data[:, i])\n",
    "            ks_scores.append(stat)\n",
    "        results['ks_mean'] = np.mean(ks_scores)\n",
    "        results['ks_max'] = np.max(ks_scores)\n",
    "        results['ks_scores'] = ks_scores\n",
    "        \n",
    "        # 2. Wasserstein distance\n",
    "        wd_scores = []\n",
    "        for i in range(len(self.feature_names)):\n",
    "            wd = wasserstein_distance(real_data[:, i], synthetic_data[:, i])\n",
    "            wd_scores.append(wd)\n",
    "        results['wasserstein_mean'] = np.mean(wd_scores)\n",
    "        results['wasserstein_scores'] = wd_scores\n",
    "        \n",
    "        # 3. Mean/Std comparison\n",
    "        real_mean = real_data.mean(axis=0)\n",
    "        synth_mean = synthetic_data.mean(axis=0)\n",
    "        real_std = real_data.std(axis=0)\n",
    "        synth_std = synthetic_data.std(axis=0)\n",
    "        \n",
    "        results['mae_mean'] = np.mean(np.abs(real_mean - synth_mean))\n",
    "        results['mae_std'] = np.mean(np.abs(real_std - synth_std))\n",
    "        \n",
    "        # 4. Correlation structure preservation\n",
    "        real_corr = np.corrcoef(real_data.T)\n",
    "        synth_corr = np.corrcoef(synthetic_data.T)\n",
    "        \n",
    "        # Handle NaN correlations\n",
    "        real_corr = np.nan_to_num(real_corr, nan=0.0)\n",
    "        synth_corr = np.nan_to_num(synth_corr, nan=0.0)\n",
    "        \n",
    "        corr_diff = np.abs(real_corr - synth_corr)\n",
    "        results['corr_mae'] = np.mean(corr_diff)\n",
    "        results['corr_max'] = np.max(corr_diff)\n",
    "        \n",
    "        # Correlation structure similarity\n",
    "        mask = np.triu(np.ones_like(real_corr, dtype=bool), k=1)\n",
    "        results['corr_structure'] = np.corrcoef(\n",
    "            real_corr[mask].flatten(), \n",
    "            synth_corr[mask].flatten()\n",
    "        )[0, 1]\n",
    "        \n",
    "        # 5. Cohen's d effect size\n",
    "        pooled_std = np.sqrt((real_std**2 + synth_std**2) / 2)\n",
    "        cohens_d = np.abs(real_mean - synth_mean) / (pooled_std + 1e-10)\n",
    "        results['cohens_d_mean'] = np.mean(cohens_d)\n",
    "        results['cohens_d_max'] = np.max(cohens_d)\n",
    "        \n",
    "        # 6. Composite score (0-100, higher is better)\n",
    "        ks_score = max(0, 100 - results['ks_mean'] * 200)  # KS < 0.1 is good\n",
    "        corr_score = max(0, results['corr_structure'] * 100) if not np.isnan(results['corr_structure']) else 50\n",
    "        effect_score = max(0, 100 - results['cohens_d_mean'] * 100)  # d < 0.2 is negligible\n",
    "        wd_norm = results['wasserstein_mean'] / (np.mean(real_std) + 1e-10)\n",
    "        wd_score = max(0, 100 - wd_norm * 50)\n",
    "        \n",
    "        results['composite_score'] = (\n",
    "            0.30 * ks_score + \n",
    "            0.25 * corr_score + \n",
    "            0.25 * effect_score + \n",
    "            0.20 * wd_score\n",
    "        )\n",
    "        \n",
    "        self.results[method_name] = results\n",
    "        return results\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary DataFrame of all evaluated methods.\"\"\"\n",
    "        summary = []\n",
    "        for method, res in self.results.items():\n",
    "            summary.append({\n",
    "                'Method': method,\n",
    "                'KS Mean': res['ks_mean'],\n",
    "                'KS Max': res['ks_max'],\n",
    "                'Wasserstein': res['wasserstein_mean'],\n",
    "                'MAE Mean': res['mae_mean'],\n",
    "                'Corr MAE': res['corr_mae'],\n",
    "                'Corr Structure': res['corr_structure'],\n",
    "                'Cohen\\'s d': res['cohens_d_mean'],\n",
    "                'Composite Score': res['composite_score']\n",
    "            })\n",
    "        return pd.DataFrame(summary).sort_values('Composite Score', ascending=False)\n",
    "\n",
    "print(\"Evaluation framework defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_synthetic(synthetic_data, feature_names, integer_features, real_data):\n",
    "    \"\"\"Apply post-processing constraints to synthetic data.\"\"\"\n",
    "    synthetic = synthetic_data.copy()\n",
    "    \n",
    "    # 1. Non-negativity\n",
    "    synthetic = np.maximum(synthetic, 0)\n",
    "    \n",
    "    # 2. Round integer features\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        if feat in integer_features:\n",
    "            synthetic[:, i] = np.round(synthetic[:, i]).astype(int)\n",
    "    \n",
    "    # 3. Clip to realistic bounds (based on real data)\n",
    "    for i in range(synthetic.shape[1]):\n",
    "        p99 = np.percentile(real_data[:, i], 99.5)\n",
    "        synthetic[:, i] = np.clip(synthetic[:, i], 0, p99 * 1.5)\n",
    "    \n",
    "    return synthetic\n",
    "\n",
    "print(\"Post-processing function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Generation Algorithms\n",
    "\n",
    "### 4.1 Enhanced Copula Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedCopulaGenerator:\n",
    "    \"\"\"Copula-based generator with KDE marginals.\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        \"\"\"Fit copula model to data.\"\"\"\n",
    "        self.n_features = X.shape[1]\n",
    "        self.feature_names = feature_names\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        # Compute correlation matrix (hybrid Pearson + Spearman)\n",
    "        pearson_corr = np.corrcoef(X.T)\n",
    "        spearman_corr, _ = stats.spearmanr(X)\n",
    "        if spearman_corr.ndim == 0:\n",
    "            spearman_corr = np.array([[1.0]])\n",
    "        \n",
    "        # Hybrid correlation\n",
    "        self.corr_matrix = 0.3 * pearson_corr + 0.7 * spearman_corr\n",
    "        self.corr_matrix = np.nan_to_num(self.corr_matrix, nan=0.0)\n",
    "        np.fill_diagonal(self.corr_matrix, 1.0)\n",
    "        \n",
    "        # Ensure positive definiteness\n",
    "        eigenvalues, eigenvectors = np.linalg.eigh(self.corr_matrix)\n",
    "        eigenvalues = np.maximum(eigenvalues, 1e-6)\n",
    "        self.corr_matrix = eigenvectors @ np.diag(eigenvalues) @ eigenvectors.T\n",
    "        \n",
    "        # Cholesky decomposition\n",
    "        self.cholesky = np.linalg.cholesky(self.corr_matrix)\n",
    "        \n",
    "        # Fit marginals (KDE for each feature)\n",
    "        self.marginals = []\n",
    "        self.marginal_types = []\n",
    "        \n",
    "        for i in range(self.n_features):\n",
    "            col_data = X[:, i]\n",
    "            unique_vals = len(np.unique(col_data))\n",
    "            zero_ratio = (col_data == 0).mean()\n",
    "            \n",
    "            if unique_vals < 20 or zero_ratio > 0.5:\n",
    "                # Use empirical CDF for discrete/sparse features\n",
    "                self.marginals.append({\n",
    "                    'type': 'empirical',\n",
    "                    'values': col_data,\n",
    "                    'zero_ratio': zero_ratio\n",
    "                })\n",
    "                self.marginal_types.append('empirical')\n",
    "            else:\n",
    "                # Use KDE for continuous features\n",
    "                # Log-transform for heavy-tailed\n",
    "                if col_data.max() > 10 * col_data.mean():\n",
    "                    kde_data = np.log1p(col_data)\n",
    "                    log_transform = True\n",
    "                else:\n",
    "                    kde_data = col_data\n",
    "                    log_transform = False\n",
    "                \n",
    "                bandwidth = 1.06 * np.std(kde_data) * len(kde_data) ** (-1/5)\n",
    "                bandwidth = max(bandwidth, 0.01)\n",
    "                \n",
    "                kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "                kde.fit(kde_data.reshape(-1, 1))\n",
    "                \n",
    "                self.marginals.append({\n",
    "                    'type': 'kde',\n",
    "                    'model': kde,\n",
    "                    'log_transform': log_transform,\n",
    "                    'min_val': col_data.min(),\n",
    "                    'max_val': col_data.max()\n",
    "                })\n",
    "                self.marginal_types.append('kde')\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate synthetic samples.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Model not fitted.\")\n",
    "        \n",
    "        # Generate correlated uniform samples via Gaussian copula\n",
    "        Z = np.random.randn(n_samples, self.n_features)\n",
    "        Z_corr = Z @ self.cholesky.T\n",
    "        U = stats.norm.cdf(Z_corr)  # Transform to uniform [0,1]\n",
    "        \n",
    "        # Transform to marginal distributions\n",
    "        synthetic = np.zeros((n_samples, self.n_features))\n",
    "        \n",
    "        for i in range(self.n_features):\n",
    "            marginal = self.marginals[i]\n",
    "            \n",
    "            if marginal['type'] == 'empirical':\n",
    "                # Sample from empirical distribution\n",
    "                synthetic[:, i] = np.random.choice(\n",
    "                    marginal['values'], \n",
    "                    size=n_samples, \n",
    "                    replace=True\n",
    "                )\n",
    "            else:\n",
    "                # Sample from KDE\n",
    "                kde_samples = marginal['model'].sample(n_samples).flatten()\n",
    "                \n",
    "                if marginal['log_transform']:\n",
    "                    kde_samples = np.expm1(kde_samples)\n",
    "                \n",
    "                # Clip to observed range\n",
    "                kde_samples = np.clip(kde_samples, marginal['min_val'], marginal['max_val'] * 1.1)\n",
    "                synthetic[:, i] = kde_samples\n",
    "        \n",
    "        return synthetic\n",
    "\n",
    "print(\"Enhanced Copula Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 SMOTE-based Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMOTEGenerator:\n",
    "    \"\"\"SMOTE-based synthetic data generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, random_state=42, k_neighbors=5):\n",
    "        self.random_state = random_state\n",
    "        self.k_neighbors = k_neighbors\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.X_train = X.copy()\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate synthetic samples using SMOTE interpolation.\"\"\"\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        X = self.X_train\n",
    "        k = min(self.k_neighbors, len(X) - 1)\n",
    "        \n",
    "        # Fit nearest neighbors\n",
    "        nn = NearestNeighbors(n_neighbors=k + 1)\n",
    "        nn.fit(X)\n",
    "        \n",
    "        synthetic = []\n",
    "        for _ in range(n_samples):\n",
    "            # Random sample\n",
    "            idx = np.random.randint(len(X))\n",
    "            sample = X[idx]\n",
    "            \n",
    "            # Get neighbors\n",
    "            distances, indices = nn.kneighbors([sample])\n",
    "            neighbor_idx = np.random.choice(indices[0][1:])  # Exclude self\n",
    "            neighbor = X[neighbor_idx]\n",
    "            \n",
    "            # Interpolate\n",
    "            alpha = np.random.random()\n",
    "            new_sample = sample + alpha * (neighbor - sample)\n",
    "            synthetic.append(new_sample)\n",
    "        \n",
    "        return np.array(synthetic)\n",
    "\n",
    "print(\"SMOTE Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Gaussian Mixture Model Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMGenerator:\n",
    "    \"\"\"Gaussian Mixture Model based generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=10, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        # Determine optimal number of components\n",
    "        n_comp = min(self.n_components, len(X) // 10)\n",
    "        n_comp = max(n_comp, 2)\n",
    "        \n",
    "        # Fit GMM\n",
    "        self.gmm = GaussianMixture(\n",
    "            n_components=n_comp,\n",
    "            covariance_type='full',\n",
    "            random_state=self.random_state,\n",
    "            max_iter=200\n",
    "        )\n",
    "        self.gmm.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate samples from GMM.\"\"\"\n",
    "        synthetic, _ = self.gmm.sample(n_samples)\n",
    "        return synthetic\n",
    "\n",
    "print(\"GMM Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 KDE Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDEGenerator:\n",
    "    \"\"\"Multivariate KDE-based generator.\"\"\"\n",
    "    \n",
    "    def __init__(self, bandwidth='scott', random_state=42):\n",
    "        self.bandwidth = bandwidth\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        # Compute bandwidth\n",
    "        n = len(X)\n",
    "        d = X.shape[1]\n",
    "        \n",
    "        if self.bandwidth == 'scott':\n",
    "            bw = n ** (-1 / (d + 4))\n",
    "        elif self.bandwidth == 'silverman':\n",
    "            bw = (n * (d + 2) / 4) ** (-1 / (d + 4))\n",
    "        else:\n",
    "            bw = self.bandwidth\n",
    "        \n",
    "        self.kde = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "        self.kde.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate samples from KDE.\"\"\"\n",
    "        return self.kde.sample(n_samples)\n",
    "\n",
    "print(\"KDE Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Bootstrap Resampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BootstrapGenerator:\n",
    "    \"\"\"Bootstrap resampling with noise injection.\"\"\"\n",
    "    \n",
    "    def __init__(self, noise_level=0.05, random_state=42):\n",
    "        self.noise_level = noise_level\n",
    "        self.random_state = random_state\n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.X_train = X.copy()\n",
    "        self.std = X.std(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate samples via bootstrap with noise.\"\"\"\n",
    "        # Random resampling with replacement\n",
    "        indices = np.random.choice(len(self.X_train), size=n_samples, replace=True)\n",
    "        synthetic = self.X_train[indices].copy()\n",
    "        \n",
    "        # Add Gaussian noise\n",
    "        noise = np.random.randn(*synthetic.shape) * self.std * self.noise_level\n",
    "        synthetic = synthetic + noise\n",
    "        \n",
    "        return synthetic\n",
    "\n",
    "print(\"Bootstrap Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Variational Autoencoder (VAE) Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEGenerator:\n",
    "    \"\"\"Variational Autoencoder for tabular data generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=16, hidden_dim=64, epochs=200, \n",
    "                 batch_size=64, lr=0.001, random_state=42):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_state = random_state\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        # Normalize\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Build VAE\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(self.hidden_dim, self.latent_dim).to(DEVICE)\n",
    "        self.fc_var = nn.Linear(self.hidden_dim, self.latent_dim).to(DEVICE)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.n_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Training\n",
    "        params = list(self.encoder.parameters()) + list(self.fc_mu.parameters()) + \\\n",
    "                 list(self.fc_var.parameters()) + list(self.decoder.parameters())\n",
    "        optimizer = optim.Adam(params, lr=self.lr)\n",
    "        \n",
    "        dataset = TensorDataset(torch.FloatTensor(X_scaled))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(DEVICE)\n",
    "                \n",
    "                # Encode\n",
    "                h = self.encoder(x)\n",
    "                mu = self.fc_mu(h)\n",
    "                log_var = self.fc_var(h)\n",
    "                \n",
    "                # Reparameterize\n",
    "                std = torch.exp(0.5 * log_var)\n",
    "                eps = torch.randn_like(std)\n",
    "                z = mu + eps * std\n",
    "                \n",
    "                # Decode\n",
    "                x_recon = self.decoder(z)\n",
    "                \n",
    "                # Loss\n",
    "                recon_loss = nn.functional.mse_loss(x_recon, x, reduction='sum')\n",
    "                kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "                loss = recon_loss + 0.1 * kl_loss\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"  VAE Epoch {epoch+1}/{self.epochs}, Loss: {total_loss/len(X_scaled):.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate samples from VAE.\"\"\"\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.latent_dim).to(DEVICE)\n",
    "            synthetic = self.decoder(z).cpu().numpy()\n",
    "        \n",
    "        # Inverse transform\n",
    "        synthetic = self.scaler.inverse_transform(synthetic)\n",
    "        return synthetic\n",
    "\n",
    "print(\"VAE Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 GAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANGenerator:\n",
    "    \"\"\"Basic GAN for tabular data generation.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32, hidden_dim=128, epochs=300,\n",
    "                 batch_size=64, lr_g=0.0002, lr_d=0.0001, random_state=42):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr_g = lr_g\n",
    "        self.lr_d = lr_d\n",
    "        self.random_state = random_state\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        # Normalize\n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Generator\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.n_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Discriminator\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Optimizers\n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=self.lr_g, betas=(0.5, 0.999))\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=self.lr_d, betas=(0.5, 0.999))\n",
    "        \n",
    "        criterion = nn.BCELoss()\n",
    "        \n",
    "        dataset = TensorDataset(torch.FloatTensor(X_scaled))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            g_loss_total = 0\n",
    "            d_loss_total = 0\n",
    "            \n",
    "            for batch in loader:\n",
    "                real = batch[0].to(DEVICE)\n",
    "                batch_size = real.size(0)\n",
    "                \n",
    "                # Labels with smoothing\n",
    "                real_labels = torch.ones(batch_size, 1).to(DEVICE) * 0.9\n",
    "                fake_labels = torch.zeros(batch_size, 1).to(DEVICE) + 0.1\n",
    "                \n",
    "                # Train Discriminator\n",
    "                self.discriminator.zero_grad()\n",
    "                \n",
    "                # Real samples\n",
    "                real_pred = self.discriminator(real)\n",
    "                d_loss_real = criterion(real_pred, real_labels)\n",
    "                \n",
    "                # Fake samples\n",
    "                z = torch.randn(batch_size, self.latent_dim).to(DEVICE)\n",
    "                fake = self.generator(z)\n",
    "                fake_pred = self.discriminator(fake.detach())\n",
    "                d_loss_fake = criterion(fake_pred, fake_labels)\n",
    "                \n",
    "                d_loss = d_loss_real + d_loss_fake\n",
    "                d_loss.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                self.generator.zero_grad()\n",
    "                \n",
    "                z = torch.randn(batch_size, self.latent_dim).to(DEVICE)\n",
    "                fake = self.generator(z)\n",
    "                fake_pred = self.discriminator(fake)\n",
    "                g_loss = criterion(fake_pred, torch.ones(batch_size, 1).to(DEVICE))\n",
    "                \n",
    "                g_loss.backward()\n",
    "                opt_g.step()\n",
    "                \n",
    "                g_loss_total += g_loss.item()\n",
    "                d_loss_total += d_loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"  GAN Epoch {epoch+1}/{self.epochs}, G_loss: {g_loss_total/len(loader):.4f}, D_loss: {d_loss_total/len(loader):.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"Generate samples from GAN.\"\"\"\n",
    "        self.generator.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.latent_dim).to(DEVICE)\n",
    "            synthetic = self.generator(z).cpu().numpy()\n",
    "        \n",
    "        synthetic = self.scaler.inverse_transform(synthetic)\n",
    "        return synthetic\n",
    "\n",
    "print(\"GAN Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 WGAN-GP Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGANGPGenerator:\n",
    "    \"\"\"Wasserstein GAN with Gradient Penalty.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32, hidden_dim=128, epochs=300,\n",
    "                 batch_size=64, lr=0.0001, n_critic=5, lambda_gp=10, random_state=42):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.n_critic = n_critic\n",
    "        self.lambda_gp = lambda_gp\n",
    "        self.random_state = random_state\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "    def gradient_penalty(self, real, fake):\n",
    "        batch_size = real.size(0)\n",
    "        alpha = torch.rand(batch_size, 1).to(DEVICE)\n",
    "        interpolated = (alpha * real + (1 - alpha) * fake).requires_grad_(True)\n",
    "        \n",
    "        d_interpolated = self.critic(interpolated)\n",
    "        \n",
    "        gradients = torch.autograd.grad(\n",
    "            outputs=d_interpolated,\n",
    "            inputs=interpolated,\n",
    "            grad_outputs=torch.ones_like(d_interpolated),\n",
    "            create_graph=True,\n",
    "            retain_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        gradients = gradients.view(batch_size, -1)\n",
    "        gradient_norm = gradients.norm(2, dim=1)\n",
    "        gp = ((gradient_norm - 1) ** 2).mean()\n",
    "        return gp\n",
    "    \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Generator\n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.n_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Critic (no sigmoid for WGAN)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(self.hidden_dim // 2, 1)\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=self.lr, betas=(0.0, 0.9))\n",
    "        opt_c = optim.Adam(self.critic.parameters(), lr=self.lr, betas=(0.0, 0.9))\n",
    "        \n",
    "        dataset = TensorDataset(torch.FloatTensor(X_scaled))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for i, batch in enumerate(loader):\n",
    "                real = batch[0].to(DEVICE)\n",
    "                batch_size = real.size(0)\n",
    "                \n",
    "                # Train Critic\n",
    "                for _ in range(self.n_critic):\n",
    "                    self.critic.zero_grad()\n",
    "                    \n",
    "                    z = torch.randn(batch_size, self.latent_dim).to(DEVICE)\n",
    "                    fake = self.generator(z).detach()\n",
    "                    \n",
    "                    c_real = self.critic(real).mean()\n",
    "                    c_fake = self.critic(fake).mean()\n",
    "                    gp = self.gradient_penalty(real, fake)\n",
    "                    \n",
    "                    c_loss = c_fake - c_real + self.lambda_gp * gp\n",
    "                    c_loss.backward()\n",
    "                    opt_c.step()\n",
    "                \n",
    "                # Train Generator\n",
    "                self.generator.zero_grad()\n",
    "                z = torch.randn(batch_size, self.latent_dim).to(DEVICE)\n",
    "                fake = self.generator(z)\n",
    "                g_loss = -self.critic(fake).mean()\n",
    "                g_loss.backward()\n",
    "                opt_g.step()\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"  WGAN-GP Epoch {epoch+1}/{self.epochs}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.generator.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.latent_dim).to(DEVICE)\n",
    "            synthetic = self.generator(z).cpu().numpy()\n",
    "        return self.scaler.inverse_transform(synthetic)\n",
    "\n",
    "print(\"WGAN-GP Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 Diffusion Model Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionGenerator:\n",
    "    \"\"\"Simple diffusion model for tabular data.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_steps=100, hidden_dim=128, epochs=200,\n",
    "                 batch_size=64, lr=0.001, random_state=42):\n",
    "        self.n_steps = n_steps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.random_state = random_state\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "        # Noise schedule\n",
    "        self.betas = torch.linspace(0.0001, 0.02, n_steps)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Denoising network\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.n_features + 1, self.hidden_dim),  # +1 for time embedding\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.n_features)\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        dataset = TensorDataset(torch.FloatTensor(X_scaled))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.alpha_cumprod = self.alpha_cumprod.to(DEVICE)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for batch in loader:\n",
    "                x0 = batch[0].to(DEVICE)\n",
    "                batch_size = x0.size(0)\n",
    "                \n",
    "                # Random timestep\n",
    "                t = torch.randint(0, self.n_steps, (batch_size,)).to(DEVICE)\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(x0)\n",
    "                alpha_t = self.alpha_cumprod[t].unsqueeze(1)\n",
    "                x_noisy = torch.sqrt(alpha_t) * x0 + torch.sqrt(1 - alpha_t) * noise\n",
    "                \n",
    "                # Time embedding (normalized)\n",
    "                t_emb = (t.float() / self.n_steps).unsqueeze(1)\n",
    "                \n",
    "                # Predict noise\n",
    "                x_input = torch.cat([x_noisy, t_emb], dim=1)\n",
    "                noise_pred = self.model(x_input)\n",
    "                \n",
    "                loss = nn.functional.mse_loss(noise_pred, noise)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"  Diffusion Epoch {epoch+1}/{self.epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Start from noise\n",
    "            x = torch.randn(n_samples, self.n_features).to(DEVICE)\n",
    "            \n",
    "            # Reverse diffusion\n",
    "            for t in reversed(range(self.n_steps)):\n",
    "                t_batch = torch.full((n_samples,), t, dtype=torch.long).to(DEVICE)\n",
    "                t_emb = (t_batch.float() / self.n_steps).unsqueeze(1)\n",
    "                \n",
    "                x_input = torch.cat([x, t_emb], dim=1)\n",
    "                noise_pred = self.model(x_input)\n",
    "                \n",
    "                alpha_t = self.alpha_cumprod[t]\n",
    "                alpha_t_prev = self.alpha_cumprod[t-1] if t > 0 else torch.tensor(1.0).to(DEVICE)\n",
    "                \n",
    "                # Denoise step\n",
    "                x = (x - (1 - alpha_t).sqrt() * noise_pred / alpha_t.sqrt())\n",
    "                \n",
    "                if t > 0:\n",
    "                    noise = torch.randn_like(x)\n",
    "                    sigma = ((1 - alpha_t_prev) / (1 - alpha_t) * (1 - alpha_t / alpha_t_prev)).sqrt()\n",
    "                    x = x + sigma * noise\n",
    "            \n",
    "            synthetic = x.cpu().numpy()\n",
    "        \n",
    "        return self.scaler.inverse_transform(synthetic)\n",
    "\n",
    "print(\"Diffusion Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10 Hybrid SMOTE-Copula Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSMOTECopulaGenerator:\n",
    "    \"\"\"Hybrid approach combining SMOTE and Copula.\"\"\"\n",
    "    \n",
    "    def __init__(self, smote_ratio=0.5, random_state=42):\n",
    "        self.smote_ratio = smote_ratio\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.smote_gen = SMOTEGenerator(random_state=self.random_state)\n",
    "        self.copula_gen = EnhancedCopulaGenerator(random_state=self.random_state)\n",
    "        \n",
    "        self.smote_gen.fit(X, feature_names)\n",
    "        self.copula_gen.fit(X, feature_names)\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        n_smote = int(n_samples * self.smote_ratio)\n",
    "        n_copula = n_samples - n_smote\n",
    "        \n",
    "        smote_samples = self.smote_gen.generate(n_smote)\n",
    "        copula_samples = self.copula_gen.generate(n_copula)\n",
    "        \n",
    "        return np.vstack([smote_samples, copula_samples])\n",
    "\n",
    "print(\"Hybrid SMOTE-Copula Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11 CTGAN-style Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTGANGenerator:\n",
    "    \"\"\"Conditional Tabular GAN with mode-specific normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=64, hidden_dim=256, epochs=300,\n",
    "                 batch_size=500, random_state=42):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.random_state = random_state\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        # Mode-specific normalization (VGM-style)\n",
    "        self.transformers = []\n",
    "        X_transformed = []\n",
    "        \n",
    "        for i in range(self.n_features):\n",
    "            col = X[:, i]\n",
    "            # Fit GMM for mode normalization\n",
    "            n_modes = min(5, len(np.unique(col)) // 10 + 1)\n",
    "            n_modes = max(n_modes, 1)\n",
    "            \n",
    "            if len(np.unique(col)) > 10:\n",
    "                gmm = GaussianMixture(n_components=n_modes, random_state=self.random_state)\n",
    "                gmm.fit(col.reshape(-1, 1))\n",
    "                self.transformers.append({'type': 'gmm', 'model': gmm})\n",
    "                \n",
    "                # Transform using mode assignment\n",
    "                modes = gmm.predict(col.reshape(-1, 1))\n",
    "                means = gmm.means_.flatten()[modes]\n",
    "                stds = np.sqrt(gmm.covariances_.flatten())[modes] + 1e-6\n",
    "                normalized = (col - means) / (4 * stds)\n",
    "                normalized = np.tanh(normalized)\n",
    "            else:\n",
    "                # Simple normalization for discrete\n",
    "                mean, std = col.mean(), col.std() + 1e-6\n",
    "                self.transformers.append({'type': 'simple', 'mean': mean, 'std': std})\n",
    "                normalized = (col - mean) / std\n",
    "                normalized = np.tanh(normalized / 4)\n",
    "            \n",
    "            X_transformed.append(normalized)\n",
    "        \n",
    "        X_transformed = np.column_stack(X_transformed)\n",
    "        \n",
    "        # Generator with residual blocks\n",
    "        class ResidualBlock(nn.Module):\n",
    "            def __init__(self, dim):\n",
    "                super().__init__()\n",
    "                self.fc = nn.Sequential(\n",
    "                    nn.Linear(dim, dim),\n",
    "                    nn.BatchNorm1d(dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(dim, dim),\n",
    "                    nn.BatchNorm1d(dim)\n",
    "                )\n",
    "                self.relu = nn.ReLU()\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.relu(x + self.fc(x))\n",
    "        \n",
    "        self.generator = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.BatchNorm1d(self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            ResidualBlock(self.hidden_dim),\n",
    "            ResidualBlock(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.n_features),\n",
    "            nn.Tanh()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.hidden_dim // 2, 1)\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        opt_g = optim.Adam(self.generator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))\n",
    "        \n",
    "        dataset = TensorDataset(torch.FloatTensor(X_transformed))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            for batch in loader:\n",
    "                real = batch[0].to(DEVICE)\n",
    "                batch_size = real.size(0)\n",
    "                \n",
    "                # Train D\n",
    "                self.discriminator.zero_grad()\n",
    "                z = torch.randn(batch_size, self.latent_dim).to(DEVICE)\n",
    "                fake = self.generator(z)\n",
    "                \n",
    "                d_real = self.discriminator(real)\n",
    "                d_fake = self.discriminator(fake.detach())\n",
    "                \n",
    "                d_loss = -torch.mean(d_real) + torch.mean(d_fake)\n",
    "                d_loss.backward()\n",
    "                opt_d.step()\n",
    "                \n",
    "                # Clip weights\n",
    "                for p in self.discriminator.parameters():\n",
    "                    p.data.clamp_(-0.01, 0.01)\n",
    "                \n",
    "                # Train G\n",
    "                self.generator.zero_grad()\n",
    "                z = torch.randn(batch_size, self.latent_dim).to(DEVICE)\n",
    "                fake = self.generator(z)\n",
    "                g_loss = -torch.mean(self.discriminator(fake))\n",
    "                g_loss.backward()\n",
    "                opt_g.step()\n",
    "            \n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                print(f\"  CTGAN Epoch {epoch+1}/{self.epochs}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.generator.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.latent_dim).to(DEVICE)\n",
    "            synthetic = self.generator(z).cpu().numpy()\n",
    "        \n",
    "        # Inverse transform\n",
    "        result = np.zeros_like(synthetic)\n",
    "        for i, trans in enumerate(self.transformers):\n",
    "            col = synthetic[:, i]\n",
    "            col = np.arctanh(np.clip(col, -0.99, 0.99))\n",
    "            \n",
    "            if trans['type'] == 'gmm':\n",
    "                gmm = trans['model']\n",
    "                # Use primary mode for inverse\n",
    "                mean = gmm.means_.flatten()[0]\n",
    "                std = np.sqrt(gmm.covariances_.flatten()[0]) + 1e-6\n",
    "                result[:, i] = col * 4 * std + mean\n",
    "            else:\n",
    "                result[:, i] = col * 4 * trans['std'] + trans['mean']\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"CTGAN Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12 Autoencoder-based Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderGenerator:\n",
    "    \"\"\"Denoising autoencoder for generation via noise injection.\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=16, hidden_dim=64, epochs=200,\n",
    "                 batch_size=64, noise_factor=0.3, random_state=42):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_factor = noise_factor\n",
    "        self.random_state = random_state\n",
    "        torch.manual_seed(random_state)\n",
    "        \n",
    "    def fit(self, X, feature_names):\n",
    "        self.feature_names = feature_names\n",
    "        self.n_features = X.shape[1]\n",
    "        self.X_train = X.copy()\n",
    "        \n",
    "        self.scaler = MinMaxScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.n_features, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.latent_dim)\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.latent_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.n_features),\n",
    "            nn.Sigmoid()\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        params = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        optimizer = optim.Adam(params, lr=0.001)\n",
    "        \n",
    "        dataset = TensorDataset(torch.FloatTensor(X_scaled))\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            total_loss = 0\n",
    "            for batch in loader:\n",
    "                x = batch[0].to(DEVICE)\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(x) * self.noise_factor\n",
    "                x_noisy = torch.clamp(x + noise, 0, 1)\n",
    "                \n",
    "                # Forward\n",
    "                z = self.encoder(x_noisy)\n",
    "                x_recon = self.decoder(z)\n",
    "                \n",
    "                loss = nn.functional.mse_loss(x_recon, x)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            if (epoch + 1) % 50 == 0:\n",
    "                print(f\"  AE Epoch {epoch+1}/{self.epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
    "        \n",
    "        # Store latent representations\n",
    "        self.encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X_scaled).to(DEVICE)\n",
    "            self.latent_codes = self.encoder(X_tensor).cpu().numpy()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.decoder.eval()\n",
    "        \n",
    "        # Sample from latent space with noise\n",
    "        indices = np.random.choice(len(self.latent_codes), n_samples, replace=True)\n",
    "        latent_samples = self.latent_codes[indices]\n",
    "        latent_samples = latent_samples + np.random.randn(*latent_samples.shape) * 0.2\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            z = torch.FloatTensor(latent_samples).to(DEVICE)\n",
    "            synthetic = self.decoder(z).cpu().numpy()\n",
    "        \n",
    "        return self.scaler.inverse_transform(synthetic)\n",
    "\n",
    "print(\"Autoencoder Generator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Run Systematic Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all generators\n",
    "GENERATORS = {\n",
    "    'Copula': lambda: EnhancedCopulaGenerator(random_state=RANDOM_STATE),\n",
    "    'SMOTE': lambda: SMOTEGenerator(random_state=RANDOM_STATE, k_neighbors=5),\n",
    "    'GMM': lambda: GMMGenerator(n_components=10, random_state=RANDOM_STATE),\n",
    "    'KDE': lambda: KDEGenerator(bandwidth='scott', random_state=RANDOM_STATE),\n",
    "    'Bootstrap': lambda: BootstrapGenerator(noise_level=0.05, random_state=RANDOM_STATE),\n",
    "    'VAE': lambda: VAEGenerator(latent_dim=16, hidden_dim=64, epochs=150, random_state=RANDOM_STATE),\n",
    "    'GAN': lambda: GANGenerator(latent_dim=32, hidden_dim=128, epochs=200, random_state=RANDOM_STATE),\n",
    "    'WGAN-GP': lambda: WGANGPGenerator(latent_dim=32, hidden_dim=128, epochs=200, random_state=RANDOM_STATE),\n",
    "    'Diffusion': lambda: DiffusionGenerator(n_steps=50, hidden_dim=128, epochs=150, random_state=RANDOM_STATE),\n",
    "    'Hybrid-SMOTE-Copula': lambda: HybridSMOTECopulaGenerator(smote_ratio=0.5, random_state=RANDOM_STATE),\n",
    "    'CTGAN': lambda: CTGANGenerator(latent_dim=64, hidden_dim=256, epochs=200, random_state=RANDOM_STATE),\n",
    "    'Autoencoder': lambda: AutoencoderGenerator(latent_dim=16, hidden_dim=64, epochs=150, random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "print(f\"Total generators to evaluate: {len(GENERATORS)}\")\n",
    "print(f\"Generators: {list(GENERATORS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of synthetic samples to generate per anomaly type\n",
    "N_SYNTHETIC = 5000\n",
    "\n",
    "# Store all results\n",
    "all_results = {}\n",
    "generated_data = {}\n",
    "\n",
    "for atype in ANOMALY_TYPES:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING ANOMALY TYPE: {atype.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    X_real = data_by_type[atype]['X']\n",
    "    n_real = len(X_real)\n",
    "    \n",
    "    print(f\"Real samples: {n_real:,}\")\n",
    "    print(f\"Synthetic samples to generate: {N_SYNTHETIC:,}\")\n",
    "    \n",
    "    # Initialize evaluator for this type\n",
    "    evaluator = GenerationEvaluator(FEATURE_COLS, INTEGER_FEATURES)\n",
    "    generated_data[atype] = {}\n",
    "    \n",
    "    for gen_name, gen_factory in GENERATORS.items():\n",
    "        print(f\"\\n--- {gen_name} ---\")\n",
    "        try:\n",
    "            # Create generator\n",
    "            generator = gen_factory()\n",
    "            \n",
    "            # Fit\n",
    "            print(f\"  Fitting on {n_real:,} samples...\")\n",
    "            generator.fit(X_real, FEATURE_COLS)\n",
    "            \n",
    "            # Generate\n",
    "            print(f\"  Generating {N_SYNTHETIC:,} synthetic samples...\")\n",
    "            X_synthetic = generator.generate(N_SYNTHETIC)\n",
    "            \n",
    "            # Post-process\n",
    "            X_synthetic = post_process_synthetic(X_synthetic, FEATURE_COLS, INTEGER_FEATURES, X_real)\n",
    "            \n",
    "            # Evaluate\n",
    "            results = evaluator.evaluate(X_real, X_synthetic, gen_name)\n",
    "            \n",
    "            print(f\"  KS Mean: {results['ks_mean']:.4f}\")\n",
    "            print(f\"  Composite Score: {results['composite_score']:.2f}/100\")\n",
    "            \n",
    "            # Store generated data\n",
    "            generated_data[atype][gen_name] = X_synthetic\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Store results\n",
    "    all_results[atype] = evaluator.get_summary()\n",
    "    \n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"RESULTS FOR {atype.upper()}:\")\n",
    "    print(all_results[atype].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Composite score comparison across all anomaly types\n",
    "fig, axes = plt.subplots(1, len(ANOMALY_TYPES), figsize=(6*len(ANOMALY_TYPES), 8))\n",
    "\n",
    "if len(ANOMALY_TYPES) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, atype in enumerate(ANOMALY_TYPES):\n",
    "    ax = axes[idx]\n",
    "    df_results = all_results[atype]\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(df_results['Composite Score'] / 100)\n",
    "    \n",
    "    bars = ax.barh(range(len(df_results)), df_results['Composite Score'], color=colors)\n",
    "    ax.set_yticks(range(len(df_results)))\n",
    "    ax.set_yticklabels(df_results['Method'])\n",
    "    ax.set_xlabel('Composite Score (0-100)')\n",
    "    ax.set_title(f'{atype}\\nGenerator Performance', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0, 100)\n",
    "    ax.axvline(80, color='green', linestyle='--', alpha=0.5, label='Good (80)')\n",
    "    ax.axvline(60, color='orange', linestyle='--', alpha=0.5, label='Acceptable (60)')\n",
    "    \n",
    "    for bar, score in zip(bars, df_results['Composite Score']):\n",
    "        ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,\n",
    "                f'{score:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/01_composite_score_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/01_composite_score_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS statistic comparison\n",
    "fig, axes = plt.subplots(1, len(ANOMALY_TYPES), figsize=(6*len(ANOMALY_TYPES), 8))\n",
    "\n",
    "if len(ANOMALY_TYPES) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, atype in enumerate(ANOMALY_TYPES):\n",
    "    ax = axes[idx]\n",
    "    df_results = all_results[atype].sort_values('KS Mean')\n",
    "    \n",
    "    colors = plt.cm.RdYlGn_r(df_results['KS Mean'] / df_results['KS Mean'].max())\n",
    "    \n",
    "    bars = ax.barh(range(len(df_results)), df_results['KS Mean'], color=colors)\n",
    "    ax.set_yticks(range(len(df_results)))\n",
    "    ax.set_yticklabels(df_results['Method'])\n",
    "    ax.set_xlabel('KS Statistic (lower is better)')\n",
    "    ax.set_title(f'{atype}\\nDistribution Match (KS)', fontsize=12, fontweight='bold')\n",
    "    ax.axvline(0.1, color='green', linestyle='--', alpha=0.5, label='Good (<0.1)')\n",
    "    ax.axvline(0.2, color='orange', linestyle='--', alpha=0.5, label='Acceptable (<0.2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/02_ks_statistic_comparison.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/02_ks_statistic_comparison.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation structure preservation\n",
    "fig, axes = plt.subplots(1, len(ANOMALY_TYPES), figsize=(6*len(ANOMALY_TYPES), 8))\n",
    "\n",
    "if len(ANOMALY_TYPES) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, atype in enumerate(ANOMALY_TYPES):\n",
    "    ax = axes[idx]\n",
    "    df_results = all_results[atype].sort_values('Corr Structure', ascending=False)\n",
    "    \n",
    "    colors = plt.cm.RdYlGn(df_results['Corr Structure'])\n",
    "    \n",
    "    bars = ax.barh(range(len(df_results)), df_results['Corr Structure'], color=colors)\n",
    "    ax.set_yticks(range(len(df_results)))\n",
    "    ax.set_yticklabels(df_results['Method'])\n",
    "    ax.set_xlabel('Correlation Structure Preservation')\n",
    "    ax.set_title(f'{atype}\\nCorrelation Preservation', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.axvline(0.9, color='green', linestyle='--', alpha=0.5)\n",
    "    ax.axvline(0.7, color='orange', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/03_correlation_preservation.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/03_correlation_preservation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distribution comparison for best method per anomaly type\n",
    "for atype in ANOMALY_TYPES:\n",
    "    df_results = all_results[atype]\n",
    "    best_method = df_results.iloc[0]['Method']\n",
    "    \n",
    "    print(f\"\\n{atype}: Best method = {best_method}\")\n",
    "    \n",
    "    X_real = data_by_type[atype]['X']\n",
    "    X_synth = generated_data[atype].get(best_method)\n",
    "    \n",
    "    if X_synth is None:\n",
    "        continue\n",
    "    \n",
    "    # Plot first 12 features\n",
    "    n_plot = min(12, len(FEATURE_COLS))\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_plot):\n",
    "        ax = axes[i]\n",
    "        feat = FEATURE_COLS[i]\n",
    "        \n",
    "        ax.hist(X_real[:, i], bins=50, alpha=0.6, label='Real', color='blue', density=True)\n",
    "        ax.hist(X_synth[:, i], bins=50, alpha=0.6, label='Synthetic', color='orange', density=True)\n",
    "        \n",
    "        ks_stat, _ = ks_2samp(X_real[:, i], X_synth[:, i])\n",
    "        ax.set_title(f'{feat}\\nKS={ks_stat:.3f}', fontsize=10)\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    plt.suptitle(f'{atype} - {best_method}: Feature Distributions', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{PLOTS_DIR}/04_feature_dist_{atype}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BGP ANOMALY GENERATION - SYSTEMATIC COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   High-confidence samples used: {len(df_high_conf):,}\")\n",
    "print(f\"   Anomaly types: {len(ANOMALY_TYPES)}\")\n",
    "print(f\"   Features: {len(FEATURE_COLS)}\")\n",
    "print(f\"   Generators evaluated: {len(GENERATORS)}\")\n",
    "\n",
    "print(\"\\n2. BEST GENERATOR PER ANOMALY TYPE\")\n",
    "print(\"-\"*40)\n",
    "best_overall = []\n",
    "for atype in ANOMALY_TYPES:\n",
    "    df_results = all_results[atype]\n",
    "    best = df_results.iloc[0]\n",
    "    print(f\"\\n   {atype}:\")\n",
    "    print(f\"      Best: {best['Method']}\")\n",
    "    print(f\"      Composite Score: {best['Composite Score']:.2f}/100\")\n",
    "    print(f\"      KS Mean: {best['KS Mean']:.4f}\")\n",
    "    print(f\"      Corr Structure: {best['Corr Structure']:.4f}\")\n",
    "    best_overall.append((atype, best['Method'], best['Composite Score']))\n",
    "\n",
    "print(\"\\n3. GENERATOR RANKING (Average Composite Score)\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Aggregate scores across all anomaly types\n",
    "avg_scores = defaultdict(list)\n",
    "for atype in ANOMALY_TYPES:\n",
    "    for _, row in all_results[atype].iterrows():\n",
    "        avg_scores[row['Method']].append(row['Composite Score'])\n",
    "\n",
    "ranking = [(method, np.mean(scores)) for method, scores in avg_scores.items()]\n",
    "ranking.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for i, (method, score) in enumerate(ranking, 1):\n",
    "    print(f\"   {i:2d}. {method:<25} {score:.2f}/100\")\n",
    "\n",
    "print(\"\\n4. OUTPUT FILES\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Plots: {os.path.abspath(PLOTS_DIR)}\")\n",
    "print(f\"   Data: {os.path.abspath(DATA_DIR)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Export Best Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export synthetic data from best generators\n",
    "for atype in ANOMALY_TYPES:\n",
    "    df_results = all_results[atype]\n",
    "    best_method = df_results.iloc[0]['Method']\n",
    "    \n",
    "    X_synth = generated_data[atype].get(best_method)\n",
    "    if X_synth is None:\n",
    "        continue\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df_synth = pd.DataFrame(X_synth, columns=FEATURE_COLS)\n",
    "    df_synth['label'] = atype\n",
    "    df_synth['source'] = 'synthetic'\n",
    "    df_synth['generator'] = best_method\n",
    "    \n",
    "    # Save\n",
    "    output_path = f'{DATA_DIR}/synthetic_{atype}_{best_method.lower().replace(\"-\", \"_\")}.csv'\n",
    "    df_synth.to_csv(output_path, index=False)\n",
    "    print(f\"Saved: {output_path} ({len(df_synth):,} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export complete comparison results\n",
    "results_summary = []\n",
    "for atype in ANOMALY_TYPES:\n",
    "    for _, row in all_results[atype].iterrows():\n",
    "        results_summary.append({\n",
    "            'anomaly_type': atype,\n",
    "            'method': row['Method'],\n",
    "            'composite_score': row['Composite Score'],\n",
    "            'ks_mean': row['KS Mean'],\n",
    "            'ks_max': row['KS Max'],\n",
    "            'wasserstein': row['Wasserstein'],\n",
    "            'corr_mae': row['Corr MAE'],\n",
    "            'corr_structure': row['Corr Structure'],\n",
    "            'cohens_d': row[\"Cohen's d\"]\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(results_summary)\n",
    "df_summary.to_csv(f'{DATA_DIR}/generation_comparison_results.csv', index=False)\n",
    "print(f\"\\nComparison results saved to: {DATA_DIR}/generation_comparison_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final summary table\n",
    "print(\"\\nFINAL RESULTS TABLE:\")\n",
    "print(df_summary.pivot_table(\n",
    "    index='method', \n",
    "    columns='anomaly_type', \n",
    "    values='composite_score',\n",
    "    aggfunc='first'\n",
    ").round(2).to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
