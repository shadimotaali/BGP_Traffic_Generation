{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup and GPU Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Setup - MUST RUN FIRST\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler  # Use MinMaxScaler for [0,1] range\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Feature Type Definitions (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FEATURE TYPE DEFINITIONS - Used for post-processing\n",
    "# ============================================================================\n",
    "\n",
    "# Integer features (counts - must be >= 0 and integer)\n",
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Continuous features (can be float but must be >= 0)\n",
    "CONTINUOUS_FEATURES = [\n",
    "    'edit_distance_avg',\n",
    "    'rare_ases_avg'\n",
    "]\n",
    "\n",
    "# All features\n",
    "ALL_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_avg', 'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'rare_ases_avg',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "print(f\"Integer features: {len(INTEGER_FEATURES)}\")\n",
    "print(f\"Continuous features: {len(CONTINUOUS_FEATURES)}\")\n",
    "print(f\"Total: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# POST-PROCESSING FUNCTION (CRITICAL)\n",
    "# ============================================================================\n",
    "\n",
    "def post_process_synthetic_data(synthetic_data, feature_names, scaler):\n",
    "    \"\"\"\n",
    "    Post-process synthetic data:\n",
    "    1. Inverse transform (denormalize)\n",
    "    2. Clip negative values to 0\n",
    "    3. Round integer features\n",
    "    \n",
    "    Args:\n",
    "        synthetic_data: numpy array (n_samples, seq_len, n_features) - NORMALIZED\n",
    "        feature_names: list of feature names\n",
    "        scaler: fitted MinMaxScaler\n",
    "    \n",
    "    Returns:\n",
    "        processed_data: numpy array with proper constraints\n",
    "    \"\"\"\n",
    "    n_samples, seq_len, n_features = synthetic_data.shape\n",
    "    \n",
    "    # Step 1: Flatten for inverse transform\n",
    "    flat_data = synthetic_data.reshape(-1, n_features)\n",
    "    \n",
    "    # Step 2: Inverse transform (denormalize)\n",
    "    denorm_data = scaler.inverse_transform(flat_data)\n",
    "    \n",
    "    # Step 3: Apply constraints per feature\n",
    "    for i, feat in enumerate(feature_names):\n",
    "        # Clip all features to >= 0 (all BGP features are non-negative)\n",
    "        denorm_data[:, i] = np.maximum(denorm_data[:, i], 0)\n",
    "        \n",
    "        # Round integer features\n",
    "        if feat in INTEGER_FEATURES:\n",
    "            denorm_data[:, i] = np.round(denorm_data[:, i]).astype(int)\n",
    "    \n",
    "    # Step 4: Reshape back\n",
    "    processed_data = denorm_data.reshape(n_samples, seq_len, n_features)\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "\n",
    "def validate_synthetic_data(synthetic_data, feature_names):\n",
    "    \"\"\"\n",
    "    Validate that synthetic data meets constraints.\n",
    "    \"\"\"\n",
    "    flat = synthetic_data.reshape(-1, len(feature_names))\n",
    "    \n",
    "    print(\"\\nValidation Report:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        col = flat[:, i]\n",
    "        min_val = col.min()\n",
    "        max_val = col.max()\n",
    "        has_negative = min_val < 0\n",
    "        has_float = not np.allclose(col, np.round(col)) if feat in INTEGER_FEATURES else False\n",
    "        \n",
    "        status = \"✓\" if not has_negative and not has_float else \"✗\"\n",
    "        \n",
    "        if has_negative or has_float:\n",
    "            issues.append(feat)\n",
    "            print(f\"{status} {feat}: min={min_val:.2f}, max={max_val:.2f}, \"\n",
    "                  f\"neg={has_negative}, float={has_float}\")\n",
    "    \n",
    "    if not issues:\n",
    "        print(\"✓ All features passed validation!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ {len(issues)} features have issues: {issues}\")\n",
    "    \n",
    "    return len(issues) == 0\n",
    "\n",
    "print(\"Post-processing functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION FOR ANOMALY TRAFFIC GENERATION\n",
    "# ============================================================================\n",
    "\n",
    "# Paths - ANOMALY DATA\n",
    "DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv'\n",
    "OUTPUT_DIR = '/home/smotaali/BGP_Traffic_Generation/results/gan_anomaly_outputs/'\n",
    "\n",
    "# High confidence anomaly labels to filter\n",
    "HIGH_CONFIDENCE_LABELS = ['medium_confidence', 'high_confidence', 'very_high_confidence']\n",
    "\n",
    "# Sequence parameters\n",
    "SEQ_LEN = 30\n",
    "STRIDE = 1\n",
    "\n",
    "# ============================================================================\n",
    "# FIXED HYPERPARAMETERS (based on best practices)\n",
    "# ============================================================================\n",
    "BATCH_SIZE = 64  # Smaller batch for stability\n",
    "EPOCHS = 150\n",
    "\n",
    "# SEPARATE LEARNING RATES (key fix!)\n",
    "LR_G = 0.0002  # Generator learning rate\n",
    "LR_D = 0.0001  # Discriminator learning rate (LOWER to prevent D from dominating)\n",
    "\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "\n",
    "# Model architecture\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 32\n",
    "NUM_LAYERS = 2  # Reduced for stability\n",
    "\n",
    "# Training stability\n",
    "LABEL_SMOOTHING = 0.1  # Smooth labels: real=0.9, fake=0.1\n",
    "NOISE_STD = 0.05  # Add noise to discriminator inputs\n",
    "N_CRITIC = 1  # Train D only once per G update\n",
    "CLIP_VALUE = 1.0  # Gradient clipping\n",
    "\n",
    "# Data split\n",
    "TEST_SIZE = 0.2\n",
    "VAL_SIZE = 0.1\n",
    "\n",
    "# Generation\n",
    "N_SYNTHETIC = 2000\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(\"Configuration set for ANOMALY traffic generation.\")\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "print(f\"Output dir: {OUTPUT_DIR}\")\n",
    "print(f\"High confidence labels: {HIGH_CONFIDENCE_LABELS}\")\n",
    "print(f\"LR_G: {LR_G}, LR_D: {LR_D}\")\n",
    "print(f\"Label smoothing: {LABEL_SMOOTHING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading anomaly data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "\n",
    "# Check available columns\n",
    "print(f\"\\nAvailable columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check confidence_label distribution\n",
    "if 'confidence_label' in df.columns:\n",
    "    print(f\"\\nConfidence labels distribution:\")\n",
    "    print(df['confidence_label'].value_counts())\n",
    "\n",
    "# Filter high confidence anomalies\n",
    "df_anomaly = df[df['confidence_label'].isin(HIGH_CONFIDENCE_LABELS)].copy()\n",
    "print(f\"\\nHigh confidence anomaly rows: {len(df_anomaly):,}\")\n",
    "\n",
    "# Select features\n",
    "available_features = [f for f in ALL_FEATURES if f in df_anomaly.columns]\n",
    "print(f\"Available features: {len(available_features)}\")\n",
    "\n",
    "# Get feature indices for post-processing\n",
    "integer_indices = [available_features.index(f) for f in INTEGER_FEATURES if f in available_features]\n",
    "continuous_indices = [available_features.index(f) for f in CONTINUOUS_FEATURES if f in available_features]\n",
    "\n",
    "# Extract data\n",
    "data = df_anomaly[available_features].fillna(0).values\n",
    "NUM_FEATURES = len(available_features)\n",
    "print(f\"Data shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show original data statistics (BEFORE normalization)\n",
    "print(\"\\nOriginal Anomaly Data Statistics (BEFORE normalization):\")\n",
    "print(\"-\" * 60)\n",
    "for i, feat in enumerate(available_features[:10]):  # Show first 10\n",
    "    print(f\"{feat}: min={data[:, i].min():.2f}, max={data[:, i].max():.2f}, \"\n",
    "          f\"mean={data[:, i].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize using MinMaxScaler (scales to [0, 1])\n",
    "# This allows us to use Sigmoid output in generator\n",
    "print(\"\\nNormalizing with MinMaxScaler [0, 1]...\")\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_norm = scaler.fit_transform(data)\n",
    "\n",
    "print(f\"Normalized range: [{data_norm.min():.4f}, {data_norm.max():.4f}]\")\n",
    "print(f\"Mean: {data_norm.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences\n",
    "print(f\"\\nCreating sequences (T={SEQ_LEN}, stride={STRIDE})...\")\n",
    "sequences = []\n",
    "for i in range(0, len(data_norm) - SEQ_LEN + 1, STRIDE):\n",
    "    sequences.append(data_norm[i:i + SEQ_LEN])\n",
    "sequences = np.array(sequences)\n",
    "print(f\"Sequences: {sequences.shape}\")\n",
    "\n",
    "# Split\n",
    "X_train_val, X_test = train_test_split(sequences, test_size=TEST_SIZE, random_state=SEED)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=VAL_SIZE, random_state=SEED)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]:,}\")\n",
    "print(f\"Val: {X_val.shape[0]:,}\")\n",
    "print(f\"Test: {X_test.shape[0]:,}\")\n",
    "\n",
    "# DataLoaders\n",
    "train_tensor = torch.FloatTensor(X_train).to(device)\n",
    "train_loader = DataLoader(TensorDataset(train_tensor), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "print(f\"Batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIXED LSTM-GAN with Sigmoid output and stability improvements\n",
    "# ============================================================================\n",
    "\n",
    "class LSTM_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()  # OUTPUT IN [0, 1] - matches MinMaxScaler\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        batch_size, seq_len, _ = z.shape\n",
    "        out, _ = self.lstm(z)\n",
    "        # Reshape for batch norm\n",
    "        out = out.contiguous().view(-1, out.size(-1))\n",
    "        out = self.fc(out)\n",
    "        return out.view(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "class LSTM_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),  # More dropout\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "            # No sigmoid - using BCEWithLogitsLoss\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "\n",
    "class FixedLSTMGAN:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, seq_len, device):\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.G = LSTM_Generator(latent_dim, hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.D = LSTM_Discriminator(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def train(self, train_loader, epochs, lr_g=0.0002, lr_d=0.0001, \n",
    "              label_smoothing=0.1, noise_std=0.05):\n",
    "        \"\"\"\n",
    "        Train with stability improvements:\n",
    "        - Separate learning rates\n",
    "        - Label smoothing\n",
    "        - Noise injection to D\n",
    "        - Gradient clipping\n",
    "        \"\"\"\n",
    "        opt_g = optim.Adam(self.G.parameters(), lr=lr_g, betas=(BETA1, BETA2))\n",
    "        opt_d = optim.Adam(self.D.parameters(), lr=lr_d, betas=(BETA1, BETA2))\n",
    "        \n",
    "        # Learning rate schedulers\n",
    "        scheduler_g = optim.lr_scheduler.StepLR(opt_g, step_size=50, gamma=0.9)\n",
    "        scheduler_d = optim.lr_scheduler.StepLR(opt_d, step_size=50, gamma=0.9)\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': [], 'd_real': [], 'd_fake': []}\n",
    "        start = time.time()\n",
    "        \n",
    "        # Smoothed labels\n",
    "        real_label = 1.0 - label_smoothing  # 0.9\n",
    "        fake_label = label_smoothing  # 0.1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            g_losses, d_losses = [], []\n",
    "            d_reals, d_fakes = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                real = batch[0]\n",
    "                bs = real.size(0)\n",
    "                \n",
    "                # Smoothed labels\n",
    "                real_labels = torch.full((bs, 1), real_label).to(self.device)\n",
    "                fake_labels = torch.full((bs, 1), fake_label).to(self.device)\n",
    "                \n",
    "                # =====================\n",
    "                # Train Discriminator\n",
    "                # =====================\n",
    "                opt_d.zero_grad()\n",
    "                \n",
    "                # Add noise to real data (instance noise)\n",
    "                real_noisy = real + noise_std * torch.randn_like(real)\n",
    "                real_noisy = torch.clamp(real_noisy, 0, 1)  # Keep in [0,1]\n",
    "                \n",
    "                d_real_out = self.D(real_noisy)\n",
    "                loss_real = self.criterion(d_real_out, real_labels)\n",
    "                \n",
    "                # Generate fake\n",
    "                z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                fake = self.G(z)\n",
    "                \n",
    "                # Add noise to fake\n",
    "                fake_noisy = fake.detach() + noise_std * torch.randn_like(fake)\n",
    "                fake_noisy = torch.clamp(fake_noisy, 0, 1)\n",
    "                \n",
    "                d_fake_out = self.D(fake_noisy)\n",
    "                loss_fake = self.criterion(d_fake_out, fake_labels)\n",
    "                \n",
    "                d_loss = loss_real + loss_fake\n",
    "                d_loss.backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(self.D.parameters(), CLIP_VALUE)\n",
    "                opt_d.step()\n",
    "                \n",
    "                # =====================\n",
    "                # Train Generator (train more when D is strong)\n",
    "                # =====================\n",
    "                n_g_steps = 2 if d_loss.item() < 0.5 else 1  # Train G more when D is too good\n",
    "                \n",
    "                for _ in range(n_g_steps):\n",
    "                    opt_g.zero_grad()\n",
    "                    z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                    fake = self.G(z)\n",
    "                    d_fake_out = self.D(fake)\n",
    "                    \n",
    "                    # Generator wants D to output real_label\n",
    "                    g_loss = self.criterion(d_fake_out, real_labels)\n",
    "                    \n",
    "                    # Feature matching loss (stabilizes training)\n",
    "                    real_mean = real.mean(dim=[0, 1])\n",
    "                    fake_mean = fake.mean(dim=[0, 1])\n",
    "                    fm_loss = torch.mean((real_mean - fake_mean) ** 2)\n",
    "                    \n",
    "                    total_g_loss = g_loss + 0.1 * fm_loss\n",
    "                    total_g_loss.backward()\n",
    "                    \n",
    "                    torch.nn.utils.clip_grad_norm_(self.G.parameters(), CLIP_VALUE)\n",
    "                    opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "                d_reals.append(torch.sigmoid(d_real_out).mean().item())\n",
    "                d_fakes.append(torch.sigmoid(d_fake_out).mean().item())\n",
    "            \n",
    "            scheduler_g.step()\n",
    "            scheduler_d.step()\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            history['d_real'].append(np.mean(d_reals))\n",
    "            history['d_fake'].append(np.mean(d_fakes))\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{epochs} | D: {history['d_loss'][-1]:.4f} | \"\n",
    "                      f\"G: {history['g_loss'][-1]:.4f} | D(real): {history['d_real'][-1]:.2f} | \"\n",
    "                      f\"D(fake): {history['d_fake'][-1]:.2f}\")\n",
    "        \n",
    "        print(f\"  Training time: {(time.time()-start)/60:.1f} min\")\n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.seq_len, self.latent_dim).to(self.device)\n",
    "            fake = self.G(z)\n",
    "        return fake.cpu().numpy()\n",
    "\n",
    "print(\"Fixed LSTM-GAN defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIXED TimeGAN\n",
    "# ============================================================================\n",
    "\n",
    "class TimeGAN_Embedder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h, _ = self.rnn(x)\n",
    "        return torch.sigmoid(self.fc(h))\n",
    "\n",
    "\n",
    "class TimeGAN_Recovery(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, h):\n",
    "        r, _ = self.rnn(h)\n",
    "        return self.fc(r)\n",
    "\n",
    "\n",
    "class TimeGAN_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(latent_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        e, _ = self.rnn(z)\n",
    "        return torch.sigmoid(self.fc(e))\n",
    "\n",
    "\n",
    "class TimeGAN_Supervisor(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, max(1, num_layers - 1), batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        s, _ = self.rnn(h)\n",
    "        return torch.sigmoid(self.fc(s))\n",
    "\n",
    "\n",
    "class TimeGAN_Discriminator(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        d, _ = self.rnn(h)\n",
    "        return self.fc(d)\n",
    "\n",
    "\n",
    "class FixedTimeGAN:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, device):\n",
    "        self.device = device\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.embedder = TimeGAN_Embedder(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.recovery = TimeGAN_Recovery(hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.generator = TimeGAN_Generator(latent_dim, hidden_dim, num_layers).to(device)\n",
    "        self.supervisor = TimeGAN_Supervisor(hidden_dim, num_layers).to(device)\n",
    "        self.discriminator = TimeGAN_Discriminator(hidden_dim, num_layers).to(device)\n",
    "        \n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train(self, train_loader, epochs, lr_g=0.0002, lr_d=0.0001, label_smoothing=0.1):\n",
    "        # Optimizers\n",
    "        opt_e = optim.Adam(list(self.embedder.parameters()) + list(self.recovery.parameters()), lr=lr_g)\n",
    "        opt_s = optim.Adam(self.supervisor.parameters(), lr=lr_g)\n",
    "        opt_g = optim.Adam(list(self.generator.parameters()) + list(self.supervisor.parameters()), lr=lr_g)\n",
    "        opt_d = optim.Adam(self.discriminator.parameters(), lr=lr_d)  # Slower D\n",
    "        \n",
    "        history = {'e_loss': [], 's_loss': [], 'g_loss': [], 'd_loss': []}\n",
    "        start = time.time()\n",
    "        \n",
    "        real_label = 1.0 - label_smoothing\n",
    "        fake_label = label_smoothing\n",
    "        \n",
    "        # Phase 1: Embedding\n",
    "        print(\"  Phase 1: Embedding...\")\n",
    "        for epoch in range(epochs // 3):\n",
    "            e_losses = []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0]\n",
    "                opt_e.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                x_tilde = self.recovery(h)\n",
    "                loss = self.mse(x, x_tilde)\n",
    "                loss.backward()\n",
    "                opt_e.step()\n",
    "                e_losses.append(loss.item())\n",
    "            history['e_loss'].append(np.mean(e_losses))\n",
    "            if (epoch + 1) % 15 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs//3} | E_loss: {history['e_loss'][-1]:.4f}\")\n",
    "        \n",
    "        # Phase 2: Supervised\n",
    "        print(\"  Phase 2: Supervised...\")\n",
    "        for epoch in range(epochs // 3):\n",
    "            s_losses = []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0]\n",
    "                opt_s.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                h_s = self.supervisor(h)\n",
    "                loss = self.mse(h[:, 1:, :], h_s[:, :-1, :])\n",
    "                loss.backward()\n",
    "                opt_s.step()\n",
    "                s_losses.append(loss.item())\n",
    "            history['s_loss'].append(np.mean(s_losses))\n",
    "            if (epoch + 1) % 15 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs//3} | S_loss: {history['s_loss'][-1]:.4f}\")\n",
    "        \n",
    "        # Phase 3: Joint\n",
    "        print(\"  Phase 3: Joint...\")\n",
    "        for epoch in range(epochs // 3):\n",
    "            g_losses, d_losses = [], []\n",
    "            for batch in train_loader:\n",
    "                x = batch[0]\n",
    "                bs, seq_len, _ = x.shape\n",
    "                z = torch.randn(bs, seq_len, self.latent_dim).to(self.device)\n",
    "                \n",
    "                real_labels = torch.full((bs, seq_len, 1), real_label).to(self.device)\n",
    "                fake_labels = torch.full((bs, seq_len, 1), fake_label).to(self.device)\n",
    "                \n",
    "                # Generator (train 2x when D is strong)\n",
    "                for _ in range(2):\n",
    "                    opt_g.zero_grad()\n",
    "                    h = self.embedder(x)\n",
    "                    h_s = self.supervisor(h)\n",
    "                    e_hat = self.generator(z)\n",
    "                    h_hat = self.supervisor(e_hat)\n",
    "                    x_hat = self.recovery(h_hat)\n",
    "                    \n",
    "                    y_fake = self.discriminator(h_hat)\n",
    "                    g_loss = self.bce(y_fake, real_labels)\n",
    "                    g_loss += self.mse(h[:, 1:, :], h_s[:, :-1, :]) * 10\n",
    "                    \n",
    "                    # Feature matching\n",
    "                    g_loss += torch.mean((x_hat.mean(dim=0) - x.mean(dim=0)) ** 2) * 100\n",
    "                    \n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.generator.parameters(), CLIP_VALUE)\n",
    "                    opt_g.step()\n",
    "                \n",
    "                # Discriminator\n",
    "                opt_d.zero_grad()\n",
    "                h = self.embedder(x)\n",
    "                e_hat = self.generator(z)\n",
    "                h_hat = self.supervisor(e_hat)\n",
    "                \n",
    "                y_real = self.discriminator(h)\n",
    "                y_fake = self.discriminator(h_hat.detach())\n",
    "                \n",
    "                d_loss = self.bce(y_real, real_labels) + self.bce(y_fake, fake_labels)\n",
    "                \n",
    "                # Only update D if not too strong\n",
    "                if d_loss.item() > 0.3:\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), CLIP_VALUE)\n",
    "                    opt_d.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 15 == 0:\n",
    "                print(f\"    Epoch {epoch+1}/{epochs//3} | D: {history['d_loss'][-1]:.4f} | \"\n",
    "                      f\"G: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        print(f\"  Training time: {(time.time()-start)/60:.1f} min\")\n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples, seq_len):\n",
    "        self.generator.eval()\n",
    "        self.supervisor.eval()\n",
    "        self.recovery.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, seq_len, self.latent_dim).to(self.device)\n",
    "            e_hat = self.generator(z)\n",
    "            h_hat = self.supervisor(e_hat)\n",
    "            x_hat = self.recovery(h_hat)\n",
    "        return x_hat.cpu().numpy()\n",
    "\n",
    "print(\"Fixed TimeGAN defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FIXED DoppelGANger\n",
    "# ============================================================================\n",
    "\n",
    "class DG_Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()  # Output in [0, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        out, _ = self.lstm(z)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "class DG_Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "\n",
    "class FixedDoppelGANger:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, num_layers, seq_len, device):\n",
    "        self.device = device\n",
    "        self.latent_dim = latent_dim\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.G = DG_Generator(latent_dim, hidden_dim, input_dim, num_layers).to(device)\n",
    "        self.D = DG_Discriminator(input_dim, hidden_dim, num_layers).to(device)\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def train(self, train_loader, epochs, lr_g=0.0002, lr_d=0.0001, label_smoothing=0.1):\n",
    "        opt_g = optim.Adam(self.G.parameters(), lr=lr_g, betas=(BETA1, BETA2))\n",
    "        opt_d = optim.Adam(self.D.parameters(), lr=lr_d, betas=(BETA1, BETA2))\n",
    "        \n",
    "        history = {'g_loss': [], 'd_loss': []}\n",
    "        start = time.time()\n",
    "        \n",
    "        real_label = 1.0 - label_smoothing\n",
    "        fake_label = label_smoothing\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            g_losses, d_losses = [], []\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                real = batch[0]\n",
    "                bs = real.size(0)\n",
    "                \n",
    "                real_labels = torch.full((bs, 1), real_label).to(self.device)\n",
    "                fake_labels = torch.full((bs, 1), fake_label).to(self.device)\n",
    "                \n",
    "                # Train D\n",
    "                opt_d.zero_grad()\n",
    "                d_real = self.D(real)\n",
    "                loss_real = self.criterion(d_real, real_labels)\n",
    "                \n",
    "                z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                fake = self.G(z)\n",
    "                d_fake = self.D(fake.detach())\n",
    "                loss_fake = self.criterion(d_fake, fake_labels)\n",
    "                \n",
    "                d_loss = loss_real + loss_fake\n",
    "                \n",
    "                # Only update if not too strong\n",
    "                if d_loss.item() > 0.3:\n",
    "                    d_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.D.parameters(), CLIP_VALUE)\n",
    "                    opt_d.step()\n",
    "                \n",
    "                # Train G (more times when D is strong)\n",
    "                n_g = 2 if d_loss.item() < 0.5 else N_CRITIC\n",
    "                for _ in range(n_g):\n",
    "                    opt_g.zero_grad()\n",
    "                    z = torch.randn(bs, self.seq_len, self.latent_dim).to(self.device)\n",
    "                    fake = self.G(z)\n",
    "                    d_fake = self.D(fake)\n",
    "                    g_loss = self.criterion(d_fake, real_labels)\n",
    "                    \n",
    "                    # Feature matching\n",
    "                    g_loss += torch.mean((fake.mean(dim=0) - real.mean(dim=0)) ** 2) * 10\n",
    "                    \n",
    "                    g_loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.G.parameters(), CLIP_VALUE)\n",
    "                    opt_g.step()\n",
    "                \n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "            \n",
    "            history['g_loss'].append(np.mean(g_losses))\n",
    "            history['d_loss'].append(np.mean(d_losses))\n",
    "            \n",
    "            if (epoch + 1) % 20 == 0:\n",
    "                print(f\"  Epoch {epoch+1:3d}/{epochs} | D: {history['d_loss'][-1]:.4f} | \"\n",
    "                      f\"G: {history['g_loss'][-1]:.4f}\")\n",
    "        \n",
    "        print(f\"  Training time: {(time.time()-start)/60:.1f} min\")\n",
    "        return history\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        self.G.eval()\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.seq_len, self.latent_dim).to(self.device)\n",
    "            fake = self.G(z)\n",
    "        return fake.cpu().numpy()\n",
    "\n",
    "print(\"Fixed DoppelGANger defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "histories = {}\n",
    "synthetic_raw = {}  # Normalized synthetic data\n",
    "synthetic_processed = {}  # Post-processed synthetic data\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING WITH FIXED HYPERPARAMETERS - ANOMALY DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"LR_G: {LR_G}, LR_D: {LR_D}\")\n",
    "print(f\"Label smoothing: {LABEL_SMOOTHING}\")\n",
    "print(f\"Noise std: {NOISE_STD}\")\n",
    "total_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LSTM-GAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Fixed LSTM-GAN on Anomaly Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lstmgan = FixedLSTMGAN(NUM_FEATURES, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, SEQ_LEN, device)\n",
    "histories['LSTM-GAN'] = lstmgan.train(\n",
    "    train_loader, EPOCHS, \n",
    "    lr_g=LR_G, lr_d=LR_D,\n",
    "    label_smoothing=LABEL_SMOOTHING,\n",
    "    noise_std=NOISE_STD\n",
    ")\n",
    "models['LSTM-GAN'] = lstmgan\n",
    "\n",
    "# Generate (normalized)\n",
    "synthetic_raw['LSTM-GAN'] = lstmgan.generate(N_SYNTHETIC)\n",
    "print(f\"Generated (normalized): {synthetic_raw['LSTM-GAN'].shape}\")\n",
    "print(f\"Range: [{synthetic_raw['LSTM-GAN'].min():.4f}, {synthetic_raw['LSTM-GAN'].max():.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TimeGAN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Fixed TimeGAN on Anomaly Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "timegan = FixedTimeGAN(NUM_FEATURES, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, device)\n",
    "histories['TimeGAN'] = timegan.train(\n",
    "    train_loader, EPOCHS,\n",
    "    lr_g=LR_G, lr_d=LR_D,\n",
    "    label_smoothing=LABEL_SMOOTHING\n",
    ")\n",
    "models['TimeGAN'] = timegan\n",
    "\n",
    "synthetic_raw['TimeGAN'] = timegan.generate(N_SYNTHETIC, SEQ_LEN)\n",
    "print(f\"Generated (normalized): {synthetic_raw['TimeGAN'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DoppelGANger\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Fixed DoppelGANger on Anomaly Data\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "doppelganger = FixedDoppelGANger(NUM_FEATURES, HIDDEN_DIM, LATENT_DIM, NUM_LAYERS, SEQ_LEN, device)\n",
    "histories['DoppelGANger'] = doppelganger.train(\n",
    "    train_loader, EPOCHS,\n",
    "    lr_g=LR_G, lr_d=LR_D,\n",
    "    label_smoothing=LABEL_SMOOTHING\n",
    ")\n",
    "models['DoppelGANger'] = doppelganger\n",
    "\n",
    "synthetic_raw['DoppelGANger'] = doppelganger.generate(N_SYNTHETIC)\n",
    "print(f\"Generated (normalized): {synthetic_raw['DoppelGANger'].shape}\")\n",
    "\n",
    "print(f\"\\nTotal time: {(time.time()-total_start)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Post-Processing (CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process all synthetic data\n",
    "print(\"=\"*60)\n",
    "print(\"POST-PROCESSING SYNTHETIC ANOMALY DATA\")\n",
    "print(\"=\"*60)\n",
    "print(\"Steps: Denormalize → Clip negatives → Round integers\")\n",
    "\n",
    "for name, raw_data in synthetic_raw.items():\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Before: range=[{raw_data.min():.4f}, {raw_data.max():.4f}]\")\n",
    "    \n",
    "    # Apply post-processing\n",
    "    processed = post_process_synthetic_data(raw_data, available_features, scaler)\n",
    "    synthetic_processed[name] = processed\n",
    "    \n",
    "    print(f\"After: range=[{processed.min():.2f}, {processed.max():.2f}]\")\n",
    "    \n",
    "    # Validate\n",
    "    validate_synthetic_data(processed, available_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample of processed data\n",
    "print(\"\\nSample of processed anomaly data (first 5 features, first timestep):\")\n",
    "for name, data in synthetic_processed.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    sample = data[0, 0, :5]  # First sequence, first timestep, first 5 features\n",
    "    for i, val in enumerate(sample):\n",
    "        print(f\"  {available_features[i]}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Fixed Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(real_data, syn_data, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics with proper NaN handling.\n",
    "    \"\"\"\n",
    "    real_flat = real_data.reshape(-1, len(feature_names))\n",
    "    syn_flat = syn_data.reshape(-1, len(feature_names))\n",
    "    \n",
    "    metrics = {\n",
    "        'per_feature': {},\n",
    "        'ks_stats': [],\n",
    "        'mae_values': [],\n",
    "        'wasserstein': []\n",
    "    }\n",
    "    \n",
    "    for i, feat in enumerate(feature_names):\n",
    "        real_col = real_flat[:, i]\n",
    "        syn_col = syn_flat[:, i]\n",
    "        \n",
    "        # Skip if all values are the same (causes NaN)\n",
    "        if real_col.std() < 1e-10 or syn_col.std() < 1e-10:\n",
    "            continue\n",
    "        \n",
    "        # KS statistic\n",
    "        ks_stat, _ = stats.ks_2samp(real_col, syn_col)\n",
    "        if not np.isnan(ks_stat):\n",
    "            metrics['ks_stats'].append(ks_stat)\n",
    "        \n",
    "        # MAE (between means)\n",
    "        mae = np.abs(real_col.mean() - syn_col.mean())\n",
    "        if not np.isnan(mae):\n",
    "            metrics['mae_values'].append(mae)\n",
    "        \n",
    "        # Wasserstein distance\n",
    "        try:\n",
    "            wd = stats.wasserstein_distance(real_col, syn_col)\n",
    "            if not np.isnan(wd):\n",
    "                metrics['wasserstein'].append(wd)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        metrics['per_feature'][feat] = {\n",
    "            'real_mean': float(real_col.mean()),\n",
    "            'syn_mean': float(syn_col.mean()),\n",
    "            'real_std': float(real_col.std()),\n",
    "            'syn_std': float(syn_col.std()),\n",
    "            'ks': float(ks_stat) if not np.isnan(ks_stat) else None,\n",
    "            'mae': float(mae) if not np.isnan(mae) else None\n",
    "        }\n",
    "    \n",
    "    # Aggregate metrics\n",
    "    metrics['ks_mean'] = np.mean(metrics['ks_stats']) if metrics['ks_stats'] else float('nan')\n",
    "    metrics['mae_mean'] = np.mean(metrics['mae_values']) if metrics['mae_values'] else float('nan')\n",
    "    metrics['wasserstein_mean'] = np.mean(metrics['wasserstein']) if metrics['wasserstein'] else float('nan')\n",
    "    \n",
    "    # Correlation matrix error\n",
    "    real_corr = np.corrcoef(real_flat.T)\n",
    "    syn_corr = np.corrcoef(syn_flat.T)\n",
    "    real_corr = np.nan_to_num(real_corr, nan=0)\n",
    "    syn_corr = np.nan_to_num(syn_corr, nan=0)\n",
    "    metrics['corr_error'] = np.mean(np.abs(real_corr - syn_corr))\n",
    "    \n",
    "    # Overall score\n",
    "    metrics['overall_score'] = (\n",
    "        0.3 * metrics['ks_mean'] +\n",
    "        0.3 * min(metrics['mae_mean'] / 100, 1) +  # Normalize MAE\n",
    "        0.2 * metrics['wasserstein_mean'] / 10 +\n",
    "        0.2 * metrics['corr_error']\n",
    "    )\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"Fixed evaluation metrics defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using POST-PROCESSED data (original scale)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION (on post-processed anomaly data)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get real test data in original scale\n",
    "X_test_np = X_test.cpu().numpy() if torch.is_tensor(X_test) else X_test\n",
    "X_test_original = post_process_synthetic_data(X_test_np, available_features, scaler)\n",
    "\n",
    "all_metrics = {}\n",
    "for name, syn_data in synthetic_processed.items():\n",
    "    metrics = calculate_metrics(X_test_original, syn_data, available_features)\n",
    "    all_metrics[name] = metrics\n",
    "    \n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  KS Statistic:     {metrics['ks_mean']:.4f}\")\n",
    "    print(f\"  MAE:              {metrics['mae_mean']:.4f}\")\n",
    "    print(f\"  Wasserstein:      {metrics['wasserstein_mean']:.4f}\")\n",
    "    print(f\"  Correlation Err:  {metrics['corr_error']:.4f}\")\n",
    "    print(f\"  Overall Score:    {metrics['overall_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "best_model = min(all_metrics.keys(), key=lambda k: all_metrics[k]['overall_score'])\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training curves with D(real) and D(fake)\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, hist) in enumerate(histories.items()):\n",
    "    ax = axes[idx]\n",
    "    ax.plot(hist['g_loss'], label='G Loss', alpha=0.8)\n",
    "    ax.plot(hist['d_loss'], label='D Loss', alpha=0.8)\n",
    "    if 'd_real' in hist:\n",
    "        ax.plot(hist['d_real'], label='D(real)', linestyle='--', alpha=0.6)\n",
    "        ax.plot(hist['d_fake'], label='D(fake)', linestyle='--', alpha=0.6)\n",
    "    ax.set_title(f'{name} Training (Anomaly)')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss / Prob')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_curves_anomaly.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison (original scale!)\n",
    "key_features = ['announcements', 'withdrawals', 'as_path_max', 'flaps', 'nadas', 'edit_distance_avg']\n",
    "key_idx = [available_features.index(f) for f in key_features if f in available_features]\n",
    "\n",
    "real_flat = X_test_original.reshape(-1, NUM_FEATURES)\n",
    "\n",
    "for name, syn in synthetic_processed.items():\n",
    "    syn_flat = syn.reshape(-1, NUM_FEATURES)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(key_idx[:6]):\n",
    "        axes[i].hist(real_flat[:, idx], bins=50, alpha=0.5, label='Real', density=True)\n",
    "        axes[i].hist(syn_flat[:, idx], bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "        axes[i].set_title(f\"{available_features[idx]}\")\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{name} - Distribution Comparison (Anomaly Data)', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'{name.lower().replace(\"-\", \"_\")}_dist_anomaly.png'), dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metric_names = ['ks_mean', 'mae_mean', 'corr_error', 'overall_score']\n",
    "titles = ['KS Statistic', 'MAE', 'Correlation Error', 'Overall Score']\n",
    "model_names = list(all_metrics.keys())\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "\n",
    "for i, (metric, title) in enumerate(zip(metric_names, titles)):\n",
    "    values = [all_metrics[m][metric] for m in model_names]\n",
    "    bars = axes[i].bar(model_names, values, color=colors)\n",
    "    axes[i].set_title(f'{title} (lower is better)')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Highlight best\n",
    "    best_idx = np.argmin(values)\n",
    "    bars[best_idx].set_edgecolor('gold')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "\n",
    "plt.suptitle('Model Comparison - Anomaly Traffic Generation', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'model_comparison_anomaly.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save post-processed synthetic anomaly data\n",
    "print(\"Saving post-processed synthetic anomaly data...\")\n",
    "\n",
    "for name, data in synthetic_processed.items():\n",
    "    records = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for t in range(data.shape[1]):\n",
    "            rec = {'sequence_id': i, 'timestep': t, 'label': 'anomaly'}\n",
    "            for j, feat in enumerate(available_features):\n",
    "                val = data[i, t, j]\n",
    "                # Ensure integer features are int\n",
    "                if feat in INTEGER_FEATURES:\n",
    "                    rec[feat] = int(val)\n",
    "                else:\n",
    "                    rec[feat] = float(val)\n",
    "            records.append(rec)\n",
    "    \n",
    "    df_out = pd.DataFrame(records)\n",
    "    path = os.path.join(OUTPUT_DIR, f'synthetic_{name.lower().replace(\"-\", \"_\")}_anomaly.csv')\n",
    "    df_out.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "\n",
    "# Save best model's data as main output\n",
    "best_path = os.path.join(OUTPUT_DIR, 'synthetic_anomaly_traffic.csv')\n",
    "import shutil\n",
    "shutil.copy(\n",
    "    os.path.join(OUTPUT_DIR, f'synthetic_{best_model.lower().replace(\"-\", \"_\")}_anomaly.csv'),\n",
    "    best_path\n",
    ")\n",
    "print(f\"Best model output: {best_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved data\n",
    "print(\"\\nVerifying saved anomaly data...\")\n",
    "df_check = pd.read_csv(best_path)\n",
    "\n",
    "print(f\"\\nSample of saved data:\")\n",
    "print(df_check.head())\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(df_check.dtypes[:10])\n",
    "\n",
    "print(f\"\\nValue ranges:\")\n",
    "for col in ['announcements', 'withdrawals', 'flaps', 'edit_distance_avg']:\n",
    "    if col in df_check.columns:\n",
    "        print(f\"  {col}: min={df_check[col].min()}, max={df_check[col].max()}, has_negative={df_check[col].min() < 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scaler and config\n",
    "with open(os.path.join(OUTPUT_DIR, 'scaler_anomaly.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "config = {\n",
    "    'data_type': 'anomaly',\n",
    "    'high_confidence_labels': HIGH_CONFIDENCE_LABELS,\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS,\n",
    "    'lr_g': LR_G,\n",
    "    'lr_d': LR_D,\n",
    "    'label_smoothing': LABEL_SMOOTHING,\n",
    "    'hidden_dim': HIDDEN_DIM,\n",
    "    'latent_dim': LATENT_DIM,\n",
    "    'features': available_features,\n",
    "    'integer_features': INTEGER_FEATURES,\n",
    "    'continuous_features': CONTINUOUS_FEATURES,\n",
    "    'best_model': best_model,\n",
    "    'metrics': {k: {kk: vv for kk, vv in v.items() if kk != 'per_feature'} \n",
    "                for k, v in all_metrics.items()},\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'config_anomaly.json'), 'w') as f:\n",
    "    json.dump(config, f, indent=2, default=str)\n",
    "\n",
    "print(\"Saved config_anomaly.json and scaler_anomaly.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPLETE - ANOMALY TRAFFIC GENERATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "Data Configuration:\n",
    "  Data path: {DATA_PATH}\n",
    "  High confidence labels: {HIGH_CONFIDENCE_LABELS}\n",
    "  Training samples: {len(X_train):,}\n",
    "\n",
    "Fixes Applied:\n",
    "  ✓ Separate learning rates (G: {LR_G}, D: {LR_D})\n",
    "  ✓ Label smoothing ({LABEL_SMOOTHING})\n",
    "  ✓ Sigmoid output (data in [0,1])\n",
    "  ✓ Post-processing: denormalize → clip → round\n",
    "  ✓ Fixed MAE calculation (NaN handling)\n",
    "\n",
    "Results:\n",
    "  Best Model: {best_model}\n",
    "  KS Statistic: {all_metrics[best_model]['ks_mean']:.4f}\n",
    "  MAE: {all_metrics[best_model]['mae_mean']:.4f}\n",
    "\n",
    "Output: {OUTPUT_DIR}\n",
    "\"\"\")\n",
    "\n",
    "for f in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f)) / 1024\n",
    "    print(f\"  {f}: {size:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
