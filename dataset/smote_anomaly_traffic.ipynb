{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE-based Anomaly Traffic Generation\n",
    "\n",
    "This notebook generates synthetic anomaly BGP traffic using SMOTE-based oversampling techniques.\n",
    "It filters data with high confidence anomaly labels: `medium_confidence`, `high_confidence`, `very_high_confidence`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Feature Sets with Enhanced Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High confidence anomaly labels to filter\n",
    "HIGH_CONFIDENCE_LABELS = ['medium_confidence', 'high_confidence', 'very_high_confidence']\n",
    "\n",
    "# Integer features (must be rounded after SMOTE)\n",
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Heavy-tailed count features (benefit from log1p transform before SMOTE)\n",
    "HEAVY_TAILED_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann',\n",
    "    'number_rare_ases', 'nadas'\n",
    "]\n",
    "\n",
    "# Continuous features (can be float but bounded)\n",
    "CONTINUOUS_FEATURES = [\n",
    "    'edit_distance_avg',\n",
    "    'rare_ases_avg'\n",
    "]\n",
    "\n",
    "# Bounded ratio features (need empirical min/max clipping)\n",
    "BOUNDED_RATIO_FEATURES = [\n",
    "    'rare_ases_avg'  # This is a ratio typically between 0 and 1\n",
    "]\n",
    "\n",
    "# Edit distance dict features (for constraint validation)\n",
    "EDIT_DISTANCE_DICT_FEATURES = [\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6'\n",
    "]\n",
    "\n",
    "# Core features for quality validation\n",
    "CORE_VALIDATION_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'edit_distance_avg'\n",
    "]\n",
    "\n",
    "# All features to use for synthetic generation\n",
    "ALL_FEATURES = INTEGER_FEATURES + CONTINUOUS_FEATURES\n",
    "\n",
    "# Columns to exclude (labels, timestamps, derived scores)\n",
    "EXCLUDE_COLS = [\n",
    "    'label', 'confidence_label', 'window_start', 'window_end',\n",
    "    'iso_forest_score', 'lof_score', 'statistical_score',\n",
    "    'elliptic_score', 'cluster', 'anomaly_votes', 'consensus_score',\n",
    "    'Incident'\n",
    "]\n",
    "\n",
    "# Potential grouping columns (if available in dataset)\n",
    "POTENTIAL_GROUP_COLS = ['peer_asn', 'peer_ip', 'prefix', 'collector', 'Incident']\n",
    "\n",
    "print(f\"Integer features: {len(INTEGER_FEATURES)}\")\n",
    "print(f\"Heavy-tailed features: {len(HEAVY_TAILED_FEATURES)}\")\n",
    "print(f\"Continuous features: {len(CONTINUOUS_FEATURES)}\")\n",
    "print(f\"Total features for generation: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - Anomaly data with reinforced labels\n",
    "DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distributions\n",
    "print(\"Confidence label distribution:\")\n",
    "print(df['confidence_label'].value_counts())\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    print(\"\\nOriginal label distribution:\")\n",
    "    print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only high confidence anomaly samples\n",
    "df_high_confidence = df[df['confidence_label'].isin(HIGH_CONFIDENCE_LABELS)].copy()\n",
    "print(f\"High confidence anomaly samples: {len(df_high_confidence)}\")\n",
    "print(f\"\\nDistribution within high confidence:\")\n",
    "print(df_high_confidence['confidence_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all features exist in the dataset\n",
    "missing_features = [f for f in ALL_FEATURES if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"WARNING: Missing features: {missing_features}\")\n",
    "    # Update ALL_FEATURES to only include available features\n",
    "    ALL_FEATURES = [f for f in ALL_FEATURES if f in df.columns]\n",
    "    INTEGER_FEATURES = [f for f in INTEGER_FEATURES if f in df.columns]\n",
    "    HEAVY_TAILED_FEATURES = [f for f in HEAVY_TAILED_FEATURES if f in df.columns]\n",
    "    CONTINUOUS_FEATURES = [f for f in CONTINUOUS_FEATURES if f in df.columns]\n",
    "    print(f\"Updated to {len(ALL_FEATURES)} available features\")\n",
    "else:\n",
    "    print(\"All features found in dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential grouping columns\n",
    "available_group_cols = [col for col in POTENTIAL_GROUP_COLS if col in df.columns]\n",
    "print(f\"Available grouping columns: {available_group_cols}\")\n",
    "\n",
    "if available_group_cols:\n",
    "    for col in available_group_cols:\n",
    "        n_unique = df_high_confidence[col].nunique()\n",
    "        print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features - use only raw BGP features\n",
    "X_anomaly = df_high_confidence[ALL_FEATURES].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "X_anomaly = X_anomaly.fillna(X_anomaly.median())\n",
    "\n",
    "print(f\"Feature matrix shape: {X_anomaly.shape}\")\n",
    "X_anomaly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store empirical bounds for all features (for clipping after inverse transform)\n",
    "EMPIRICAL_BOUNDS = {}\n",
    "for col in ALL_FEATURES:\n",
    "    EMPIRICAL_BOUNDS[col] = {\n",
    "        'min': X_anomaly[col].min(),\n",
    "        'max': X_anomaly[col].max(),\n",
    "        'q01': X_anomaly[col].quantile(0.01),\n",
    "        'q99': X_anomaly[col].quantile(0.99)\n",
    "    }\n",
    "\n",
    "print(\"Empirical bounds computed for all features.\")\n",
    "print(\"\\nExample bounds for key features:\")\n",
    "for feat in ['announcements', 'withdrawals', 'rare_ases_avg', 'edit_distance_avg']:\n",
    "    if feat in EMPIRICAL_BOUNDS:\n",
    "        print(f\"  {feat}: [{EMPIRICAL_BOUNDS[feat]['min']:.4f}, {EMPIRICAL_BOUNDS[feat]['max']:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Transform Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log1p_transform(X, features):\n",
    "    \"\"\"Apply log1p transform to heavy-tailed features.\"\"\"\n",
    "    X_transformed = X.copy()\n",
    "    for feat in features:\n",
    "        if feat in X_transformed.columns:\n",
    "            X_transformed[feat] = np.log1p(X_transformed[feat])\n",
    "    return X_transformed\n",
    "\n",
    "def inverse_log1p_transform(X, features):\n",
    "    \"\"\"Inverse log1p transform.\"\"\"\n",
    "    X_inverse = X.copy()\n",
    "    for feat in features:\n",
    "        if feat in X_inverse.columns:\n",
    "            X_inverse[feat] = np.expm1(X_inverse[feat])\n",
    "    return X_inverse\n",
    "\n",
    "def post_process_synthetic(df_synthetic, empirical_bounds, integer_features, bounded_features):\n",
    "    \"\"\"\n",
    "    Post-process synthetic data:\n",
    "    1. Round integer features\n",
    "    2. Clip to empirical bounds\n",
    "    3. Ensure non-negativity\n",
    "    \"\"\"\n",
    "    df_processed = df_synthetic.copy()\n",
    "    \n",
    "    # Ensure non-negativity first\n",
    "    for col in df_processed.columns:\n",
    "        df_processed[col] = np.maximum(df_processed[col], 0)\n",
    "    \n",
    "    # Round integer features\n",
    "    for col in integer_features:\n",
    "        if col in df_processed.columns:\n",
    "            df_processed[col] = np.round(df_processed[col]).astype(int)\n",
    "    \n",
    "    # Clip bounded features\n",
    "    for col in bounded_features:\n",
    "        if col in df_processed.columns and col in empirical_bounds:\n",
    "            df_processed[col] = np.clip(\n",
    "                df_processed[col],\n",
    "                empirical_bounds[col]['min'],\n",
    "                empirical_bounds[col]['max']\n",
    "            )\n",
    "    \n",
    "    # Clip all features to empirical bounds (using q01, q99 for robustness)\n",
    "    for col in df_processed.columns:\n",
    "        if col in empirical_bounds:\n",
    "            # Use slightly expanded bounds\n",
    "            min_val = empirical_bounds[col]['min']\n",
    "            max_val = empirical_bounds[col]['max'] * 1.1  # Allow 10% overshoot\n",
    "            df_processed[col] = np.clip(df_processed[col], min_val, max_val)\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "print(\"Transform functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cluster-Based SMOTE Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_based_smote(X, n_clusters=5, target_samples=None, k_neighbors=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Perform cluster-based SMOTE generation.\n",
    "    \n",
    "    1. Cluster the data\n",
    "    2. Apply SMOTE within each cluster\n",
    "    3. Combine results\n",
    "    \n",
    "    This preserves local structure better than global SMOTE.\n",
    "    \"\"\"\n",
    "    if target_samples is None:\n",
    "        target_samples = len(X) * 2  # Default: double the data\n",
    "    \n",
    "    # Standardize for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Cluster the data\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "    clusters = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Calculate samples per cluster proportionally\n",
    "    cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "    total_original = len(X)\n",
    "    new_samples_needed = target_samples - total_original\n",
    "    \n",
    "    synthetic_dfs = []\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = clusters == cluster_id\n",
    "        X_cluster = X[cluster_mask].copy()\n",
    "        \n",
    "        if len(X_cluster) < k_neighbors + 1:\n",
    "            print(f\"  Cluster {cluster_id}: Too few samples ({len(X_cluster)}), skipping SMOTE\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate target for this cluster\n",
    "        cluster_proportion = len(X_cluster) / total_original\n",
    "        cluster_target = int(new_samples_needed * cluster_proportion)\n",
    "        \n",
    "        if cluster_target <= 0:\n",
    "            continue\n",
    "        \n",
    "        # Create dummy labels for SMOTE (minority class = 1)\n",
    "        y_dummy = np.array([0] * len(X_cluster) + [1] * 2)  # Need at least 2 minority samples\n",
    "        X_dummy = pd.concat([X_cluster, X_cluster.iloc[:2]], ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Calculate appropriate k_neighbors\n",
    "            k = min(k_neighbors, len(X_cluster) - 1)\n",
    "            \n",
    "            smote = SMOTE(\n",
    "                sampling_strategy={1: cluster_target + 2},\n",
    "                k_neighbors=k,\n",
    "                random_state=random_state + cluster_id\n",
    "            )\n",
    "            \n",
    "            X_resampled, y_resampled = smote.fit_resample(X_dummy, y_dummy)\n",
    "            \n",
    "            # Extract only the new synthetic samples\n",
    "            synthetic_mask = y_resampled == 1\n",
    "            X_synthetic = pd.DataFrame(X_resampled[synthetic_mask], columns=X.columns)\n",
    "            \n",
    "            # Remove the original dummy minority samples\n",
    "            X_synthetic = X_synthetic.iloc[2:]\n",
    "            \n",
    "            synthetic_dfs.append(X_synthetic)\n",
    "            print(f\"  Cluster {cluster_id}: Generated {len(X_synthetic)} samples from {len(X_cluster)} original\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Cluster {cluster_id}: Error - {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    if synthetic_dfs:\n",
    "        return pd.concat(synthetic_dfs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=X.columns)\n",
    "\n",
    "print(\"Cluster-based SMOTE function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quality Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_synthetic_quality(X_original, X_synthetic, feature_names=None, show_plots=True):\n",
    "    \"\"\"\n",
    "    Comprehensive validation of synthetic data quality.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Quality metrics including KS statistics, correlation preservation, etc.\n",
    "    \"\"\"\n",
    "    if feature_names is None:\n",
    "        feature_names = X_original.columns.tolist()\n",
    "    \n",
    "    results = {\n",
    "        'ks_statistics': {},\n",
    "        'mean_differences': {},\n",
    "        'std_differences': {},\n",
    "        'correlation_diff': None\n",
    "    }\n",
    "    \n",
    "    # 1. KS Test for each feature\n",
    "    ks_stats = []\n",
    "    for feat in feature_names:\n",
    "        if feat in X_original.columns and feat in X_synthetic.columns:\n",
    "            stat, p_value = ks_2samp(X_original[feat], X_synthetic[feat])\n",
    "            results['ks_statistics'][feat] = {'statistic': stat, 'p_value': p_value}\n",
    "            ks_stats.append(stat)\n",
    "    \n",
    "    results['avg_ks_statistic'] = np.mean(ks_stats) if ks_stats else 0\n",
    "    \n",
    "    # 2. Mean and Std differences\n",
    "    for feat in feature_names:\n",
    "        if feat in X_original.columns and feat in X_synthetic.columns:\n",
    "            orig_mean = X_original[feat].mean()\n",
    "            syn_mean = X_synthetic[feat].mean()\n",
    "            results['mean_differences'][feat] = abs(orig_mean - syn_mean) / (orig_mean + 1e-10)\n",
    "            \n",
    "            orig_std = X_original[feat].std()\n",
    "            syn_std = X_synthetic[feat].std()\n",
    "            results['std_differences'][feat] = abs(orig_std - syn_std) / (orig_std + 1e-10)\n",
    "    \n",
    "    # 3. Correlation matrix comparison\n",
    "    try:\n",
    "        corr_orig = X_original[feature_names].corr()\n",
    "        corr_syn = X_synthetic[feature_names].corr()\n",
    "        results['correlation_diff'] = np.abs(corr_orig - corr_syn).mean().mean()\n",
    "    except:\n",
    "        results['correlation_diff'] = None\n",
    "    \n",
    "    # 4. Print summary\n",
    "    print(\"\\n=== Synthetic Data Quality Summary ===\")\n",
    "    print(f\"Average KS Statistic: {results['avg_ks_statistic']:.4f} (lower is better, <0.1 is good)\")\n",
    "    print(f\"Average Mean Difference: {np.mean(list(results['mean_differences'].values())):.4f}\")\n",
    "    print(f\"Average Std Difference: {np.mean(list(results['std_differences'].values())):.4f}\")\n",
    "    if results['correlation_diff'] is not None:\n",
    "        print(f\"Correlation Preservation Error: {results['correlation_diff']:.4f} (lower is better)\")\n",
    "    \n",
    "    # 5. Visualization\n",
    "    if show_plots:\n",
    "        # Plot distributions for key features\n",
    "        key_features = ['announcements', 'withdrawals', 'edit_distance_avg', 'rare_ases_avg']\n",
    "        key_features = [f for f in key_features if f in feature_names]\n",
    "        \n",
    "        if key_features:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "            axes = axes.flatten()\n",
    "            \n",
    "            for i, feat in enumerate(key_features[:4]):\n",
    "                ax = axes[i]\n",
    "                ax.hist(X_original[feat], bins=50, alpha=0.5, label='Original', density=True)\n",
    "                ax.hist(X_synthetic[feat], bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "                ax.set_title(f'{feat}\\nKS={results[\"ks_statistics\"].get(feat, {}).get(\"statistic\", 0):.3f}')\n",
    "                ax.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.suptitle('Distribution Comparison: Original vs Synthetic Anomaly Data', y=1.02)\n",
    "            plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Quality validation function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Synthetic Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TARGET_MULTIPLIER = 3  # Generate 3x the original data\n",
    "N_CLUSTERS = 8  # Number of clusters for cluster-based SMOTE\n",
    "K_NEIGHBORS = 5  # K neighbors for SMOTE\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "target_samples = len(X_anomaly) * TARGET_MULTIPLIER\n",
    "print(f\"Original high confidence anomaly samples: {len(X_anomaly)}\")\n",
    "print(f\"Target total samples: {target_samples}\")\n",
    "print(f\"New samples to generate: {target_samples - len(X_anomaly)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Apply log1p transform to heavy-tailed features\n",
    "print(\"Step 1: Applying log1p transform to heavy-tailed features...\")\n",
    "X_transformed = log1p_transform(X_anomaly, HEAVY_TAILED_FEATURES)\n",
    "print(f\"Transformed features: {HEAVY_TAILED_FEATURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate synthetic data using cluster-based SMOTE\n",
    "print(\"\\nStep 2: Generating synthetic data using cluster-based SMOTE...\")\n",
    "X_synthetic_transformed = cluster_based_smote(\n",
    "    X_transformed,\n",
    "    n_clusters=N_CLUSTERS,\n",
    "    target_samples=target_samples,\n",
    "    k_neighbors=K_NEIGHBORS,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(X_synthetic_transformed)} synthetic samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Inverse transform\n",
    "print(\"\\nStep 3: Applying inverse log1p transform...\")\n",
    "X_synthetic_raw = inverse_log1p_transform(X_synthetic_transformed, HEAVY_TAILED_FEATURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Post-process synthetic data\n",
    "print(\"\\nStep 4: Post-processing synthetic data...\")\n",
    "X_synthetic_final = post_process_synthetic(\n",
    "    X_synthetic_raw,\n",
    "    EMPIRICAL_BOUNDS,\n",
    "    INTEGER_FEATURES,\n",
    "    BOUNDED_RATIO_FEATURES\n",
    ")\n",
    "\n",
    "print(f\"Final synthetic samples: {len(X_synthetic_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Validate Synthetic Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate synthetic data quality\n",
    "validation_results = validate_synthetic_quality(\n",
    "    X_anomaly,\n",
    "    X_synthetic_final,\n",
    "    feature_names=ALL_FEATURES,\n",
    "    show_plots=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed feature comparison\n",
    "print(\"\\n=== Detailed Feature Statistics Comparison ===\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Feature': ALL_FEATURES,\n",
    "    'Original_Mean': [X_anomaly[f].mean() for f in ALL_FEATURES],\n",
    "    'Synthetic_Mean': [X_synthetic_final[f].mean() for f in ALL_FEATURES],\n",
    "    'Original_Std': [X_anomaly[f].std() for f in ALL_FEATURES],\n",
    "    'Synthetic_Std': [X_synthetic_final[f].std() for f in ALL_FEATURES],\n",
    "    'KS_Statistic': [validation_results['ks_statistics'].get(f, {}).get('statistic', 0) for f in ALL_FEATURES]\n",
    "})\n",
    "\n",
    "comparison_df['Mean_Diff_%'] = abs(comparison_df['Original_Mean'] - comparison_df['Synthetic_Mean']) / (comparison_df['Original_Mean'] + 1e-10) * 100\n",
    "comparison_df['Std_Diff_%'] = abs(comparison_df['Original_Std'] - comparison_df['Synthetic_Std']) / (comparison_df['Original_Std'] + 1e-10) * 100\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any negative values\n",
    "print(\"Checking for negative values...\")\n",
    "negative_counts = (X_synthetic_final < 0).sum()\n",
    "if negative_counts.any():\n",
    "    print(f\"WARNING: Negative values found in: {negative_counts[negative_counts > 0].to_dict()}\")\n",
    "else:\n",
    "    print(\"✓ No negative values found\")\n",
    "\n",
    "# Check integer features are actually integers\n",
    "print(\"\\nChecking integer feature types...\")\n",
    "for feat in INTEGER_FEATURES:\n",
    "    if feat in X_synthetic_final.columns:\n",
    "        non_int = (X_synthetic_final[feat] != X_synthetic_final[feat].astype(int)).sum()\n",
    "        if non_int > 0:\n",
    "            print(f\"  WARNING: {feat} has {non_int} non-integer values\")\n",
    "print(\"✓ Integer features validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Original correlation\n",
    "corr_orig = X_anomaly[CORE_VALIDATION_FEATURES].corr()\n",
    "sns.heatmap(corr_orig, annot=True, cmap='coolwarm', center=0, ax=axes[0], fmt='.2f')\n",
    "axes[0].set_title('Original Anomaly Data Correlations')\n",
    "\n",
    "# Synthetic correlation\n",
    "corr_syn = X_synthetic_final[CORE_VALIDATION_FEATURES].corr()\n",
    "sns.heatmap(corr_syn, annot=True, cmap='coolwarm', center=0, ax=axes[1], fmt='.2f')\n",
    "axes[1].set_title('Synthetic Anomaly Data Correlations')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Combine and Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined dataset with labels\n",
    "print(\"Creating combined dataset...\")\n",
    "\n",
    "# Add source label to original data\n",
    "df_original_labeled = X_anomaly.copy()\n",
    "df_original_labeled['data_source'] = 'original'\n",
    "df_original_labeled['label'] = 'anomaly'\n",
    "\n",
    "# Add confidence labels from original data if available\n",
    "if 'confidence_label' in df_high_confidence.columns:\n",
    "    df_original_labeled['confidence_label'] = df_high_confidence['confidence_label'].values\n",
    "\n",
    "# Add source label to synthetic data\n",
    "df_synthetic_labeled = X_synthetic_final.copy()\n",
    "df_synthetic_labeled['data_source'] = 'synthetic'\n",
    "df_synthetic_labeled['label'] = 'anomaly'\n",
    "df_synthetic_labeled['confidence_label'] = 'synthetic_anomaly'\n",
    "\n",
    "# Combine\n",
    "df_combined = pd.concat([df_original_labeled, df_synthetic_labeled], ignore_index=True)\n",
    "\n",
    "print(f\"\\nCombined dataset shape: {df_combined.shape}\")\n",
    "print(f\"Data source distribution:\\n{df_combined['data_source'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export paths\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "OUTPUT_DIR = '/home/smotaali/BGP_Traffic_Generation/results/synthetic_anomaly_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Export synthetic data only\n",
    "synthetic_only_path = os.path.join(OUTPUT_DIR, 'synthetic_anomaly_data.csv')\n",
    "X_synthetic_final.to_csv(synthetic_only_path, index=False)\n",
    "print(f\"Synthetic data saved to: {synthetic_only_path}\")\n",
    "\n",
    "# Export combined data (original + synthetic)\n",
    "combined_path = os.path.join(OUTPUT_DIR, 'combined_anomaly_data.csv')\n",
    "df_combined.to_csv(combined_path, index=False)\n",
    "print(f\"Combined data saved to: {combined_path}\")\n",
    "\n",
    "print(f\"\\n=== Export Complete ===\")\n",
    "print(f\"Original samples: {len(X_anomaly)}\")\n",
    "print(f\"Synthetic samples: {len(X_synthetic_final)}\")\n",
    "print(f\"Total combined: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SYNTHETIC ANOMALY DATA GENERATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput Data:\")\n",
    "print(f\"  - Source file: {DATA_PATH}\")\n",
    "print(f\"  - Confidence labels used: {HIGH_CONFIDENCE_LABELS}\")\n",
    "print(f\"  - Original high confidence samples: {len(X_anomaly)}\")\n",
    "\n",
    "print(f\"\\nGeneration Parameters:\")\n",
    "print(f\"  - Target multiplier: {TARGET_MULTIPLIER}x\")\n",
    "print(f\"  - Number of clusters: {N_CLUSTERS}\")\n",
    "print(f\"  - K neighbors: {K_NEIGHBORS}\")\n",
    "\n",
    "print(f\"\\nOutput:\")\n",
    "print(f\"  - Synthetic samples generated: {len(X_synthetic_final)}\")\n",
    "print(f\"  - Total combined samples: {len(df_combined)}\")\n",
    "\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"  - Average KS Statistic: {validation_results['avg_ks_statistic']:.4f}\")\n",
    "if validation_results['correlation_diff'] is not None:\n",
    "    print(f\"  - Correlation Preservation Error: {validation_results['correlation_diff']:.4f}\")\n",
    "\n",
    "print(f\"\\nExport Locations:\")\n",
    "print(f\"  - Synthetic only: {synthetic_only_path}\")\n",
    "print(f\"  - Combined data: {combined_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of generated data\n",
    "print(\"\\nSample of Generated Synthetic Anomaly Data:\")\n",
    "X_synthetic_final.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
