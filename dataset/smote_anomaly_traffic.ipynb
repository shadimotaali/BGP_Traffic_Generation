{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE-based Anomaly Traffic Generation\n",
    "\n",
    "This notebook generates synthetic anomaly BGP traffic using SMOTE-based oversampling techniques.\n",
    "It filters data with high confidence anomaly labels: `medium_confidence`, `high_confidence`, `very_high_confidence`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Feature Sets with Enhanced Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High confidence anomaly labels to filter\n",
    "HIGH_CONFIDENCE_LABELS = ['medium_confidence', 'high_confidence', 'very_high_confidence']\n",
    "\n",
    "# Integer features (must be rounded after SMOTE)\n",
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Heavy-tailed count features (benefit from log1p transform before SMOTE)\n",
    "HEAVY_TAILED_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann',\n",
    "    'number_rare_ases', 'nadas'\n",
    "]\n",
    "\n",
    "# Continuous features (can be float but bounded)\n",
    "CONTINUOUS_FEATURES = [\n",
    "    'edit_distance_avg',\n",
    "    'rare_ases_avg'\n",
    "]\n",
    "\n",
    "# Bounded ratio features (need empirical min/max clipping)\n",
    "BOUNDED_RATIO_FEATURES = [\n",
    "    'rare_ases_avg'  # This is a ratio typically between 0 and 1\n",
    "]\n",
    "\n",
    "# Edit distance dict features (for constraint validation)\n",
    "EDIT_DISTANCE_DICT_FEATURES = [\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6'\n",
    "]\n",
    "\n",
    "# Core features for quality validation\n",
    "CORE_VALIDATION_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'edit_distance_avg'\n",
    "]\n",
    "\n",
    "# All features to use for synthetic generation\n",
    "ALL_FEATURES = INTEGER_FEATURES + CONTINUOUS_FEATURES\n",
    "\n",
    "# Columns to exclude (labels, timestamps, derived scores)\n",
    "EXCLUDE_COLS = [\n",
    "    'label', 'confidence_label', 'window_start', 'window_end',\n",
    "    'iso_forest_score', 'lof_score', 'statistical_score',\n",
    "    'elliptic_score', 'cluster', 'anomaly_votes', 'consensus_score',\n",
    "    'Incident'\n",
    "]\n",
    "\n",
    "# Potential grouping columns (if available in dataset)\n",
    "POTENTIAL_GROUP_COLS = ['peer_asn', 'peer_ip', 'prefix', 'collector', 'Incident']\n",
    "\n",
    "print(f\"Integer features: {len(INTEGER_FEATURES)}\")\n",
    "print(f\"Heavy-tailed features: {len(HEAVY_TAILED_FEATURES)}\")\n",
    "print(f\"Continuous features: {len(CONTINUOUS_FEATURES)}\")\n",
    "print(f\"Total features for generation: {len(ALL_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - Anomaly data with reinforced labels\n",
    "DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check label distributions\n",
    "print(\"Confidence label distribution:\")\n",
    "print(df['confidence_label'].value_counts())\n",
    "\n",
    "if 'label' in df.columns:\n",
    "    print(\"\\nOriginal label distribution:\")\n",
    "    print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only high confidence anomaly samples\n",
    "df_high_confidence = df[df['confidence_label'].isin(HIGH_CONFIDENCE_LABELS)].copy()\n",
    "print(f\"High confidence anomaly samples: {len(df_high_confidence)}\")\n",
    "print(f\"\\nDistribution within high confidence:\")\n",
    "print(df_high_confidence['confidence_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all features exist in the dataset\n",
    "missing_features = [f for f in ALL_FEATURES if f not in df.columns]\n",
    "if missing_features:\n",
    "    print(f\"WARNING: Missing features: {missing_features}\")\n",
    "    # Update ALL_FEATURES to only include available features\n",
    "    ALL_FEATURES = [f for f in ALL_FEATURES if f in df.columns]\n",
    "    INTEGER_FEATURES = [f for f in INTEGER_FEATURES if f in df.columns]\n",
    "    HEAVY_TAILED_FEATURES = [f for f in HEAVY_TAILED_FEATURES if f in df.columns]\n",
    "    CONTINUOUS_FEATURES = [f for f in CONTINUOUS_FEATURES if f in df.columns]\n",
    "    print(f\"Updated to {len(ALL_FEATURES)} available features\")\n",
    "else:\n",
    "    print(\"All features found in dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for potential grouping columns\n",
    "available_group_cols = [col for col in POTENTIAL_GROUP_COLS if col in df.columns]\n",
    "print(f\"Available grouping columns: {available_group_cols}\")\n",
    "\n",
    "if available_group_cols:\n",
    "    for col in available_group_cols:\n",
    "        n_unique = df_high_confidence[col].nunique()\n",
    "        print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features - use only raw BGP features\n",
    "X_anomaly = df_high_confidence[ALL_FEATURES].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "X_anomaly = X_anomaly.fillna(X_anomaly.median())\n",
    "\n",
    "print(f\"Feature matrix shape: {X_anomaly.shape}\")\n",
    "X_anomaly.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store empirical bounds for all features (for clipping after inverse transform)\n",
    "EMPIRICAL_BOUNDS = {}\n",
    "for col in ALL_FEATURES:\n",
    "    EMPIRICAL_BOUNDS[col] = {\n",
    "        'min': X_anomaly[col].min(),\n",
    "        'max': X_anomaly[col].max(),\n",
    "        'q01': X_anomaly[col].quantile(0.01),\n",
    "        'q99': X_anomaly[col].quantile(0.99)\n",
    "    }\n",
    "\n",
    "print(\"Empirical bounds computed for all features.\")\n",
    "print(\"\\nExample bounds for key features:\")\n",
    "for feat in ['announcements', 'withdrawals', 'rare_ases_avg', 'edit_distance_avg']:\n",
    "    if feat in EMPIRICAL_BOUNDS:\n",
    "        print(f\"  {feat}: [{EMPIRICAL_BOUNDS[feat]['min']:.4f}, {EMPIRICAL_BOUNDS[feat]['max']:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Preprocessing: Log1p Transform for Heavy-Tailed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_log1p_transform(df, features):\n",
    "    \"\"\"\n",
    "    Apply log1p transform to heavy-tailed features.\n",
    "    \n",
    "    This helps SMOTE work better with count data that has high skewness.\n",
    "    log1p(x) = log(1 + x) handles zeros gracefully.\n",
    "    \n",
    "    Returns:\n",
    "        Transformed dataframe and dict of original stats for monitoring\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    original_stats = {}\n",
    "    \n",
    "    for col in features:\n",
    "        if col in result.columns:\n",
    "            original_stats[col] = {\n",
    "                'mean': result[col].mean(),\n",
    "                'std': result[col].std(),\n",
    "                'skew': result[col].skew(),\n",
    "                'max': result[col].max()\n",
    "            }\n",
    "            result[col] = np.log1p(result[col])\n",
    "            \n",
    "    return result, original_stats\n",
    "\n",
    "\n",
    "def apply_expm1_inverse(df, features):\n",
    "    \"\"\"\n",
    "    Apply inverse log1p transform (expm1).\n",
    "    expm1(x) = exp(x) - 1\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    for col in features:\n",
    "        if col in result.columns:\n",
    "            result[col] = np.expm1(result[col])\n",
    "            # Ensure non-negative after inverse transform\n",
    "            result[col] = np.maximum(result[col], 0)\n",
    "            \n",
    "    return result\n",
    "\n",
    "\n",
    "# Test transform on sample data\n",
    "print(\"Testing log1p transform on heavy-tailed features:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for col in HEAVY_TAILED_FEATURES:\n",
    "    if col in X_anomaly.columns:\n",
    "        original_skew = X_anomaly[col].skew()\n",
    "        transformed_skew = np.log1p(X_anomaly[col]).skew()\n",
    "        print(f\"{col}:\")\n",
    "        print(f\"  Original skewness: {original_skew:.2f}\")\n",
    "        print(f\"  Transformed skewness: {transformed_skew:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Enhanced Post-Processing with BGP Semantic Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_synthetic(df, empirical_bounds=None, enforce_bgp_constraints=True):\n",
    "    \"\"\"\n",
    "    Enhanced post-processing for synthetic data with:\n",
    "    1. Integer rounding and non-negative constraints\n",
    "    2. Empirical min/max clipping (not just >= 0)\n",
    "    3. BGP semantic constraints enforcement\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Synthetic data to post-process\n",
    "    empirical_bounds : dict\n",
    "        Dictionary with min/max bounds for each feature\n",
    "    enforce_bgp_constraints : bool\n",
    "        Whether to enforce BGP domain-specific constraints\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Step 1: Round integer features and ensure non-negative\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = result[col].round().astype(int).clip(lower=0)\n",
    "    \n",
    "    # Step 2: Ensure continuous features are non-negative\n",
    "    for col in CONTINUOUS_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = result[col].clip(lower=0)\n",
    "    \n",
    "    # Step 3: Apply empirical bounds clipping (especially for bounded ratios)\n",
    "    if empirical_bounds is not None:\n",
    "        for col in BOUNDED_RATIO_FEATURES:\n",
    "            if col in result.columns and col in empirical_bounds:\n",
    "                bounds = empirical_bounds[col]\n",
    "                result[col] = result[col].clip(\n",
    "                    lower=bounds['min'],\n",
    "                    upper=bounds['max']\n",
    "                )\n",
    "    \n",
    "    # Step 4: Enforce BGP semantic constraints\n",
    "    if enforce_bgp_constraints:\n",
    "        result = enforce_bgp_semantic_constraints(result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def enforce_bgp_semantic_constraints(df):\n",
    "    \"\"\"\n",
    "    Enforce BGP-specific structural constraints on synthetic data.\n",
    "    \n",
    "    Constraints:\n",
    "    1. nlri_ann <= announcements (NLRI count cannot exceed announcements)\n",
    "    2. origin_0 + origin_2 <= announcements (origin types are subset of announcements)\n",
    "    3. imp_wd_spath + imp_wd_dpath <= imp_wd (implicit withdrawals breakdown)\n",
    "    4. edit_distance_dict sums should be consistent with edit_distance_max\n",
    "    5. dups <= announcements (duplicates cannot exceed announcements)\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    n_violations = {}\n",
    "    \n",
    "    # Constraint 1: nlri_ann <= announcements\n",
    "    if 'nlri_ann' in result.columns and 'announcements' in result.columns:\n",
    "        mask = result['nlri_ann'] > result['announcements']\n",
    "        n_violations['nlri_ann > announcements'] = mask.sum()\n",
    "        result.loc[mask, 'nlri_ann'] = result.loc[mask, 'announcements']\n",
    "    \n",
    "    # Constraint 2: origin_0 + origin_2 <= announcements\n",
    "    if all(col in result.columns for col in ['origin_0', 'origin_2', 'announcements']):\n",
    "        origin_sum = result['origin_0'] + result['origin_2']\n",
    "        mask = origin_sum > result['announcements']\n",
    "        n_violations['origin_0 + origin_2 > announcements'] = mask.sum()\n",
    "        if mask.any():\n",
    "            # Scale down proportionally\n",
    "            scale_factor = result.loc[mask, 'announcements'] / origin_sum[mask]\n",
    "            result.loc[mask, 'origin_0'] = (result.loc[mask, 'origin_0'] * scale_factor).round().astype(int)\n",
    "            result.loc[mask, 'origin_2'] = (result.loc[mask, 'origin_2'] * scale_factor).round().astype(int)\n",
    "    \n",
    "    # Constraint 3: imp_wd_spath + imp_wd_dpath <= imp_wd\n",
    "    if all(col in result.columns for col in ['imp_wd_spath', 'imp_wd_dpath', 'imp_wd']):\n",
    "        imp_sum = result['imp_wd_spath'] + result['imp_wd_dpath']\n",
    "        mask = imp_sum > result['imp_wd']\n",
    "        n_violations['imp_wd_spath + imp_wd_dpath > imp_wd'] = mask.sum()\n",
    "        if mask.any():\n",
    "            scale_factor = result.loc[mask, 'imp_wd'] / imp_sum[mask].replace(0, 1)\n",
    "            result.loc[mask, 'imp_wd_spath'] = (result.loc[mask, 'imp_wd_spath'] * scale_factor).round().astype(int)\n",
    "            result.loc[mask, 'imp_wd_dpath'] = (result.loc[mask, 'imp_wd_dpath'] * scale_factor).round().astype(int)\n",
    "    \n",
    "    # Constraint 4: dups <= announcements\n",
    "    if 'dups' in result.columns and 'announcements' in result.columns:\n",
    "        mask = result['dups'] > result['announcements']\n",
    "        n_violations['dups > announcements'] = mask.sum()\n",
    "        result.loc[mask, 'dups'] = result.loc[mask, 'announcements']\n",
    "    \n",
    "    # Constraint 5: edit_distance_dict values should have max <= edit_distance_max\n",
    "    if 'edit_distance_max' in result.columns:\n",
    "        for ed_col in EDIT_DISTANCE_DICT_FEATURES:\n",
    "            if ed_col in result.columns:\n",
    "                # Individual edit distances shouldn't exceed max\n",
    "                # (This is a soft constraint - edit_distance_dict_i represents count at distance i)\n",
    "                pass  # This constraint is more complex and domain-specific\n",
    "    \n",
    "    # Report violations fixed\n",
    "    total_violations = sum(n_violations.values())\n",
    "    if total_violations > 0:\n",
    "        print(f\"\\nBGP constraint violations fixed: {total_violations}\")\n",
    "        for constraint, count in n_violations.items():\n",
    "            if count > 0:\n",
    "                print(f\"  {constraint}: {count}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "print(\"Post-processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. k_neighbors Validation and Safe SMOTE Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_k_neighbors(n_samples, requested_k, method_name=\"SMOTE\"):\n",
    "    \"\"\"\n",
    "    Validate and adjust k_neighbors to avoid silent failures.\n",
    "    \n",
    "    SMOTE requires k_neighbors < number of minority samples.\n",
    "    This function ensures safe k_neighbors selection and warns if adjusted.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Number of samples in the minority class\n",
    "    requested_k : int\n",
    "        Requested number of neighbors\n",
    "    method_name : str\n",
    "        Name of the method for logging\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    int : Safe k_neighbors value\n",
    "    \"\"\"\n",
    "    max_k = n_samples - 1\n",
    "    \n",
    "    if requested_k >= n_samples:\n",
    "        safe_k = max(1, max_k)\n",
    "        print(f\"WARNING [{method_name}]: k_neighbors ({requested_k}) >= n_samples ({n_samples})\")\n",
    "        print(f\"  Adjusting k_neighbors to {safe_k}\")\n",
    "        return safe_k\n",
    "    \n",
    "    return requested_k\n",
    "\n",
    "\n",
    "def get_safe_smote_config(n_minority_samples, base_k=5):\n",
    "    \"\"\"\n",
    "    Get safe SMOTE configuration based on available samples.\n",
    "    \n",
    "    Returns a dictionary with recommended parameters.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'k_neighbors': validate_k_neighbors(n_minority_samples, base_k),\n",
    "        'n_minority_samples': n_minority_samples,\n",
    "        'is_safe': n_minority_samples >= 2  # Minimum for SMOTE\n",
    "    }\n",
    "    \n",
    "    if not config['is_safe']:\n",
    "        print(f\"WARNING: Not enough minority samples ({n_minority_samples}) for SMOTE. Minimum is 2.\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "\n",
    "# Test validation\n",
    "print(\"Testing k_neighbors validation:\")\n",
    "print(\"=\"*50)\n",
    "test_cases = [(100, 5), (10, 5), (5, 5), (3, 5), (2, 5)]\n",
    "for n_samples, requested_k in test_cases:\n",
    "    safe_k = validate_k_neighbors(n_samples, requested_k, \"Test\")\n",
    "    print(f\"  n_samples={n_samples}, requested_k={requested_k} -> safe_k={safe_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Enhanced SMOTE Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_smote_enhanced(\n",
    "    X, \n",
    "    n_samples, \n",
    "    use_log_transform=True,\n",
    "    enforce_constraints=True,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced SMOTE generation with:\n",
    "    - Log1p transform for heavy-tailed features\n",
    "    - k_neighbors validation\n",
    "    - Empirical bounds clipping\n",
    "    - BGP constraint enforcement\n",
    "    \"\"\"\n",
    "    n_original = len(X)\n",
    "    \n",
    "    # Step 1: Apply log1p transform if requested\n",
    "    if use_log_transform:\n",
    "        X_work, _ = apply_log1p_transform(X.copy(), HEAVY_TAILED_FEATURES)\n",
    "    else:\n",
    "        X_work = X.copy()\n",
    "    \n",
    "    # Step 2: Create artificial labels with validated minority size\n",
    "    minority_size = max(10, int(n_original * 0.01))\n",
    "    \n",
    "    # Validate k_neighbors\n",
    "    safe_k = validate_k_neighbors(minority_size, 5, \"Standard SMOTE\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    minority_idx = np.random.choice(n_original, minority_size, replace=False)\n",
    "    \n",
    "    X_array = X_work.values\n",
    "    y = np.zeros(n_original)\n",
    "    y[minority_idx] = 1\n",
    "    \n",
    "    desired_minority = n_samples + minority_size\n",
    "    \n",
    "    # Step 3: Apply SMOTE with safe k_neighbors\n",
    "    smote = SMOTE(\n",
    "        sampling_strategy={1: desired_minority},\n",
    "        k_neighbors=safe_k,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_resampled, y_resampled = smote.fit_resample(X_array, y)\n",
    "    synthetic_samples = X_resampled[y_resampled == 1][minority_size:]\n",
    "    \n",
    "    result = pd.DataFrame(synthetic_samples, columns=X.columns)\n",
    "    \n",
    "    # Step 4: Apply inverse log transform if used\n",
    "    if use_log_transform:\n",
    "        result = apply_expm1_inverse(result, HEAVY_TAILED_FEATURES)\n",
    "    \n",
    "    # Step 5: Post-process with empirical bounds and BGP constraints\n",
    "    result = post_process_synthetic(\n",
    "        result, \n",
    "        empirical_bounds=EMPIRICAL_BOUNDS,\n",
    "        enforce_bgp_constraints=enforce_constraints\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_borderline_smote_enhanced(\n",
    "    X, \n",
    "    n_samples, \n",
    "    use_log_transform=True,\n",
    "    enforce_constraints=True,\n",
    "    kind='borderline-1',  # 'borderline-1' or 'borderline-2'\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced Borderline SMOTE generation.\n",
    "    \n",
    "    BorderlineSMOTE focuses on samples near the decision boundary,\n",
    "    which may be useful for model training (not necessarily for traffic simulation).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    kind : str\n",
    "        'borderline-1': Only oversample borderline samples\n",
    "        'borderline-2': Also consider samples from majority class\n",
    "    \"\"\"\n",
    "    n_original = len(X)\n",
    "    \n",
    "    if use_log_transform:\n",
    "        X_work, _ = apply_log1p_transform(X.copy(), HEAVY_TAILED_FEATURES)\n",
    "    else:\n",
    "        X_work = X.copy()\n",
    "    \n",
    "    minority_size = max(10, int(n_original * 0.01))\n",
    "    safe_k = validate_k_neighbors(minority_size, 5, \"Borderline SMOTE\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    minority_idx = np.random.choice(n_original, minority_size, replace=False)\n",
    "    \n",
    "    X_array = X_work.values\n",
    "    y = np.zeros(n_original)\n",
    "    y[minority_idx] = 1\n",
    "    \n",
    "    desired_minority = n_samples + minority_size\n",
    "    \n",
    "    borderline_smote = BorderlineSMOTE(\n",
    "        sampling_strategy={1: desired_minority},\n",
    "        k_neighbors=safe_k,\n",
    "        m_neighbors=min(10, n_original - 1),\n",
    "        kind=kind,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_resampled, y_resampled = borderline_smote.fit_resample(X_array, y)\n",
    "    synthetic_samples = X_resampled[y_resampled == 1][minority_size:]\n",
    "    \n",
    "    result = pd.DataFrame(synthetic_samples, columns=X.columns)\n",
    "    \n",
    "    if use_log_transform:\n",
    "        result = apply_expm1_inverse(result, HEAVY_TAILED_FEATURES)\n",
    "    \n",
    "    result = post_process_synthetic(\n",
    "        result,\n",
    "        empirical_bounds=EMPIRICAL_BOUNDS,\n",
    "        enforce_bgp_constraints=enforce_constraints\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_adasyn_enhanced(\n",
    "    X, \n",
    "    n_samples, \n",
    "    use_log_transform=True,\n",
    "    enforce_constraints=True,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced ADASYN generation.\n",
    "    Adaptively generates more samples in regions where density is low.\n",
    "    \"\"\"\n",
    "    n_original = len(X)\n",
    "    \n",
    "    if use_log_transform:\n",
    "        X_work, _ = apply_log1p_transform(X.copy(), HEAVY_TAILED_FEATURES)\n",
    "    else:\n",
    "        X_work = X.copy()\n",
    "    \n",
    "    minority_size = max(10, int(n_original * 0.01))\n",
    "    safe_k = validate_k_neighbors(minority_size, 5, \"ADASYN\")\n",
    "    \n",
    "    np.random.seed(random_state)\n",
    "    minority_idx = np.random.choice(n_original, minority_size, replace=False)\n",
    "    \n",
    "    X_array = X_work.values\n",
    "    y = np.zeros(n_original)\n",
    "    y[minority_idx] = 1\n",
    "    \n",
    "    desired_minority = n_samples + minority_size\n",
    "    \n",
    "    adasyn = ADASYN(\n",
    "        sampling_strategy={1: desired_minority},\n",
    "        n_neighbors=safe_k,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        X_resampled, y_resampled = adasyn.fit_resample(X_array, y)\n",
    "        synthetic_samples = X_resampled[y_resampled == 1][minority_size:]\n",
    "        result = pd.DataFrame(synthetic_samples, columns=X.columns)\n",
    "        \n",
    "        if use_log_transform:\n",
    "            result = apply_expm1_inverse(result, HEAVY_TAILED_FEATURES)\n",
    "        \n",
    "        result = post_process_synthetic(\n",
    "            result,\n",
    "            empirical_bounds=EMPIRICAL_BOUNDS,\n",
    "            enforce_bgp_constraints=enforce_constraints\n",
    "        )\n",
    "        return result\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"ADASYN failed: {e}. Falling back to standard SMOTE.\")\n",
    "        return generate_with_smote_enhanced(\n",
    "            X, n_samples, use_log_transform, enforce_constraints, random_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_kmeans_smote_enhanced(\n",
    "    X, \n",
    "    n_samples, \n",
    "    n_clusters=10, \n",
    "    use_log_transform=True,\n",
    "    enforce_constraints=True,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhanced K-Means clustering + SMOTE.\n",
    "    Clusters the data first, then applies SMOTE within each cluster.\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    if use_log_transform:\n",
    "        X_work, _ = apply_log1p_transform(X.copy(), HEAVY_TAILED_FEATURES)\n",
    "    else:\n",
    "        X_work = X.copy()\n",
    "    \n",
    "    X_array = X_work.values\n",
    "    \n",
    "    # Scale for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_array)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    cluster_sizes = np.bincount(cluster_labels)\n",
    "    samples_per_cluster = (cluster_sizes / cluster_sizes.sum() * n_samples).astype(int)\n",
    "    samples_per_cluster = np.maximum(samples_per_cluster, 1)\n",
    "    \n",
    "    synthetic_all = []\n",
    "    \n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        X_cluster = X_array[cluster_mask]\n",
    "        \n",
    "        if len(X_cluster) < 3:\n",
    "            continue\n",
    "            \n",
    "        n_to_generate = samples_per_cluster[cluster_id]\n",
    "        minority_size = max(2, int(len(X_cluster) * 0.1))\n",
    "        \n",
    "        # Validate k_neighbors for this cluster\n",
    "        safe_k = validate_k_neighbors(\n",
    "            minority_size, 3, f\"K-Means Cluster {cluster_id}\"\n",
    "        )\n",
    "        \n",
    "        if safe_k < 1:\n",
    "            continue\n",
    "            \n",
    "        minority_idx = np.random.choice(len(X_cluster), minority_size, replace=False)\n",
    "        \n",
    "        y_cluster = np.zeros(len(X_cluster))\n",
    "        y_cluster[minority_idx] = 1\n",
    "        \n",
    "        try:\n",
    "            smote = SMOTE(\n",
    "                sampling_strategy={1: n_to_generate + minority_size},\n",
    "                k_neighbors=safe_k,\n",
    "                random_state=random_state\n",
    "            )\n",
    "            X_res, y_res = smote.fit_resample(X_cluster, y_cluster)\n",
    "            synthetic = X_res[y_res == 1][minority_size:]\n",
    "            synthetic_all.append(synthetic)\n",
    "        except Exception as e:\n",
    "            print(f\"  Cluster {cluster_id} failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if synthetic_all:\n",
    "        synthetic_combined = np.vstack(synthetic_all)\n",
    "        result = pd.DataFrame(synthetic_combined[:n_samples], columns=X.columns)\n",
    "        \n",
    "        if use_log_transform:\n",
    "            result = apply_expm1_inverse(result, HEAVY_TAILED_FEATURES)\n",
    "        \n",
    "        result = post_process_synthetic(\n",
    "            result,\n",
    "            empirical_bounds=EMPIRICAL_BOUNDS,\n",
    "            enforce_bgp_constraints=enforce_constraints\n",
    "        )\n",
    "        return result\n",
    "    else:\n",
    "        print(\"K-Means SMOTE failed for all clusters. Falling back to standard SMOTE.\")\n",
    "        return generate_with_smote_enhanced(\n",
    "            X, n_samples, use_log_transform, enforce_constraints, random_state\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stratified SMOTE (Group-based Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stratified_smote(\n",
    "    df_full,\n",
    "    feature_cols,\n",
    "    group_col,\n",
    "    n_samples,\n",
    "    min_group_size=20,\n",
    "    use_log_transform=True,\n",
    "    enforce_constraints=True,\n",
    "    random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Stratified SMOTE: Apply SMOTE within subpopulations (e.g., by peer or prefix).\n",
    "    \n",
    "    This avoids mixing very different behaviors from different groups.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_full : pd.DataFrame\n",
    "        Full dataframe with features and grouping column\n",
    "    feature_cols : list\n",
    "        List of feature column names\n",
    "    group_col : str\n",
    "        Column to group by (e.g., 'peer_asn', 'prefix', 'Incident')\n",
    "    n_samples : int\n",
    "        Total number of samples to generate\n",
    "    min_group_size : int\n",
    "        Minimum group size to apply SMOTE (smaller groups are skipped)\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Get group sizes\n",
    "    group_sizes = df_full.groupby(group_col).size()\n",
    "    valid_groups = group_sizes[group_sizes >= min_group_size].index.tolist()\n",
    "    \n",
    "    print(f\"Stratified SMOTE by '{group_col}':\")\n",
    "    print(f\"  Total groups: {len(group_sizes)}\")\n",
    "    print(f\"  Valid groups (>= {min_group_size} samples): {len(valid_groups)}\")\n",
    "    \n",
    "    if len(valid_groups) == 0:\n",
    "        print(f\"  No valid groups found. Falling back to non-stratified SMOTE.\")\n",
    "        return generate_with_smote_enhanced(\n",
    "            df_full[feature_cols], n_samples, use_log_transform, \n",
    "            enforce_constraints, random_state\n",
    "        )\n",
    "    \n",
    "    # Calculate samples per group proportionally\n",
    "    valid_sizes = group_sizes[valid_groups]\n",
    "    samples_per_group = (valid_sizes / valid_sizes.sum() * n_samples).astype(int)\n",
    "    samples_per_group = np.maximum(samples_per_group, 1)\n",
    "    \n",
    "    synthetic_all = []\n",
    "    \n",
    "    for group_id in valid_groups:\n",
    "        group_df = df_full[df_full[group_col] == group_id]\n",
    "        X_group = group_df[feature_cols].copy()\n",
    "        n_to_generate = samples_per_group[group_id]\n",
    "        \n",
    "        if len(X_group) < 10:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            synthetic_group = generate_with_smote_enhanced(\n",
    "                X_group, \n",
    "                n_to_generate,\n",
    "                use_log_transform=use_log_transform,\n",
    "                enforce_constraints=enforce_constraints,\n",
    "                random_state=random_state + hash(str(group_id)) % 1000\n",
    "            )\n",
    "            synthetic_group[group_col] = group_id\n",
    "            synthetic_all.append(synthetic_group)\n",
    "        except Exception as e:\n",
    "            print(f\"  Group '{group_id}' failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if synthetic_all:\n",
    "        result = pd.concat(synthetic_all, ignore_index=True)\n",
    "        print(f\"  Generated {len(result)} samples across {len(synthetic_all)} groups\")\n",
    "        return result\n",
    "    else:\n",
    "        print(\"  All groups failed. Falling back to non-stratified SMOTE.\")\n",
    "        return generate_with_smote_enhanced(\n",
    "            df_full[feature_cols], n_samples, use_log_transform,\n",
    "            enforce_constraints, random_state\n",
    "        )\n",
    "\n",
    "\n",
    "print(\"Stratified SMOTE function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Distributional Validation Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kolmogorov_smirnov_tests(original, synthetic, features=None, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform Kolmogorov-Smirnov tests to compare distributions.\n",
    "    \n",
    "    The KS test measures the maximum difference between the empirical\n",
    "    cumulative distribution functions of two samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    original : pd.DataFrame\n",
    "        Original data\n",
    "    synthetic : pd.DataFrame\n",
    "        Synthetic data\n",
    "    features : list\n",
    "        Features to test (default: core validation features)\n",
    "    alpha : float\n",
    "        Significance level\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Results with KS statistic and p-value for each feature\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        features = CORE_VALIDATION_FEATURES\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for col in features:\n",
    "        if col in original.columns and col in synthetic.columns:\n",
    "            ks_stat, p_value = ks_2samp(original[col], synthetic[col])\n",
    "            \n",
    "            results.append({\n",
    "                'Feature': col,\n",
    "                'KS Statistic': ks_stat,\n",
    "                'P-Value': p_value,\n",
    "                'Significant Diff': 'Yes' if p_value < alpha else 'No',\n",
    "                'Quality': 'Good' if ks_stat < 0.1 else ('Fair' if ks_stat < 0.2 else 'Poor')\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def compare_correlation_matrices(original, synthetic, features=None):\n",
    "    \"\"\"\n",
    "    Compare correlation matrices between original and synthetic data.\n",
    "    \n",
    "    This checks if SMOTE preserves the dependency structure, not just marginals.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with correlation matrices and difference metrics\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        features = CORE_VALIDATION_FEATURES + ['origin_0', 'origin_2', 'dups']\n",
    "    \n",
    "    # Ensure features exist in both\n",
    "    common_features = [f for f in features if f in original.columns and f in synthetic.columns]\n",
    "    \n",
    "    corr_original = original[common_features].corr()\n",
    "    corr_synthetic = synthetic[common_features].corr()\n",
    "    \n",
    "    # Calculate difference metrics\n",
    "    corr_diff = np.abs(corr_original - corr_synthetic)\n",
    "    \n",
    "    # Frobenius norm of difference (overall measure)\n",
    "    frobenius_norm = np.sqrt((corr_diff ** 2).sum().sum())\n",
    "    \n",
    "    # Max absolute difference\n",
    "    max_diff = corr_diff.values.max()\n",
    "    \n",
    "    # Mean absolute difference\n",
    "    mean_diff = corr_diff.values.mean()\n",
    "    \n",
    "    return {\n",
    "        'corr_original': corr_original,\n",
    "        'corr_synthetic': corr_synthetic,\n",
    "        'corr_diff': corr_diff,\n",
    "        'frobenius_norm': frobenius_norm,\n",
    "        'max_diff': max_diff,\n",
    "        'mean_diff': mean_diff,\n",
    "        'features': common_features\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_correlation_comparison(corr_results, figsize=(16, 5)):\n",
    "    \"\"\"\n",
    "    Plot correlation matrix comparison.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    \n",
    "    # Original correlation\n",
    "    sns.heatmap(\n",
    "        corr_results['corr_original'], \n",
    "        annot=True, fmt='.2f', cmap='coolwarm',\n",
    "        center=0, vmin=-1, vmax=1,\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    axes[0].set_title('Original Anomaly Data Correlation')\n",
    "    \n",
    "    # Synthetic correlation\n",
    "    sns.heatmap(\n",
    "        corr_results['corr_synthetic'], \n",
    "        annot=True, fmt='.2f', cmap='coolwarm',\n",
    "        center=0, vmin=-1, vmax=1,\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    axes[1].set_title('Synthetic Anomaly Data Correlation')\n",
    "    \n",
    "    # Difference\n",
    "    sns.heatmap(\n",
    "        corr_results['corr_diff'], \n",
    "        annot=True, fmt='.2f', cmap='Reds',\n",
    "        vmin=0, vmax=0.5,\n",
    "        ax=axes[2]\n",
    "    )\n",
    "    axes[2].set_title(f\"Absolute Difference (Mean: {corr_results['mean_diff']:.3f})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_distribution_comparison(original, synthetic, features=None, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Plot distribution comparisons for key features.\n",
    "    \"\"\"\n",
    "    if features is None:\n",
    "        features = CORE_VALIDATION_FEATURES\n",
    "    \n",
    "    n_features = len(features)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_features + 1) // 2\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(features):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Plot histograms\n",
    "        ax.hist(original[col], bins=50, alpha=0.5, label='Original', density=True)\n",
    "        ax.hist(synthetic[col], bins=50, alpha=0.5, label='Synthetic', density=True)\n",
    "        \n",
    "        # Add KS statistic\n",
    "        ks_stat, p_value = ks_2samp(original[col], synthetic[col])\n",
    "        ax.set_title(f'{col}\\nKS={ks_stat:.3f}, p={p_value:.3e}')\n",
    "        ax.legend()\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for i in range(n_features, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "print(\"Distributional validation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Enhanced Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_statistics_enhanced(original, synthetic, name):\n",
    "    \"\"\"\n",
    "    Enhanced comparison with more metrics and all features.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Enhanced Comparison: {name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    results = []\n",
    "    for col in ALL_FEATURES:\n",
    "        if col not in original.columns or col not in synthetic.columns:\n",
    "            continue\n",
    "            \n",
    "        orig_mean = original[col].mean()\n",
    "        orig_std = original[col].std()\n",
    "        orig_median = original[col].median()\n",
    "        orig_skew = original[col].skew()\n",
    "        \n",
    "        synth_mean = synthetic[col].mean()\n",
    "        synth_std = synthetic[col].std()\n",
    "        synth_median = synthetic[col].median()\n",
    "        synth_skew = synthetic[col].skew()\n",
    "        \n",
    "        mean_diff = abs(orig_mean - synth_mean) / (abs(orig_mean) + 1e-10) * 100\n",
    "        std_diff = abs(orig_std - synth_std) / (abs(orig_std) + 1e-10) * 100\n",
    "        median_diff = abs(orig_median - synth_median) / (abs(orig_median) + 1e-10) * 100\n",
    "        \n",
    "        # KS test\n",
    "        ks_stat, ks_pval = ks_2samp(original[col], synthetic[col])\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': col,\n",
    "            'Orig Mean': f'{orig_mean:.2f}',\n",
    "            'Synth Mean': f'{synth_mean:.2f}',\n",
    "            'Mean Diff %': f'{mean_diff:.1f}%',\n",
    "            'Orig Std': f'{orig_std:.2f}',\n",
    "            'Synth Std': f'{synth_std:.2f}',\n",
    "            'Std Diff %': f'{std_diff:.1f}%',\n",
    "            'KS Stat': f'{ks_stat:.3f}',\n",
    "            'Quality': 'Good' if ks_stat < 0.1 else ('Fair' if ks_stat < 0.2 else 'Poor')\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "print(\"Enhanced statistics comparison function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Synthetic Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "N_SYNTHETIC = 20000  # Number of synthetic samples to generate\n",
    "USE_LOG_TRANSFORM = True  # Apply log1p transform for heavy-tailed features\n",
    "ENFORCE_BGP_CONSTRAINTS = True  # Enforce BGP semantic constraints\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  N_SYNTHETIC: {N_SYNTHETIC}\")\n",
    "print(f\"  USE_LOG_TRANSFORM: {USE_LOG_TRANSFORM}\")\n",
    "print(f\"  ENFORCE_BGP_CONSTRAINTS: {ENFORCE_BGP_CONSTRAINTS}\")\n",
    "print(f\"\\nOriginal high confidence anomaly samples: {len(X_anomaly)}\")\n",
    "print(f\"\\nGenerating synthetic anomaly samples...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Enhanced Standard SMOTE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating with Enhanced Standard SMOTE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "synthetic_smote = generate_with_smote_enhanced(\n",
    "    X_anomaly, \n",
    "    N_SYNTHETIC,\n",
    "    use_log_transform=USE_LOG_TRANSFORM,\n",
    "    enforce_constraints=ENFORCE_BGP_CONSTRAINTS\n",
    ")\n",
    "print(f\"Generated: {len(synthetic_smote)} samples\")\n",
    "synthetic_smote.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Enhanced Borderline SMOTE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating with Enhanced Borderline SMOTE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "synthetic_borderline = generate_with_borderline_smote_enhanced(\n",
    "    X_anomaly, \n",
    "    N_SYNTHETIC,\n",
    "    use_log_transform=USE_LOG_TRANSFORM,\n",
    "    enforce_constraints=ENFORCE_BGP_CONSTRAINTS,\n",
    "    kind='borderline-1'\n",
    ")\n",
    "print(f\"Generated: {len(synthetic_borderline)} samples\")\n",
    "synthetic_borderline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Enhanced ADASYN\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating with Enhanced ADASYN...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "synthetic_adasyn = generate_with_adasyn_enhanced(\n",
    "    X_anomaly, \n",
    "    N_SYNTHETIC,\n",
    "    use_log_transform=USE_LOG_TRANSFORM,\n",
    "    enforce_constraints=ENFORCE_BGP_CONSTRAINTS\n",
    ")\n",
    "print(f\"Generated: {len(synthetic_adasyn)} samples\")\n",
    "synthetic_adasyn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: K-Means SMOTE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Generating with K-Means SMOTE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "synthetic_kmeans = generate_with_kmeans_smote_enhanced(\n",
    "    X_anomaly, \n",
    "    N_SYNTHETIC,\n",
    "    n_clusters=10,\n",
    "    use_log_transform=USE_LOG_TRANSFORM,\n",
    "    enforce_constraints=ENFORCE_BGP_CONSTRAINTS\n",
    ")\n",
    "print(f\"Generated: {len(synthetic_kmeans)} samples\")\n",
    "synthetic_kmeans.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5: Stratified SMOTE (if grouping column available)\n",
    "# Try using 'Incident' as grouping column if available\n",
    "synthetic_stratified = None\n",
    "\n",
    "if 'Incident' in df_high_confidence.columns:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Generating with Stratified SMOTE (by Incident)...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    synthetic_stratified = generate_stratified_smote(\n",
    "        df_high_confidence,\n",
    "        ALL_FEATURES,\n",
    "        'Incident',\n",
    "        N_SYNTHETIC,\n",
    "        min_group_size=20,\n",
    "        use_log_transform=USE_LOG_TRANSFORM,\n",
    "        enforce_constraints=ENFORCE_BGP_CONSTRAINTS\n",
    "    )\n",
    "    print(f\"Generated: {len(synthetic_stratified)} samples\")\n",
    "else:\n",
    "    print(\"\\nNo 'Incident' column found - skipping stratified SMOTE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Validation: Verify Data Types and Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_synthetic_data(synthetic_df, original_df, method_name):\n",
    "    \"\"\"\n",
    "    Validate synthetic data quality.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Validation: {method_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    issues = []\n",
    "    \n",
    "    # Check 1: Negative values\n",
    "    for col in synthetic_df.columns:\n",
    "        if (synthetic_df[col] < 0).any():\n",
    "            n_negative = (synthetic_df[col] < 0).sum()\n",
    "            issues.append(f\"  {col}: {n_negative} negative values\")\n",
    "    \n",
    "    if issues:\n",
    "        print(\"Negative values found:\")\n",
    "        for issue in issues:\n",
    "            print(issue)\n",
    "    else:\n",
    "        print(\"No negative values found\")\n",
    "    \n",
    "    # Check 2: Integer features are integers\n",
    "    non_int_issues = []\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in synthetic_df.columns:\n",
    "            if not (synthetic_df[col] == synthetic_df[col].astype(int)).all():\n",
    "                non_int_issues.append(col)\n",
    "    \n",
    "    if non_int_issues:\n",
    "        print(f\"Non-integer values in integer features: {non_int_issues}\")\n",
    "    else:\n",
    "        print(\"All integer features are properly rounded\")\n",
    "    \n",
    "    # Check 3: BGP constraint violations\n",
    "    violations = {}\n",
    "    \n",
    "    if 'nlri_ann' in synthetic_df.columns and 'announcements' in synthetic_df.columns:\n",
    "        mask = synthetic_df['nlri_ann'] > synthetic_df['announcements']\n",
    "        if mask.any():\n",
    "            violations['nlri_ann > announcements'] = mask.sum()\n",
    "    \n",
    "    if all(col in synthetic_df.columns for col in ['origin_0', 'origin_2', 'announcements']):\n",
    "        mask = (synthetic_df['origin_0'] + synthetic_df['origin_2']) > synthetic_df['announcements']\n",
    "        if mask.any():\n",
    "            violations['origin_0 + origin_2 > announcements'] = mask.sum()\n",
    "    \n",
    "    if violations:\n",
    "        print(f\"BGP constraint violations: {violations}\")\n",
    "    else:\n",
    "        print(\"All BGP constraints satisfied\")\n",
    "    \n",
    "    return len(issues) == 0 and len(non_int_issues) == 0 and len(violations) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate all methods\n",
    "methods = {\n",
    "    'Standard SMOTE': synthetic_smote,\n",
    "    'Borderline SMOTE': synthetic_borderline,\n",
    "    'ADASYN': synthetic_adasyn,\n",
    "    'K-Means SMOTE': synthetic_kmeans\n",
    "}\n",
    "\n",
    "if synthetic_stratified is not None:\n",
    "    # For stratified, we need to exclude the grouping column\n",
    "    methods['Stratified SMOTE'] = synthetic_stratified[ALL_FEATURES]\n",
    "\n",
    "validation_results = {}\n",
    "for name, synthetic in methods.items():\n",
    "    validation_results[name] = validate_synthetic_data(synthetic, X_anomaly, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Distributional Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS tests for all methods\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Kolmogorov-Smirnov Tests for All Methods\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ks_results_all = {}\n",
    "for name, synthetic in methods.items():\n",
    "    print(f\"\\n{name}:\")\n",
    "    ks_df = kolmogorov_smirnov_tests(X_anomaly, synthetic, ALL_FEATURES)\n",
    "    ks_results_all[name] = ks_df\n",
    "    \n",
    "    # Summary\n",
    "    n_good = (ks_df['Quality'] == 'Good').sum()\n",
    "    n_fair = (ks_df['Quality'] == 'Fair').sum()\n",
    "    n_poor = (ks_df['Quality'] == 'Poor').sum()\n",
    "    print(f\"  Quality Summary: Good={n_good}, Fair={n_fair}, Poor={n_poor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation comparison for best method (Standard SMOTE)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Correlation Matrix Comparison (Standard SMOTE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "corr_results = compare_correlation_matrices(X_anomaly, synthetic_smote)\n",
    "print(f\"\\nCorrelation preservation metrics:\")\n",
    "print(f\"  Frobenius norm of difference: {corr_results['frobenius_norm']:.4f}\")\n",
    "print(f\"  Max absolute difference: {corr_results['max_diff']:.4f}\")\n",
    "print(f\"  Mean absolute difference: {corr_results['mean_diff']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation comparison\n",
    "fig = plot_correlation_comparison(corr_results)\n",
    "plt.suptitle('Correlation Matrix Comparison: Original vs Synthetic Anomaly Data', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution comparison\n",
    "fig = plot_distribution_comparison(X_anomaly, synthetic_smote, CORE_VALIDATION_FEATURES)\n",
    "plt.suptitle('Distribution Comparison: Standard SMOTE', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Enhanced Statistics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed statistics comparison for each method\n",
    "stats_comparison = {}\n",
    "\n",
    "for name, synthetic in methods.items():\n",
    "    stats_df = compare_statistics_enhanced(X_anomaly, synthetic, name)\n",
    "    stats_comparison[name] = stats_df\n",
    "    print(stats_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Quality Summary Across Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"QUALITY SUMMARY ACROSS ALL METHODS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "summary_data = []\n",
    "for name, ks_df in ks_results_all.items():\n",
    "    avg_ks = ks_df['KS Statistic'].mean()\n",
    "    n_good = (ks_df['Quality'] == 'Good').sum()\n",
    "    n_fair = (ks_df['Quality'] == 'Fair').sum()\n",
    "    n_poor = (ks_df['Quality'] == 'Poor').sum()\n",
    "    \n",
    "    # Correlation preservation for this method\n",
    "    corr_result = compare_correlation_matrices(X_anomaly, methods[name])\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Method': name,\n",
    "        'Avg KS Stat': f'{avg_ks:.4f}',\n",
    "        'Good Features': n_good,\n",
    "        'Fair Features': n_fair,\n",
    "        'Poor Features': n_poor,\n",
    "        'Corr Diff': f\"{corr_result['mean_diff']:.4f}\",\n",
    "        'Validated': 'Yes' if validation_results[name] else 'No'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommend best method\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find method with lowest average KS statistic\n",
    "best_method = min(ks_results_all.items(), key=lambda x: x[1]['KS Statistic'].mean())\n",
    "print(f\"\\nBest method based on KS statistic: {best_method[0]}\")\n",
    "print(f\"Average KS statistic: {best_method[1]['KS Statistic'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Synthetic Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create output directory\n",
    "OUTPUT_DIR = '/home/smotaali/BGP_Traffic_Generation/results/synthetic_anomaly_data'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(\"\\nSaving synthetic anomaly data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all methods\n",
    "for name, synthetic in methods.items():\n",
    "    # Create filename from method name\n",
    "    filename = name.lower().replace(' ', '_').replace('-', '_') + '_anomaly.csv'\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    synthetic.to_csv(filepath, index=False)\n",
    "    print(f\"Saved: {filepath} ({len(synthetic)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataset (original + best synthetic)\n",
    "print(\"\\nCreating combined dataset with original + synthetic data...\")\n",
    "\n",
    "# Use Standard SMOTE as default best method\n",
    "best_synthetic = synthetic_smote.copy()\n",
    "\n",
    "# Add labels\n",
    "df_original_labeled = X_anomaly.copy()\n",
    "df_original_labeled['data_source'] = 'original'\n",
    "df_original_labeled['label'] = 'anomaly'\n",
    "\n",
    "df_synthetic_labeled = best_synthetic.copy()\n",
    "df_synthetic_labeled['data_source'] = 'synthetic'\n",
    "df_synthetic_labeled['label'] = 'anomaly'\n",
    "\n",
    "# Combine\n",
    "df_combined = pd.concat([df_original_labeled, df_synthetic_labeled], ignore_index=True)\n",
    "\n",
    "# Save combined\n",
    "combined_filepath = os.path.join(OUTPUT_DIR, 'combined_anomaly_data.csv')\n",
    "df_combined.to_csv(combined_filepath, index=False)\n",
    "print(f\"Saved combined data: {combined_filepath}\")\n",
    "print(f\"  Original samples: {len(X_anomaly)}\")\n",
    "print(f\"  Synthetic samples: {len(best_synthetic)}\")\n",
    "print(f\"  Total: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SYNTHETIC ANOMALY DATA GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nInput Data:\")\n",
    "print(f\"  Source: {DATA_PATH}\")\n",
    "print(f\"  Confidence labels used: {HIGH_CONFIDENCE_LABELS}\")\n",
    "print(f\"  Original high-confidence anomaly samples: {len(X_anomaly)}\")\n",
    "\n",
    "print(f\"\\nGeneration Settings:\")\n",
    "print(f\"  Target synthetic samples: {N_SYNTHETIC}\")\n",
    "print(f\"  Log transform: {USE_LOG_TRANSFORM}\")\n",
    "print(f\"  BGP constraints: {ENFORCE_BGP_CONSTRAINTS}\")\n",
    "\n",
    "print(f\"\\nMethods used:\")\n",
    "for name in methods.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "print(f\"\\nOutput files saved to: {OUTPUT_DIR}\")\n",
    "for f in os.listdir(OUTPUT_DIR):\n",
    "    filepath = os.path.join(OUTPUT_DIR, f)\n",
    "    size = os.path.getsize(filepath) / 1024  # KB\n",
    "    print(f\"  - {f} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of final synthetic data\n",
    "print(\"\\nSample of generated synthetic anomaly data (Standard SMOTE):\")\n",
    "synthetic_smote.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
