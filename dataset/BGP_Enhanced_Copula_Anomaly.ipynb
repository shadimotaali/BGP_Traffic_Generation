{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import ks_2samp, gaussian_kde, spearmanr, pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - ANOMALY TRAFFIC GENERATION\n",
    "# =============================================================================\n",
    "\n",
    "# Data paths - ANOMALY DATA\n",
    "REAL_DATA_PATH = '/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS/all_incidents_anomalies_reinforced_v2.csv'\n",
    "OUTPUT_DIR = '/home/smotaali/BGP_Traffic_Generation/results/Copula_Anomaly/'\n",
    "\n",
    "# High confidence anomaly labels to filter\n",
    "HIGH_CONFIDENCE_LABELS = ['medium_confidence', 'high_confidence', 'very_high_confidence']\n",
    "\n",
    "# Generation settings\n",
    "N_SYNTHETIC = 20000  # Number of samples to generate\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# All 27 BGP features\n",
    "ALL_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_avg', 'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'rare_ases_avg',\n",
    "    'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Integer features (must be rounded)\n",
    "INTEGER_FEATURES = [\n",
    "    'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "    'origin_0', 'origin_2', 'origin_changes',\n",
    "    'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "    'as_path_max', 'unique_as_path_max',\n",
    "    'edit_distance_max',\n",
    "    'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "    'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "    'edit_distance_dict_6',\n",
    "    'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "    'number_rare_ases', 'nadas', 'flaps'\n",
    "]\n",
    "\n",
    "# Zero-inflated features (high proportion of zeros)\n",
    "ZERO_INFLATED_FEATURES = ['flaps', 'nadas', 'imp_wd', 'number_rare_ases']\n",
    "\n",
    "# Heavy-tailed features (need special handling)\n",
    "HEAVY_TAILED_FEATURES = ['unique_as_path_max', 'edit_distance_max', 'rare_ases_avg', 'as_path_max']\n",
    "\n",
    "print(\"Configuration loaded for ANOMALY TRAFFIC GENERATION!\")\n",
    "print(f\"  Data path: {REAL_DATA_PATH}\")\n",
    "print(f\"  Confidence labels: {HIGH_CONFIDENCE_LABELS}\")\n",
    "print(f\"  Total features: {len(ALL_FEATURES)}\")\n",
    "print(f\"  Integer features: {len(INTEGER_FEATURES)}\")\n",
    "print(f\"  Zero-inflated features: {len(ZERO_INFLATED_FEATURES)}\")\n",
    "print(f\"  Heavy-tailed features: {len(HEAVY_TAILED_FEATURES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Real Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOAD ANOMALY DATA\n",
    "# =============================================================================\n",
    "\n",
    "try:\n",
    "    df_raw = pd.read_csv(REAL_DATA_PATH)\n",
    "    print(f\"Loaded raw data: {df_raw.shape}\")\n",
    "    print(f\"\\nColumns: {df_raw.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {REAL_DATA_PATH}\")\n",
    "    print(\"Please update REAL_DATA_PATH to your data location\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check confidence label distribution\n",
    "print(\"Confidence label distribution:\")\n",
    "print(df_raw['confidence_label'].value_counts())\n",
    "\n",
    "if 'label' in df_raw.columns:\n",
    "    print(\"\\nOriginal label distribution:\")\n",
    "    print(df_raw['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for high confidence anomaly labels\n",
    "df_filtered = df_raw[df_raw['confidence_label'].isin(HIGH_CONFIDENCE_LABELS)].copy()\n",
    "print(f\"\\nFiltered to high confidence anomalies: {len(df_filtered)} samples\")\n",
    "print(f\"Distribution within filtered data:\")\n",
    "print(df_filtered['confidence_label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "available_features = [f for f in ALL_FEATURES if f in df_filtered.columns]\n",
    "missing_features = [f for f in ALL_FEATURES if f not in df_filtered.columns]\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"WARNING: Missing features: {missing_features}\")\n",
    "    # Update feature lists\n",
    "    ALL_FEATURES = available_features\n",
    "    INTEGER_FEATURES = [f for f in INTEGER_FEATURES if f in available_features]\n",
    "    ZERO_INFLATED_FEATURES = [f for f in ZERO_INFLATED_FEATURES if f in available_features]\n",
    "    HEAVY_TAILED_FEATURES = [f for f in HEAVY_TAILED_FEATURES if f in available_features]\n",
    "\n",
    "X_real = df_filtered[available_features].copy()\n",
    "X_real = X_real.fillna(X_real.median())\n",
    "\n",
    "print(f\"\\nUsing {len(available_features)} features\")\n",
    "print(f\"Anomaly data shape: {X_real.shape}\")\n",
    "print(f\"\\nFeature statistics:\")\n",
    "X_real.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Feature Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANALYZE FEATURE CHARACTERISTICS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_features(X):\n",
    "    \"\"\"Analyze each feature's characteristics for optimal copula configuration.\"\"\"\n",
    "    analysis = []\n",
    "    \n",
    "    for col in X.columns:\n",
    "        vals = X[col].dropna().values\n",
    "        \n",
    "        # Basic stats\n",
    "        zero_ratio = (vals == 0).mean()\n",
    "        unique_ratio = len(np.unique(vals)) / len(vals)\n",
    "        skewness = stats.skew(vals)\n",
    "        kurtosis = stats.kurtosis(vals)\n",
    "        \n",
    "        # Determine characteristics\n",
    "        is_zero_inflated = zero_ratio > 0.2\n",
    "        is_heavy_tailed = kurtosis > 3 or skewness > 2\n",
    "        is_discrete = unique_ratio < 0.1 or col in INTEGER_FEATURES\n",
    "        \n",
    "        analysis.append({\n",
    "            'feature': col,\n",
    "            'zero_ratio': zero_ratio,\n",
    "            'unique_ratio': unique_ratio,\n",
    "            'skewness': skewness,\n",
    "            'kurtosis': kurtosis,\n",
    "            'is_zero_inflated': is_zero_inflated,\n",
    "            'is_heavy_tailed': is_heavy_tailed,\n",
    "            'is_discrete': is_discrete\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(analysis)\n",
    "\n",
    "feature_analysis = analyze_features(X_real)\n",
    "\n",
    "print(\"Feature Analysis Summary for ANOMALY Data:\")\n",
    "print(f\"  Zero-inflated features: {feature_analysis['is_zero_inflated'].sum()}\")\n",
    "print(f\"  Heavy-tailed features: {feature_analysis['is_heavy_tailed'].sum()}\")\n",
    "print(f\"  Discrete features: {feature_analysis['is_discrete'].sum()}\")\n",
    "\n",
    "# Show problematic features\n",
    "print(\"\\nFeatures needing special handling:\")\n",
    "problematic = feature_analysis[\n",
    "    feature_analysis['is_zero_inflated'] | feature_analysis['is_heavy_tailed']\n",
    "][['feature', 'zero_ratio', 'skewness', 'kurtosis']]\n",
    "print(problematic.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Copula Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYBRID CORRELATION (PEARSON + SPEARMAN)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_hybrid_correlation(X, pearson_weight=0.3, spearman_weight=0.7):\n",
    "    \"\"\"\n",
    "    Compute hybrid correlation matrix using both Pearson and Spearman.\n",
    "    \n",
    "    Why use both?\n",
    "    - Pearson: Captures linear relationships, sensitive to outliers\n",
    "    - Spearman: Captures monotonic (non-linear) relationships, robust to outliers\n",
    "    \n",
    "    For BGP anomaly data with heavy tails and non-linear relationships,\n",
    "    Spearman should have higher weight.\n",
    "    \"\"\"\n",
    "    assert abs(pearson_weight + spearman_weight - 1.0) < 1e-6, \"Weights must sum to 1\"\n",
    "    \n",
    "    # Compute Pearson correlation\n",
    "    pearson_corr = X.corr(method='pearson').values\n",
    "    \n",
    "    # Compute Spearman correlation\n",
    "    spearman_corr = X.corr(method='spearman').values\n",
    "    \n",
    "    # Handle NaN values\n",
    "    pearson_corr = np.nan_to_num(pearson_corr, nan=0.0)\n",
    "    spearman_corr = np.nan_to_num(spearman_corr, nan=0.0)\n",
    "    \n",
    "    # Weighted combination\n",
    "    hybrid_corr = pearson_weight * pearson_corr + spearman_weight * spearman_corr\n",
    "    \n",
    "    # Ensure diagonal is 1\n",
    "    np.fill_diagonal(hybrid_corr, 1.0)\n",
    "    \n",
    "    # Ensure positive definiteness\n",
    "    eigvals, eigvecs = np.linalg.eigh(hybrid_corr)\n",
    "    eigvals = np.maximum(eigvals, 1e-6)\n",
    "    hybrid_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "    \n",
    "    # Normalize to correlation matrix\n",
    "    d = np.sqrt(np.diag(hybrid_corr))\n",
    "    hybrid_corr = hybrid_corr / np.outer(d, d)\n",
    "    np.fill_diagonal(hybrid_corr, 1.0)\n",
    "    \n",
    "    return hybrid_corr\n",
    "\n",
    "print(\"Hybrid correlation function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MARGINAL ESTIMATION (KDE + EMPIRICAL)\n",
    "# =============================================================================\n",
    "\n",
    "class MarginalEstimator:\n",
    "    \"\"\"\n",
    "    Estimates marginal distributions using adaptive methods.\n",
    "    \n",
    "    - KDE for continuous features with many unique values\n",
    "    - Empirical CDF for discrete/low-cardinality features\n",
    "    - Special handling for zero-inflated features\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_kde=True, handle_zeros=True, log_transform_heavy=True, kde_bandwidth_factor=1.0):\n",
    "        self.use_kde = use_kde\n",
    "        self.handle_zeros = handle_zeros\n",
    "        self.log_transform_heavy = log_transform_heavy\n",
    "        self.kde_bandwidth_factor = kde_bandwidth_factor\n",
    "        self.marginals = {}\n",
    "    \n",
    "    def fit(self, X, feature_analysis=None):\n",
    "        \"\"\"\n",
    "        Fit marginal distributions for each feature.\n",
    "        \"\"\"\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.n_features = len(self.feature_names)\n",
    "        \n",
    "        for col in self.feature_names:\n",
    "            vals = X[col].values.copy()\n",
    "            \n",
    "            # Analyze feature\n",
    "            zero_ratio = (vals == 0).mean()\n",
    "            n_unique = len(np.unique(vals))\n",
    "            is_heavy = col in HEAVY_TAILED_FEATURES or (vals.max() > 100 and stats.skew(vals) > 2)\n",
    "            \n",
    "            marginal_info = {\n",
    "                'zero_ratio': zero_ratio,\n",
    "                'is_zero_inflated': zero_ratio > 0.2,\n",
    "                'is_heavy_tailed': is_heavy,\n",
    "                'is_integer': col in INTEGER_FEATURES,\n",
    "                'original_values': vals,\n",
    "                'sorted_values': np.sort(vals),\n",
    "                'method': 'empirical'\n",
    "            }\n",
    "            \n",
    "            # Choose estimation method\n",
    "            if self.use_kde and n_unique > 20 and not marginal_info['is_integer']:\n",
    "                try:\n",
    "                    bw_method = lambda obj: obj.scotts_factor() * self.kde_bandwidth_factor\n",
    "                    \n",
    "                    if is_heavy and self.log_transform_heavy and vals.min() >= 0:\n",
    "                        log_vals = np.log1p(vals)\n",
    "                        kde = gaussian_kde(log_vals, bw_method=bw_method)\n",
    "                        marginal_info['kde'] = kde\n",
    "                        marginal_info['method'] = 'kde_log'\n",
    "                        marginal_info['log_sorted'] = np.sort(log_vals)\n",
    "                    else:\n",
    "                        kde = gaussian_kde(vals, bw_method=bw_method)\n",
    "                        marginal_info['kde'] = kde\n",
    "                        marginal_info['method'] = 'kde'\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            self.marginals[col] = marginal_info\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform_to_uniform(self, X):\n",
    "        \"\"\"\n",
    "        Transform data to uniform [0,1] using fitted marginals.\n",
    "        \"\"\"\n",
    "        n_samples = len(X)\n",
    "        uniform_data = np.zeros((n_samples, self.n_features))\n",
    "        \n",
    "        for i, col in enumerate(self.feature_names):\n",
    "            vals = X[col].values\n",
    "            info = self.marginals[col]\n",
    "            \n",
    "            ranks = stats.rankdata(vals, method='average')\n",
    "            uniform_data[:, i] = np.clip(ranks / (n_samples + 1), 0.001, 0.999)\n",
    "        \n",
    "        return uniform_data\n",
    "    \n",
    "    def inverse_transform(self, uniform_data, n_samples):\n",
    "        \"\"\"\n",
    "        Transform uniform samples back to original scale.\n",
    "        \"\"\"\n",
    "        synthetic_data = np.zeros((n_samples, self.n_features))\n",
    "        \n",
    "        for i, col in enumerate(self.feature_names):\n",
    "            info = self.marginals[col]\n",
    "            u = uniform_data[:, i]\n",
    "            \n",
    "            if self.handle_zeros and info['is_zero_inflated']:\n",
    "                synthetic_data[:, i] = self._inverse_zero_inflated(u, info)\n",
    "            else:\n",
    "                synthetic_data[:, i] = self._inverse_standard(u, info)\n",
    "            \n",
    "            synthetic_data[:, i] = np.maximum(0, synthetic_data[:, i])\n",
    "            \n",
    "            if info['is_integer']:\n",
    "                synthetic_data[:, i] = np.round(synthetic_data[:, i])\n",
    "        \n",
    "        return synthetic_data\n",
    "    \n",
    "    def _inverse_standard(self, u, info):\n",
    "        \"\"\"Standard inverse CDF using quantile function.\"\"\"\n",
    "        sorted_vals = info['sorted_values']\n",
    "        indices = (u * len(sorted_vals)).astype(int)\n",
    "        indices = np.clip(indices, 0, len(sorted_vals) - 1)\n",
    "        return sorted_vals[indices]\n",
    "    \n",
    "    def _inverse_zero_inflated(self, u, info):\n",
    "        \"\"\"Inverse CDF for zero-inflated features.\"\"\"\n",
    "        zero_ratio = info['zero_ratio']\n",
    "        original = info['original_values']\n",
    "        \n",
    "        result = np.zeros(len(u))\n",
    "        is_zero = u < zero_ratio\n",
    "        \n",
    "        non_zero_vals = original[original > 0]\n",
    "        if len(non_zero_vals) > 0:\n",
    "            scaled_u = (u[~is_zero] - zero_ratio) / (1 - zero_ratio + 1e-10)\n",
    "            scaled_u = np.clip(scaled_u, 0, 1)\n",
    "            \n",
    "            sorted_non_zero = np.sort(non_zero_vals)\n",
    "            indices = (scaled_u * len(sorted_non_zero)).astype(int)\n",
    "            indices = np.clip(indices, 0, len(sorted_non_zero) - 1)\n",
    "            result[~is_zero] = sorted_non_zero[indices]\n",
    "        \n",
    "        return result\n",
    "\n",
    "print(\"Marginal Estimator class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED COPULA GENERATOR\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedCopulaGenerator:\n",
    "    \"\"\"\n",
    "    Enhanced Copula Generator with multiple improvements:\n",
    "    \n",
    "    1. Hybrid correlation (Pearson + Spearman)\n",
    "    2. Adaptive marginal estimation (KDE + Empirical)\n",
    "    3. Zero-inflation handling\n",
    "    4. t-Copula option for heavy tails\n",
    "    5. Built-in constraint enforcement\n",
    "    6. KDE bandwidth tuning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 copula_type='gaussian',\n",
    "                 t_df=5,\n",
    "                 pearson_weight=0.3,\n",
    "                 spearman_weight=0.7,\n",
    "                 use_kde_marginals=True,\n",
    "                 kde_bandwidth_factor=1.0,\n",
    "                 handle_zeros=True,\n",
    "                 random_state=42):\n",
    "        \n",
    "        self.copula_type = copula_type\n",
    "        self.t_df = t_df\n",
    "        self.pearson_weight = pearson_weight\n",
    "        self.spearman_weight = spearman_weight\n",
    "        self.use_kde_marginals = use_kde_marginals\n",
    "        self.kde_bandwidth_factor = kde_bandwidth_factor\n",
    "        self.handle_zeros = handle_zeros\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.marginal_estimator = None\n",
    "        self.correlation_matrix = None\n",
    "        self.cholesky_L = None\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the copula model to real anomaly data.\n",
    "        \"\"\"\n",
    "        print(\"Fitting Enhanced Copula for ANOMALY data...\")\n",
    "        print(f\"  Configuration:\")\n",
    "        print(f\"    - Copula type: {self.copula_type}\" + \n",
    "              (f\" (df={self.t_df})\" if self.copula_type == 't' else \"\"))\n",
    "        print(f\"    - Correlation: {self.pearson_weight:.0%} Pearson + {self.spearman_weight:.0%} Spearman\")\n",
    "        print(f\"    - Marginals: {'KDE' if self.use_kde_marginals else 'Empirical'}\" +\n",
    "              (f\" (bw={self.kde_bandwidth_factor}x)\" if self.use_kde_marginals else \"\"))\n",
    "        print(f\"    - Zero handling: {self.handle_zeros}\")\n",
    "        \n",
    "        self.feature_names = X.columns.tolist()\n",
    "        self.n_features = len(self.feature_names)\n",
    "        \n",
    "        print(\"\\n  [1/3] Fitting marginal distributions...\")\n",
    "        self.marginal_estimator = MarginalEstimator(\n",
    "            use_kde=self.use_kde_marginals,\n",
    "            handle_zeros=self.handle_zeros,\n",
    "            kde_bandwidth_factor=self.kde_bandwidth_factor\n",
    "        )\n",
    "        self.marginal_estimator.fit(X)\n",
    "        \n",
    "        print(\"  [2/3] Computing hybrid correlation matrix...\")\n",
    "        self.correlation_matrix = compute_hybrid_correlation(\n",
    "            X, \n",
    "            pearson_weight=self.pearson_weight,\n",
    "            spearman_weight=self.spearman_weight\n",
    "        )\n",
    "        \n",
    "        print(\"  [3/3] Computing Cholesky decomposition...\")\n",
    "        try:\n",
    "            self.cholesky_L = np.linalg.cholesky(self.correlation_matrix)\n",
    "        except np.linalg.LinAlgError:\n",
    "            self.correlation_matrix += 0.01 * np.eye(self.n_features)\n",
    "            self.cholesky_L = np.linalg.cholesky(self.correlation_matrix)\n",
    "        \n",
    "        print(\"  Fitting complete!\")\n",
    "        return self\n",
    "    \n",
    "    def generate(self, n_samples):\n",
    "        \"\"\"\n",
    "        Generate synthetic anomaly samples.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        print(f\"\\nGenerating {n_samples} synthetic anomaly samples...\")\n",
    "        \n",
    "        if self.copula_type == 'gaussian':\n",
    "            independent_samples = np.random.randn(n_samples, self.n_features)\n",
    "        else:\n",
    "            chi2_samples = np.random.chisquare(self.t_df, n_samples) / self.t_df\n",
    "            independent_samples = np.random.randn(n_samples, self.n_features)\n",
    "            independent_samples = independent_samples / np.sqrt(chi2_samples)[:, np.newaxis]\n",
    "        \n",
    "        correlated_samples = independent_samples @ self.cholesky_L.T\n",
    "        \n",
    "        if self.copula_type == 'gaussian':\n",
    "            uniform_samples = stats.norm.cdf(correlated_samples)\n",
    "        else:\n",
    "            uniform_samples = stats.t.cdf(correlated_samples, df=self.t_df)\n",
    "        \n",
    "        synthetic_data = self.marginal_estimator.inverse_transform(uniform_samples, n_samples)\n",
    "        \n",
    "        synthetic_df = pd.DataFrame(synthetic_data, columns=self.feature_names)\n",
    "        \n",
    "        print(f\"  Generated shape: {synthetic_df.shape}\")\n",
    "        \n",
    "        return synthetic_df\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return configuration as dictionary.\"\"\"\n",
    "        return {\n",
    "            'copula_type': self.copula_type,\n",
    "            't_df': self.t_df,\n",
    "            'pearson_weight': self.pearson_weight,\n",
    "            'spearman_weight': self.spearman_weight,\n",
    "            'use_kde_marginals': self.use_kde_marginals,\n",
    "            'kde_bandwidth_factor': self.kde_bandwidth_factor,\n",
    "            'handle_zeros': self.handle_zeros\n",
    "        }\n",
    "\n",
    "print(\"EnhancedCopulaGenerator class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Post-Processing & Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BGP CONSTRAINT ENFORCEMENT\n",
    "# =============================================================================\n",
    "\n",
    "def enforce_bgp_constraints(synthetic, real, verbose=True):\n",
    "    \"\"\"\n",
    "    Enforce BGP domain-specific constraints.\n",
    "    \n",
    "    Constraints:\n",
    "    1. All features non-negative\n",
    "    2. Integer features properly rounded\n",
    "    3. origin_0 + origin_2 <= announcements\n",
    "    4. imp_wd_spath + imp_wd_dpath <= imp_wd\n",
    "    5. Values within realistic bounds\n",
    "    \"\"\"\n",
    "    result = synthetic.copy()\n",
    "    \n",
    "    violations = {\n",
    "        'nan_inf': 0,\n",
    "        'negative': 0,\n",
    "        'origin_constraint': 0,\n",
    "        'imp_wd_constraint': 0,\n",
    "        'bounds': 0\n",
    "    }\n",
    "    \n",
    "    # 1. Handle NaN/Inf\n",
    "    for col in result.columns:\n",
    "        nan_count = result[col].isna().sum()\n",
    "        if nan_count > 0:\n",
    "            violations['nan_inf'] += nan_count\n",
    "            if col in INTEGER_FEATURES:\n",
    "                result[col] = result[col].fillna(0)\n",
    "            else:\n",
    "                result[col] = result[col].fillna(real[col].median() if col in real.columns else 0)\n",
    "        \n",
    "        result[col] = result[col].replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    # 2. Non-negative\n",
    "    for col in result.columns:\n",
    "        neg_count = (result[col] < 0).sum()\n",
    "        if neg_count > 0:\n",
    "            violations['negative'] += neg_count\n",
    "            result[col] = np.maximum(0, result[col])\n",
    "    \n",
    "    # 3. Integer features\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = np.round(result[col]).astype(int)\n",
    "    \n",
    "    # 4. Origin constraint: origin_0 + origin_2 <= announcements\n",
    "    if all(c in result.columns for c in ['origin_0', 'origin_2', 'announcements']):\n",
    "        origin_sum = result['origin_0'] + result['origin_2']\n",
    "        excess = origin_sum > result['announcements']\n",
    "        violations['origin_constraint'] = excess.sum()\n",
    "        \n",
    "        if excess.any():\n",
    "            scale = result.loc[excess, 'announcements'] / (origin_sum[excess] + 1e-10)\n",
    "            result.loc[excess, 'origin_0'] = np.floor(result.loc[excess, 'origin_0'] * scale).astype(int)\n",
    "            result.loc[excess, 'origin_2'] = np.floor(result.loc[excess, 'origin_2'] * scale).astype(int)\n",
    "    \n",
    "    # 5. Implicit withdrawal constraint: imp_wd_spath + imp_wd_dpath <= imp_wd\n",
    "    if all(c in result.columns for c in ['imp_wd_spath', 'imp_wd_dpath', 'imp_wd']):\n",
    "        imp_sum = result['imp_wd_spath'] + result['imp_wd_dpath']\n",
    "        excess = imp_sum > result['imp_wd']\n",
    "        violations['imp_wd_constraint'] = excess.sum()\n",
    "        \n",
    "        if excess.any():\n",
    "            scale = result.loc[excess, 'imp_wd'] / (imp_sum[excess] + 1e-10)\n",
    "            result.loc[excess, 'imp_wd_spath'] = np.floor(result.loc[excess, 'imp_wd_spath'] * scale).astype(int)\n",
    "            result.loc[excess, 'imp_wd_dpath'] = np.floor(result.loc[excess, 'imp_wd_dpath'] * scale).astype(int)\n",
    "    \n",
    "    # 6. Realistic bounds (99.5th percentile)\n",
    "    for col in result.columns:\n",
    "        if col in real.columns:\n",
    "            upper = np.percentile(real[col].dropna(), 99.5) * 1.1\n",
    "            over = (result[col] > upper).sum()\n",
    "            if over > 0:\n",
    "                violations['bounds'] += over\n",
    "                result[col] = np.clip(result[col], 0, upper)\n",
    "    \n",
    "    # Final integer enforcement\n",
    "    for col in INTEGER_FEATURES:\n",
    "        if col in result.columns:\n",
    "            result[col] = np.maximum(0, np.round(result[col])).astype(int)\n",
    "    \n",
    "    if verbose:\n",
    "        total = sum(violations.values())\n",
    "        if total > 0:\n",
    "            print(f\"  Constraints fixed: {violations}\")\n",
    "        else:\n",
    "            print(\"  No constraint violations\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Constraint enforcement function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELATION ALIGNMENT POST-PROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "def align_correlation_structure(synthetic, real, n_iterations=10, learning_rate=0.3, verbose=True):\n",
    "    \"\"\"\n",
    "    Post-process synthetic data to better match real data's correlation structure.\n",
    "    \"\"\"\n",
    "    result = synthetic.copy()\n",
    "    features = list(result.columns)\n",
    "    n_features = len(features)\n",
    "    \n",
    "    original_ranks = {}\n",
    "    for col in features:\n",
    "        original_ranks[col] = stats.rankdata(result[col].values)\n",
    "    \n",
    "    target_corr = real[features].corr(method='pearson').values\n",
    "    target_corr = np.nan_to_num(target_corr, nan=0.0)\n",
    "    np.fill_diagonal(target_corr, 1.0)\n",
    "    \n",
    "    eigvals, eigvecs = np.linalg.eigh(target_corr)\n",
    "    eigvals = np.maximum(eigvals, 1e-6)\n",
    "    target_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "    d = np.sqrt(np.diag(target_corr))\n",
    "    target_corr = target_corr / np.outer(d, d)\n",
    "    np.fill_diagonal(target_corr, 1.0)\n",
    "    \n",
    "    try:\n",
    "        target_L = np.linalg.cholesky(target_corr)\n",
    "    except:\n",
    "        target_corr += 0.01 * np.eye(n_features)\n",
    "        target_L = np.linalg.cholesky(target_corr)\n",
    "    \n",
    "    if verbose:\n",
    "        current_corr = result[features].corr(method='pearson').values\n",
    "        current_flat = current_corr[np.triu_indices(n_features, k=1)]\n",
    "        target_flat = target_corr[np.triu_indices(n_features, k=1)]\n",
    "        initial_match = np.corrcoef(current_flat, target_flat)[0, 1]\n",
    "        print(f\"Correlation Alignment: Initial match = {initial_match:.4f}\")\n",
    "    \n",
    "    data = result[features].values.copy()\n",
    "    means = data.mean(axis=0)\n",
    "    stds = data.std(axis=0)\n",
    "    stds[stds == 0] = 1\n",
    "    Z = (data - means) / stds\n",
    "    \n",
    "    for iteration in range(n_iterations):\n",
    "        current_corr = np.corrcoef(Z.T)\n",
    "        current_corr = np.nan_to_num(current_corr, nan=0.0)\n",
    "        np.fill_diagonal(current_corr, 1.0)\n",
    "        \n",
    "        eigvals, eigvecs = np.linalg.eigh(current_corr)\n",
    "        eigvals = np.maximum(eigvals, 1e-6)\n",
    "        current_corr = eigvecs @ np.diag(eigvals) @ eigvecs.T\n",
    "        d = np.sqrt(np.diag(current_corr))\n",
    "        current_corr = current_corr / np.outer(d, d)\n",
    "        np.fill_diagonal(current_corr, 1.0)\n",
    "        \n",
    "        try:\n",
    "            current_L = np.linalg.cholesky(current_corr)\n",
    "        except:\n",
    "            current_corr += 0.01 * np.eye(n_features)\n",
    "            current_L = np.linalg.cholesky(current_corr)\n",
    "        \n",
    "        current_L_inv = np.linalg.inv(current_L)\n",
    "        blended_L = (1 - learning_rate) * current_L + learning_rate * target_L\n",
    "        \n",
    "        Z = Z @ current_L_inv.T @ blended_L.T\n",
    "        Z = (Z - Z.mean(axis=0)) / (Z.std(axis=0) + 1e-10)\n",
    "    \n",
    "    adjusted_data = np.zeros_like(Z)\n",
    "    for i, col in enumerate(features):\n",
    "        adjusted_ranks = stats.rankdata(Z[:, i])\n",
    "        sorted_original = np.sort(result[col].values)\n",
    "        rank_indices = ((adjusted_ranks - 1) / (len(adjusted_ranks) - 1) * (len(sorted_original) - 1)).astype(int)\n",
    "        rank_indices = np.clip(rank_indices, 0, len(sorted_original) - 1)\n",
    "        adjusted_data[:, i] = sorted_original[rank_indices]\n",
    "    \n",
    "    aligned_result = pd.DataFrame(adjusted_data, columns=features)\n",
    "    \n",
    "    if verbose:\n",
    "        final_corr = aligned_result[features].corr(method='pearson').values\n",
    "        final_flat = final_corr[np.triu_indices(n_features, k=1)]\n",
    "        final_match = np.corrcoef(final_flat, target_flat)[0, 1]\n",
    "        improvement = final_match - initial_match\n",
    "        print(f\"Correlation Alignment: Final match = {final_match:.4f} ({'+' if improvement >= 0 else ''}{improvement:.4f})\")\n",
    "    \n",
    "    return aligned_result\n",
    "\n",
    "\n",
    "def iterative_correlation_refinement(synthetic, real, target_corr_match=0.95, max_rounds=5, verbose=True):\n",
    "    \"\"\"\n",
    "    Apply multiple rounds of correlation alignment until target is reached.\n",
    "    \"\"\"\n",
    "    result = synthetic.copy()\n",
    "    features = list(result.columns)\n",
    "    n_features = len(features)\n",
    "    \n",
    "    target_corr = real[features].corr(method='pearson').values\n",
    "    target_flat = target_corr[np.triu_indices(n_features, k=1)]\n",
    "    target_flat = np.nan_to_num(target_flat, nan=0.0)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*60)\n",
    "        print(\"ITERATIVE CORRELATION REFINEMENT\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    for round_num in range(1, max_rounds + 1):\n",
    "        current_corr = result[features].corr(method='pearson').values\n",
    "        current_flat = current_corr[np.triu_indices(n_features, k=1)]\n",
    "        current_flat = np.nan_to_num(current_flat, nan=0.0)\n",
    "        \n",
    "        valid_mask = ~(np.isnan(current_flat) | np.isnan(target_flat))\n",
    "        if valid_mask.sum() > 1:\n",
    "            corr_match = np.corrcoef(current_flat[valid_mask], target_flat[valid_mask])[0, 1]\n",
    "        else:\n",
    "            corr_match = 0\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nRound {round_num}: Correlation match = {corr_match:.4f}\")\n",
    "        \n",
    "        if corr_match >= target_corr_match:\n",
    "            if verbose:\n",
    "                print(f\"Target reached ({target_corr_match:.2f})!\")\n",
    "            break\n",
    "        \n",
    "        lr = min(0.5, 0.2 + 0.1 * round_num)\n",
    "        result = align_correlation_structure(result, real, n_iterations=15, learning_rate=lr, verbose=False)\n",
    "    \n",
    "    final_corr = result[features].corr(method='pearson').values\n",
    "    final_flat = final_corr[np.triu_indices(n_features, k=1)]\n",
    "    final_flat = np.nan_to_num(final_flat, nan=0.0)\n",
    "    \n",
    "    valid_mask = ~(np.isnan(final_flat) | np.isnan(target_flat))\n",
    "    final_match = np.corrcoef(final_flat[valid_mask], target_flat[valid_mask])[0, 1] if valid_mask.sum() > 1 else 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal correlation match: {final_match:.4f}\")\n",
    "        print(\"=\"*60)\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"Correlation alignment functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "KS_EXCELLENT = 0.05\n",
    "KS_GOOD = 0.10\n",
    "KS_MODERATE = 0.15\n",
    "\n",
    "def evaluate_synthetic(real, synthetic, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate synthetic anomaly data quality.\n",
    "    \"\"\"\n",
    "    common_cols = [c for c in real.columns if c in synthetic.columns]\n",
    "    n_features = len(common_cols)\n",
    "    \n",
    "    ks_stats = {}\n",
    "    cohens_d = {}\n",
    "    wasserstein = {}\n",
    "    \n",
    "    for col in common_cols:\n",
    "        real_vals = real[col].dropna().values\n",
    "        syn_vals = synthetic[col].dropna().values\n",
    "        \n",
    "        if len(real_vals) == 0 or len(syn_vals) == 0:\n",
    "            continue\n",
    "        \n",
    "        ks_stat, _ = ks_2samp(syn_vals, real_vals)\n",
    "        ks_stats[col] = ks_stat\n",
    "        \n",
    "        pooled_std = np.sqrt((real_vals.std()**2 + syn_vals.std()**2) / 2)\n",
    "        if pooled_std > 0:\n",
    "            d = (syn_vals.mean() - real_vals.mean()) / pooled_std\n",
    "            d = np.clip(d, -10, 10)\n",
    "        else:\n",
    "            d = 0\n",
    "        cohens_d[col] = d\n",
    "        \n",
    "        r_range = real_vals.max() - real_vals.min()\n",
    "        s_range = syn_vals.max() - syn_vals.min()\n",
    "        r_norm = (real_vals - real_vals.min()) / (r_range + 1e-10) if r_range > 0 else np.zeros_like(real_vals)\n",
    "        s_norm = (syn_vals - syn_vals.min()) / (s_range + 1e-10) if s_range > 0 else np.zeros_like(syn_vals)\n",
    "        wasserstein[col] = stats.wasserstein_distance(r_norm, s_norm)\n",
    "    \n",
    "    good_or_better = sum(1 for ks in ks_stats.values() if ks < KS_GOOD)\n",
    "    distribution_score = (good_or_better / n_features) * 100\n",
    "    \n",
    "    real_corr = real[common_cols].corr()\n",
    "    syn_corr = synthetic[common_cols].corr()\n",
    "    \n",
    "    real_flat = real_corr.values[np.triu_indices(n_features, k=1)]\n",
    "    syn_flat = syn_corr.values[np.triu_indices(n_features, k=1)]\n",
    "    \n",
    "    valid = ~(np.isnan(real_flat) | np.isnan(syn_flat))\n",
    "    if valid.sum() > 1:\n",
    "        structure_corr = np.corrcoef(real_flat[valid], syn_flat[valid])[0, 1]\n",
    "    else:\n",
    "        structure_corr = 0\n",
    "    \n",
    "    correlation_score = ((structure_corr + 1) / 2) * 100\n",
    "    \n",
    "    negligible = sum(1 for d in cohens_d.values() if abs(d) < 0.2)\n",
    "    small = sum(1 for d in cohens_d.values() if 0.2 <= abs(d) < 0.5)\n",
    "    medium = sum(1 for d in cohens_d.values() if 0.5 <= abs(d) < 0.8)\n",
    "    large = sum(1 for d in cohens_d.values() if abs(d) >= 0.8)\n",
    "    \n",
    "    effect_score = ((negligible * 1.0 + small * 0.75 + medium * 0.25 + large * 0.0) / n_features) * 100\n",
    "    \n",
    "    mean_wd = np.mean(list(wasserstein.values()))\n",
    "    wasserstein_score = max(0, (1 - mean_wd * 2)) * 100\n",
    "    \n",
    "    weights = {'distribution': 0.25, 'correlation': 0.25, 'effect_size': 0.30, 'wasserstein': 0.20}\n",
    "    overall_score = (\n",
    "        distribution_score * weights['distribution'] +\n",
    "        correlation_score * weights['correlation'] +\n",
    "        effect_score * weights['effect_size'] +\n",
    "        wasserstein_score * weights['wasserstein']\n",
    "    )\n",
    "    \n",
    "    results = {\n",
    "        'overall_score': overall_score,\n",
    "        'distribution_score': distribution_score,\n",
    "        'correlation_score': correlation_score,\n",
    "        'effect_score': effect_score,\n",
    "        'wasserstein_score': wasserstein_score,\n",
    "        'mean_ks': np.mean(list(ks_stats.values())),\n",
    "        'structure_corr': structure_corr,\n",
    "        'ks_stats': ks_stats,\n",
    "        'cohens_d': cohens_d,\n",
    "        'effect_counts': {'negligible': negligible, 'small': small, 'medium': medium, 'large': large},\n",
    "        'good_features': good_or_better,\n",
    "        'n_features': n_features\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"=\"*70)\n",
    "        print(\"EVALUATION RESULTS - ANOMALY DATA\")\n",
    "        print(\"=\"*70)\n",
    "        print(f\"\\nDistribution (KS < {KS_GOOD}): {good_or_better}/{n_features} = {distribution_score:.1f}/100\")\n",
    "        print(f\"Correlation Structure: {structure_corr:.3f} -> {correlation_score:.1f}/100\")\n",
    "        print(f\"Effect Size: neg={negligible}, small={small}, med={medium}, large={large} -> {effect_score:.1f}/100\")\n",
    "        print(f\"Wasserstein: {mean_wd:.4f} -> {wasserstein_score:.1f}/100\")\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"OVERALL SCORE: {overall_score:.1f}/100\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        if overall_score >= 80:\n",
    "            print(\"Verdict: EXCELLENT\")\n",
    "        elif overall_score >= 70:\n",
    "            print(\"Verdict: GOOD\")\n",
    "        elif overall_score >= 50:\n",
    "            print(\"Verdict: MODERATE\")\n",
    "        else:\n",
    "            print(\"Verdict: POOR\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Enhanced Copula Generation for Anomaly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TEST MULTIPLE CONFIGURATIONS FOR ANOMALY DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"TESTING COPULA CONFIGURATIONS FOR ANOMALY DATA\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nOriginal anomaly samples: {len(X_real)}\")\n",
    "print(f\"Target synthetic samples: {N_SYNTHETIC}\\n\")\n",
    "\n",
    "configurations = [\n",
    "    # Baseline configurations\n",
    "    {\n",
    "        'name': 'Gaussian Copula + Spearman',\n",
    "        'copula_type': 'gaussian',\n",
    "        't_df': 5,\n",
    "        'pearson_weight': 0.3,\n",
    "        'spearman_weight': 0.7,\n",
    "        'use_kde_marginals': True,\n",
    "        'kde_bandwidth_factor': 1.0,\n",
    "        'handle_zeros': True\n",
    "    },\n",
    "    {\n",
    "        'name': 't-Copula(df=3) + Pearson',\n",
    "        'copula_type': 't',\n",
    "        't_df': 3,\n",
    "        'pearson_weight': 1.0,\n",
    "        'spearman_weight': 0.0,\n",
    "        'use_kde_marginals': True,\n",
    "        'kde_bandwidth_factor': 1.0,\n",
    "        'handle_zeros': True\n",
    "    },\n",
    "    {\n",
    "        'name': 't-Copula(df=4) + Pearson',\n",
    "        'copula_type': 't',\n",
    "        't_df': 4,\n",
    "        'pearson_weight': 1.0,\n",
    "        'spearman_weight': 0.0,\n",
    "        'use_kde_marginals': True,\n",
    "        'kde_bandwidth_factor': 1.0,\n",
    "        'handle_zeros': True\n",
    "    },\n",
    "    {\n",
    "        'name': 't-Copula(df=5) + Pearson',\n",
    "        'copula_type': 't',\n",
    "        't_df': 5,\n",
    "        'pearson_weight': 1.0,\n",
    "        'spearman_weight': 0.0,\n",
    "        'use_kde_marginals': True,\n",
    "        'kde_bandwidth_factor': 1.0,\n",
    "        'handle_zeros': True\n",
    "    },\n",
    "    {\n",
    "        'name': 't-Copula(df=3) + 90%Pearson/10%Spearman',\n",
    "        'copula_type': 't',\n",
    "        't_df': 3,\n",
    "        'pearson_weight': 0.90,\n",
    "        'spearman_weight': 0.10,\n",
    "        'use_kde_marginals': True,\n",
    "        'kde_bandwidth_factor': 1.0,\n",
    "        'handle_zeros': True\n",
    "    },\n",
    "    {\n",
    "        'name': 't-Copula(df=4) + KDE(bw=0.75x)',\n",
    "        'copula_type': 't',\n",
    "        't_df': 4,\n",
    "        'pearson_weight': 1.0,\n",
    "        'spearman_weight': 0.0,\n",
    "        'use_kde_marginals': True,\n",
    "        'kde_bandwidth_factor': 0.75,\n",
    "        'handle_zeros': True\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Testing {len(configurations)} configurations...\\n\")\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for i, config in enumerate(configurations, 1):\n",
    "    name = config.pop('name')\n",
    "    print(f\"[{i}/{len(configurations)}] {name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    try:\n",
    "        generator = EnhancedCopulaGenerator(**config, random_state=RANDOM_STATE)\n",
    "        generator.fit(X_real)\n",
    "        \n",
    "        synthetic = generator.generate(N_SYNTHETIC)\n",
    "        synthetic = enforce_bgp_constraints(synthetic, X_real, verbose=False)\n",
    "        \n",
    "        eval_result = evaluate_synthetic(X_real, synthetic, verbose=False)\n",
    "        \n",
    "        print(f\"  Score: {eval_result['overall_score']:.1f}/100\")\n",
    "        print(f\"  KS Good: {eval_result['good_features']}/{eval_result['n_features']}\")\n",
    "        print(f\"  Correlation: {eval_result['structure_corr']:.3f}\")\n",
    "        \n",
    "        results_list.append({\n",
    "            'name': name,\n",
    "            'score': eval_result['overall_score'],\n",
    "            'synthetic': synthetic,\n",
    "            'eval': eval_result,\n",
    "            'config': config\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  FAILED: {e}\")\n",
    "    \n",
    "    config['name'] = name\n",
    "    print()\n",
    "\n",
    "# Sort and display results\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURATION RANKING\")\n",
    "print(\"=\"*70)\n",
    "results_sorted = sorted(results_list, key=lambda x: x['score'], reverse=True)\n",
    "for i, r in enumerate(results_sorted, 1):\n",
    "    marker = \" <-- BEST\" if i == 1 else \"\"\n",
    "    print(f\"{i:2d}. {r['name']}: {r['score']:.1f}/100{marker}\")\n",
    "\n",
    "if results_list:\n",
    "    best = max(results_list, key=lambda x: x['score'])\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"BEST CONFIGURATION: {best['name']}\")\n",
    "    print(f\"SCORE: {best['score']:.1f}/100\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best configuration\n",
    "if results_list:\n",
    "    best_synthetic = best['synthetic']\n",
    "    best_eval = best['eval']\n",
    "    \n",
    "    print(\"\\nFull Evaluation of Best Configuration:\")\n",
    "    _ = evaluate_synthetic(X_real, best_synthetic, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply correlation alignment if needed\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"APPLYING CORRELATION ALIGNMENT POST-PROCESSING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "aligned_synthetic = iterative_correlation_refinement(\n",
    "    best_synthetic, X_real, \n",
    "    target_corr_match=0.92, \n",
    "    max_rounds=5, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nRe-applying BGP constraints...\")\n",
    "aligned_synthetic = enforce_bgp_constraints(aligned_synthetic, X_real, verbose=True)\n",
    "\n",
    "print(\"\\nEvaluating aligned synthetic data:\")\n",
    "aligned_eval = evaluate_synthetic(X_real, aligned_synthetic, verbose=True)\n",
    "\n",
    "# Use better result\n",
    "if aligned_eval['overall_score'] > best_eval['overall_score']:\n",
    "    final_synthetic = aligned_synthetic\n",
    "    final_eval = aligned_eval\n",
    "    print(\"\\nUsing ALIGNED synthetic data (better score)\")\n",
    "else:\n",
    "    final_synthetic = best_synthetic\n",
    "    final_eval = best_eval\n",
    "    print(\"\\nUsing ORIGINAL best synthetic data (better score)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution comparison for key features\n",
    "key_features = ['announcements', 'withdrawals', 'edit_distance_avg', 'rare_ases_avg']\n",
    "key_features = [f for f in key_features if f in X_real.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(key_features[:4]):\n",
    "    ax = axes[i]\n",
    "    ax.hist(X_real[col], bins=50, alpha=0.5, label='Real Anomaly', density=True)\n",
    "    ax.hist(final_synthetic[col], bins=50, alpha=0.5, label='Synthetic Anomaly', density=True)\n",
    "    \n",
    "    ks_stat = final_eval['ks_stats'].get(col, 0)\n",
    "    ax.set_title(f'{col}\\nKS={ks_stat:.3f}')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Distribution Comparison: Real vs Synthetic ANOMALY Data', y=1.02, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Real correlation\n",
    "corr_features = key_features + ['origin_0', 'origin_2', 'dups']\n",
    "corr_features = [f for f in corr_features if f in X_real.columns]\n",
    "\n",
    "real_corr = X_real[corr_features].corr()\n",
    "sns.heatmap(real_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[0])\n",
    "axes[0].set_title('Real Anomaly Data Correlation')\n",
    "\n",
    "# Synthetic correlation\n",
    "syn_corr = final_synthetic[corr_features].corr()\n",
    "sns.heatmap(syn_corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=axes[1])\n",
    "axes[1].set_title('Synthetic Anomaly Data Correlation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Save synthetic anomaly data\n",
    "synthetic_path = os.path.join(OUTPUT_DIR, 'copula_synthetic_anomaly.csv')\n",
    "final_synthetic.to_csv(synthetic_path, index=False)\n",
    "print(f\"\\nSaved synthetic anomaly data: {synthetic_path}\")\n",
    "print(f\"  Samples: {len(final_synthetic)}\")\n",
    "\n",
    "# Save combined data (original + synthetic)\n",
    "df_original = X_real.copy()\n",
    "df_original['data_source'] = 'original'\n",
    "df_original['label'] = 'anomaly'\n",
    "\n",
    "df_synthetic = final_synthetic.copy()\n",
    "df_synthetic['data_source'] = 'synthetic'\n",
    "df_synthetic['label'] = 'anomaly'\n",
    "\n",
    "df_combined = pd.concat([df_original, df_synthetic], ignore_index=True)\n",
    "combined_path = os.path.join(OUTPUT_DIR, 'combined_anomaly_data.csv')\n",
    "df_combined.to_csv(combined_path, index=False)\n",
    "print(f\"\\nSaved combined anomaly data: {combined_path}\")\n",
    "print(f\"  Original: {len(X_real)}\")\n",
    "print(f\"  Synthetic: {len(final_synthetic)}\")\n",
    "print(f\"  Total: {len(df_combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COPULA ANOMALY TRAFFIC GENERATION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nInput Data:\")\n",
    "print(f\"  Source: {REAL_DATA_PATH}\")\n",
    "print(f\"  Confidence labels: {HIGH_CONFIDENCE_LABELS}\")\n",
    "print(f\"  Original samples: {len(X_real)}\")\n",
    "\n",
    "print(f\"\\nBest Configuration: {best['name']}\")\n",
    "\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"  Overall Score: {final_eval['overall_score']:.1f}/100\")\n",
    "print(f\"  Distribution Score: {final_eval['distribution_score']:.1f}/100\")\n",
    "print(f\"  Correlation Score: {final_eval['correlation_score']:.1f}/100\")\n",
    "print(f\"  Effect Size Score: {final_eval['effect_score']:.1f}/100\")\n",
    "\n",
    "print(f\"\\nOutput Files:\")\n",
    "print(f\"  Synthetic only: {synthetic_path}\")\n",
    "print(f\"  Combined data: {combined_path}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of generated data\n",
    "print(\"\\nSample of generated synthetic anomaly data:\")\n",
    "final_synthetic.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
