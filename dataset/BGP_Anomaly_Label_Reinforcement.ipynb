{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Anomaly Label Reinforcement Methodology\n",
    "\n",
    "## Purpose\n",
    "This notebook implements a **reinforcement methodology for anomaly labeling** - validating and strengthening the confidence of anomaly labels assigned to real-world BGP incidents.\n",
    "\n",
    "## Key Differences from Normal Traffic Labeling\n",
    "While normal traffic labeling uses anomaly detection to **discover** outliers, anomaly label reinforcement:\n",
    "1. **Validates** that labeled anomalies truly deviate from normal patterns\n",
    "2. **Quantifies** the confidence level of each anomaly label\n",
    "3. **Verifies** incident coherence (samples from same incident should cluster)\n",
    "4. **Cross-validates** anomaly types using multiple methodologies\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "### 1. Baseline Normal Profile Construction\n",
    "- Build statistical profiles of \"normal\" BGP behavior from reference data\n",
    "- Define multivariate boundaries for normal traffic\n",
    "\n",
    "### 2. Anomaly Deviation Scoring (5 Methods)\n",
    "| Method | Purpose |\n",
    "|--------|--------|\n",
    "| Mahalanobis Distance | Deviation from normal centroid |\n",
    "| One-Class SVM | Boundary violation scoring |\n",
    "| Autoencoder Reconstruction | Reconstruction error as anomaly proxy |\n",
    "| Statistical Z-Score | Multi-feature z-score aggregation |\n",
    "| LOF vs Normal Baseline | Density deviation from normal |\n",
    "\n",
    "### 3. Incident Coherence Validation\n",
    "- Verify samples within same incident cluster together\n",
    "- Measure inter-incident separation\n",
    "\n",
    "### 4. Temporal Consistency Check\n",
    "- Validate anomaly patterns align with known incident timelines\n",
    "\n",
    "### 5. Cross-Validation with Supervised Learning\n",
    "- Train classifiers to distinguish anomaly types\n",
    "- Identify ambiguous or potentially mislabeled samples\n",
    "\n",
    "### 6. Final Confidence Scoring\n",
    "$$\\text{confidence}(x) = \\frac{\\sum_{m=1}^{M} w_m \\cdot s_m(x)}{\\sum_{m=1}^{M} w_m}$$\n",
    "\n",
    "Where $s_m(x)$ is the normalized score from method $m$ and $w_m$ is the method weight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "\n",
    "# Anomaly Detection Methods\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy import stats\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import hdbscan\n",
    "\n",
    "# Classification for cross-validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# Deep Learning for Autoencoder\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, Model\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available. Autoencoder method will be skipped.\")\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"TensorFlow available: {TF_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load and Explore Anomaly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anomaly dataset\n",
    "anomaly_df = pd.read_csv('all_incidents_anomalies_only.csv')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANOMALY DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(anomaly_df):,}\")\n",
    "print(f\"Features: {anomaly_df.shape[1]}\")\n",
    "print(f\"\\nColumns: {list(anomaly_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "anomaly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "print(\"\\nLABEL DISTRIBUTION:\")\n",
    "print(\"-\"*40)\n",
    "label_counts = anomaly_df['label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(anomaly_df) * 100\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "axes[0].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, explode=[0.02]*len(label_counts))\n",
    "axes[0].set_title('Anomaly Type Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = axes[1].bar(label_counts.index, label_counts.values, color=colors)\n",
    "axes[1].set_title('Sample Count by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "for bar, count in zip(bars, label_counts.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200, \n",
    "                 f'{count:,}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incident distribution\n",
    "print(\"\\nINCIDENT DISTRIBUTION:\")\n",
    "print(\"-\"*60)\n",
    "incident_counts = anomaly_df['Incident'].value_counts()\n",
    "for incident, count in incident_counts.head(15).items():\n",
    "    label = anomaly_df[anomaly_df['Incident'] == incident]['label'].iloc[0]\n",
    "    print(f\"  {incident}: {count:,} samples [{label}]\")\n",
    "print(f\"  ... and {len(incident_counts) - 15} more incidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude metadata columns)\n",
    "metadata_cols = ['label', 'Incident', 'window_start', 'window_end']\n",
    "feature_cols = [col for col in anomaly_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and handle missing values\n",
    "X = anomaly_df[feature_cols].copy()\n",
    "y_label = anomaly_df['label'].copy()\n",
    "y_incident = anomaly_df['Incident'].copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing = X.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"Missing values found:\")\n",
    "    print(missing[missing > 0])\n",
    "    # Fill with median\n",
    "    X = X.fillna(X.median())\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_mask = np.isinf(X.values)\n",
    "if inf_mask.any():\n",
    "    print(f\"\\nInfinite values found: {inf_mask.sum()}\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using RobustScaler (handles outliers better)\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "print(\"Features normalized with RobustScaler\")\n",
    "print(f\"Scaled feature statistics:\")\n",
    "print(f\"  Mean range: [{X_scaled.mean(axis=0).min():.3f}, {X_scaled.mean(axis=0).max():.3f}]\")\n",
    "print(f\"  Std range: [{X_scaled.std(axis=0).min():.3f}, {X_scaled.std(axis=0).max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Baseline Normal Profile Construction\n",
    "\n",
    "To validate anomalies, we need a reference \"normal\" profile. We have two approaches:\n",
    "1. **Synthetic Normal Profile**: Use statistical characteristics of typical BGP traffic\n",
    "2. **Load Real Normal Data**: If available, use your labeled normal traffic\n",
    "\n",
    "We'll construct a synthetic baseline based on domain knowledge of normal BGP behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalBaselineProfile:\n",
    "    \"\"\"\n",
    "    Constructs a statistical profile of \"normal\" BGP traffic behavior.\n",
    "    \n",
    "    Based on domain knowledge:\n",
    "    - Normal traffic has low announcement/withdrawal ratios\n",
    "    - Stable AS paths with minimal changes\n",
    "    - Low edit distances (path consistency)\n",
    "    - Minimal origin changes and flaps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Domain knowledge: typical ranges for normal BGP traffic\n",
    "        # These are approximate - adjust based on your reference data\n",
    "        self.normal_ranges = {\n",
    "            'announcements': (0, 50),\n",
    "            'withdrawals': (0, 10),\n",
    "            'nlri_ann': (0, 100),\n",
    "            'dups': (0, 5),\n",
    "            'origin_changes': (0, 2),\n",
    "            'imp_wd': (0, 5),\n",
    "            'as_path_max': (1, 10),\n",
    "            'edit_distance_avg': (0, 2),\n",
    "            'edit_distance_max': (0, 5),\n",
    "            'flaps': (0, 3),\n",
    "            'nadas': (0, 5)\n",
    "        }\n",
    "        \n",
    "        self.fitted = False\n",
    "        self.mean_ = None\n",
    "        self.cov_ = None\n",
    "        self.cov_inv_ = None\n",
    "    \n",
    "    def fit_from_data(self, normal_data):\n",
    "        \"\"\"Fit baseline from actual normal traffic data.\"\"\"\n",
    "        self.mean_ = normal_data.mean(axis=0)\n",
    "        self.cov_ = np.cov(normal_data.T)\n",
    "        try:\n",
    "            self.cov_inv_ = np.linalg.pinv(self.cov_)\n",
    "        except:\n",
    "            self.cov_inv_ = np.eye(normal_data.shape[1])\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def fit_synthetic(self, feature_names, n_samples=10000):\n",
    "        \"\"\"\n",
    "        Generate synthetic normal baseline when real normal data unavailable.\n",
    "        Uses domain knowledge to create realistic normal traffic patterns.\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        n_features = len(feature_names)\n",
    "        \n",
    "        # Generate synthetic normal samples\n",
    "        synthetic_normal = np.zeros((n_samples, n_features))\n",
    "        \n",
    "        for i, feat in enumerate(feature_names):\n",
    "            # Use known ranges or generate low-variance normal distribution\n",
    "            if feat in self.normal_ranges:\n",
    "                low, high = self.normal_ranges[feat]\n",
    "                mean = (low + high) / 2\n",
    "                std = (high - low) / 4\n",
    "            else:\n",
    "                # For unknown features, use low values (normal = quiet)\n",
    "                mean, std = 1.0, 0.5\n",
    "            \n",
    "            synthetic_normal[:, i] = np.abs(np.random.normal(mean, std, n_samples))\n",
    "        \n",
    "        self.fit_from_data(synthetic_normal)\n",
    "        self.synthetic_data = synthetic_normal\n",
    "        return self\n",
    "    \n",
    "    def mahalanobis_distance(self, X):\n",
    "        \"\"\"Calculate Mahalanobis distance from normal baseline.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Profile not fitted. Call fit_from_data() or fit_synthetic() first.\")\n",
    "        \n",
    "        distances = []\n",
    "        for x in X:\n",
    "            try:\n",
    "                d = mahalanobis(x, self.mean_, self.cov_inv_)\n",
    "            except:\n",
    "                d = np.sqrt(np.sum((x - self.mean_)**2))\n",
    "            distances.append(d)\n",
    "        return np.array(distances)\n",
    "\n",
    "\n",
    "# Try to load real normal data if available\n",
    "normal_data_paths = [\n",
    "    '../scripts/labeled_bgp_features.csv',\n",
    "    '../scripts/ripe_rrc04_features_labeled.csv',\n",
    "    'normal_traffic_features.csv'\n",
    "]\n",
    "\n",
    "normal_baseline = NormalBaselineProfile()\n",
    "real_normal_loaded = False\n",
    "\n",
    "for path in normal_data_paths:\n",
    "    try:\n",
    "        normal_df = pd.read_csv(path)\n",
    "        if 'label' in normal_df.columns:\n",
    "            normal_samples = normal_df[normal_df['label'].str.contains('normal', case=False, na=False)]\n",
    "            if len(normal_samples) > 100:\n",
    "                # Use common features\n",
    "                common_features = [f for f in feature_cols if f in normal_samples.columns]\n",
    "                if len(common_features) > 5:\n",
    "                    normal_X = normal_samples[common_features].values\n",
    "                    normal_X = np.nan_to_num(normal_X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "                    normal_baseline.fit_from_data(normal_X)\n",
    "                    real_normal_loaded = True\n",
    "                    print(f\"Loaded real normal baseline from: {path}\")\n",
    "                    print(f\"  Normal samples: {len(normal_samples):,}\")\n",
    "                    print(f\"  Common features: {len(common_features)}\")\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if not real_normal_loaded:\n",
    "    print(\"No real normal data found. Using synthetic baseline.\")\n",
    "    normal_baseline.fit_synthetic(feature_cols)\n",
    "    print(f\"Synthetic baseline generated with {len(feature_cols)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Anomaly Deviation Scoring (5 Methods)\n",
    "\n",
    "Each method provides an independent score indicating how \"anomalous\" each sample is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyReinforcementScorer:\n",
    "    \"\"\"\n",
    "    Ensemble anomaly reinforcement scoring using 5 complementary methods.\n",
    "    Higher scores indicate stronger anomaly evidence.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, normal_baseline=None):\n",
    "        self.normal_baseline = normal_baseline\n",
    "        self.scores = {}\n",
    "        self.models = {}\n",
    "    \n",
    "    def score_mahalanobis(self, X):\n",
    "        \"\"\"\n",
    "        Method 1: Mahalanobis Distance from Normal Baseline\n",
    "        \n",
    "        Measures how far each sample is from the \"normal\" centroid,\n",
    "        accounting for feature correlations.\n",
    "        \"\"\"\n",
    "        print(\"[1/5] Computing Mahalanobis distances...\")\n",
    "        \n",
    "        if self.normal_baseline is not None and self.normal_baseline.fitted:\n",
    "            distances = self.normal_baseline.mahalanobis_distance(X)\n",
    "        else:\n",
    "            # Fallback: compute relative to data centroid\n",
    "            mean = X.mean(axis=0)\n",
    "            cov = np.cov(X.T)\n",
    "            try:\n",
    "                cov_inv = np.linalg.pinv(cov)\n",
    "                distances = np.array([mahalanobis(x, mean, cov_inv) for x in X])\n",
    "            except:\n",
    "                distances = np.sqrt(np.sum((X - mean)**2, axis=1))\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        self.scores['mahalanobis'] = self._normalize_scores(distances)\n",
    "        return self.scores['mahalanobis']\n",
    "    \n",
    "    def score_one_class_svm(self, X):\n",
    "        \"\"\"\n",
    "        Method 2: One-Class SVM Decision Function\n",
    "        \n",
    "        Trains on the assumption that all data is anomalous,\n",
    "        then uses decision function as anomaly score.\n",
    "        More negative = more anomalous.\n",
    "        \"\"\"\n",
    "        print(\"[2/5] Fitting One-Class SVM...\")\n",
    "        \n",
    "        # Use a sample if data is too large\n",
    "        if len(X) > 10000:\n",
    "            np.random.seed(42)\n",
    "            sample_idx = np.random.choice(len(X), 10000, replace=False)\n",
    "            X_train = X[sample_idx]\n",
    "        else:\n",
    "            X_train = X\n",
    "        \n",
    "        ocsvm = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')\n",
    "        ocsvm.fit(X_train)\n",
    "        self.models['ocsvm'] = ocsvm\n",
    "        \n",
    "        # Decision function: negative = outlier\n",
    "        decision = -ocsvm.decision_function(X)  # Negate so higher = more anomalous\n",
    "        self.scores['ocsvm'] = self._normalize_scores(decision)\n",
    "        return self.scores['ocsvm']\n",
    "    \n",
    "    def score_statistical(self, X):\n",
    "        \"\"\"\n",
    "        Method 3: Statistical Z-Score Aggregation\n",
    "        \n",
    "        Computes z-scores for each feature and aggregates.\n",
    "        Higher aggregate z-score = more anomalous.\n",
    "        \"\"\"\n",
    "        print(\"[3/5] Computing statistical z-scores...\")\n",
    "        \n",
    "        # Compute z-scores\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0) + 1e-10  # Avoid division by zero\n",
    "        z_scores = np.abs((X - mean) / std)\n",
    "        \n",
    "        # Aggregate: max z-score or sum of extreme z-scores\n",
    "        max_z = z_scores.max(axis=1)\n",
    "        extreme_count = (z_scores > 3).sum(axis=1)  # Count features beyond 3 sigma\n",
    "        \n",
    "        # Combined score\n",
    "        combined = max_z + extreme_count * 0.5\n",
    "        self.scores['statistical'] = self._normalize_scores(combined)\n",
    "        return self.scores['statistical']\n",
    "    \n",
    "    def score_lof(self, X):\n",
    "        \"\"\"\n",
    "        Method 4: Local Outlier Factor\n",
    "        \n",
    "        Measures local density deviation. Samples in sparse regions\n",
    "        relative to their neighbors get higher LOF scores.\n",
    "        \"\"\"\n",
    "        print(\"[4/5] Computing Local Outlier Factor...\")\n",
    "        \n",
    "        n_neighbors = min(int(np.sqrt(len(X))), 50)\n",
    "        n_neighbors = max(n_neighbors, 10)\n",
    "        \n",
    "        lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1, novelty=False)\n",
    "        lof.fit_predict(X)\n",
    "        \n",
    "        # Negative outlier factor: more negative = more anomalous\n",
    "        lof_scores = -lof.negative_outlier_factor_\n",
    "        self.scores['lof'] = self._normalize_scores(lof_scores)\n",
    "        return self.scores['lof']\n",
    "    \n",
    "    def score_isolation_forest(self, X):\n",
    "        \"\"\"\n",
    "        Method 5: Isolation Forest Score\n",
    "        \n",
    "        Anomalies are easier to isolate, requiring fewer splits.\n",
    "        Lower score = more anomalous (we negate for consistency).\n",
    "        \"\"\"\n",
    "        print(\"[5/5] Fitting Isolation Forest...\")\n",
    "        \n",
    "        iso_forest = IsolationForest(n_estimators=200, contamination=0.1, \n",
    "                                      random_state=42, n_jobs=-1)\n",
    "        iso_forest.fit(X)\n",
    "        self.models['isolation_forest'] = iso_forest\n",
    "        \n",
    "        # Score: lower = more anomalous, negate for consistency\n",
    "        iso_scores = -iso_forest.score_samples(X)\n",
    "        self.scores['isolation_forest'] = self._normalize_scores(iso_scores)\n",
    "        return self.scores['isolation_forest']\n",
    "    \n",
    "    def _normalize_scores(self, scores):\n",
    "        \"\"\"Normalize scores to [0, 1] range.\"\"\"\n",
    "        scores = np.array(scores)\n",
    "        min_s, max_s = scores.min(), scores.max()\n",
    "        if max_s - min_s > 0:\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        return np.zeros_like(scores)\n",
    "    \n",
    "    def compute_all_scores(self, X):\n",
    "        \"\"\"Compute all 5 anomaly scores.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"COMPUTING ANOMALY REINFORCEMENT SCORES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.score_mahalanobis(X)\n",
    "        self.score_one_class_svm(X)\n",
    "        self.score_statistical(X)\n",
    "        self.score_lof(X)\n",
    "        self.score_isolation_forest(X)\n",
    "        \n",
    "        print(\"\\nAll scores computed.\")\n",
    "        return self.scores\n",
    "    \n",
    "    def get_ensemble_score(self, weights=None):\n",
    "        \"\"\"\n",
    "        Compute weighted ensemble score.\n",
    "        \n",
    "        Higher score = stronger evidence of anomaly.\n",
    "        \"\"\"\n",
    "        if not self.scores:\n",
    "            raise ValueError(\"No scores computed. Call compute_all_scores() first.\")\n",
    "        \n",
    "        if weights is None:\n",
    "            # Equal weights\n",
    "            weights = {k: 1.0 for k in self.scores.keys()}\n",
    "        \n",
    "        total_weight = sum(weights.values())\n",
    "        ensemble = np.zeros(len(list(self.scores.values())[0]))\n",
    "        \n",
    "        for method, score in self.scores.items():\n",
    "            w = weights.get(method, 1.0)\n",
    "            ensemble += w * score\n",
    "        \n",
    "        return ensemble / total_weight\n",
    "\n",
    "\n",
    "# Initialize scorer and compute all scores\n",
    "scorer = AnomalyReinforcementScorer(normal_baseline=normal_baseline)\n",
    "all_scores = scorer.compute_all_scores(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method, scores) in enumerate(all_scores.items()):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Histogram by label\n",
    "    for label in y_label.unique():\n",
    "        mask = y_label == label\n",
    "        ax.hist(scores[mask], bins=50, alpha=0.6, label=label, density=True)\n",
    "    \n",
    "    ax.set_title(f'{method.upper()} Score Distribution', fontweight='bold')\n",
    "    ax.set_xlabel('Anomaly Score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "# Ensemble score\n",
    "ensemble_score = scorer.get_ensemble_score()\n",
    "ax = axes[5]\n",
    "for label in y_label.unique():\n",
    "    mask = y_label == label\n",
    "    ax.hist(ensemble_score[mask], bins=50, alpha=0.6, label=label, density=True)\n",
    "ax.set_title('ENSEMBLE Score Distribution', fontweight='bold')\n",
    "ax.set_xlabel('Anomaly Score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score correlation analysis\n",
    "score_df = pd.DataFrame(all_scores)\n",
    "score_df['ensemble'] = ensemble_score\n",
    "\n",
    "print(\"\\nSCORE CORRELATION MATRIX:\")\n",
    "print(\"-\"*40)\n",
    "corr_matrix = score_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Anomaly Score Method Correlation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Confidence Label Assignment\n",
    "\n",
    "Based on ensemble scores, assign confidence levels to anomaly labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_anomaly_confidence(ensemble_scores, method_scores, thresholds=None):\n",
    "    \"\"\"\n",
    "    Assign confidence levels to anomaly labels.\n",
    "    \n",
    "    Confidence Levels:\n",
    "    - high_confidence_anomaly: Ensemble score >= 0.7 AND 4+ methods agree\n",
    "    - confirmed_anomaly: Ensemble score >= 0.5 AND 3+ methods agree\n",
    "    - likely_anomaly: Ensemble score >= 0.3 AND 2+ methods agree\n",
    "    - uncertain_anomaly: Lower scores, needs review\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = {\n",
    "            'high_confidence': (0.7, 4),\n",
    "            'confirmed': (0.5, 3),\n",
    "            'likely': (0.3, 2)\n",
    "        }\n",
    "    \n",
    "    n_samples = len(ensemble_scores)\n",
    "    confidence_labels = np.array(['uncertain_anomaly'] * n_samples, dtype=object)\n",
    "    \n",
    "    # Count methods flagging as high anomaly (score > 0.5)\n",
    "    method_agreement = np.zeros(n_samples)\n",
    "    for method, scores in method_scores.items():\n",
    "        method_agreement += (scores > 0.5).astype(int)\n",
    "    \n",
    "    # Assign confidence levels\n",
    "    for i in range(n_samples):\n",
    "        score = ensemble_scores[i]\n",
    "        agreement = method_agreement[i]\n",
    "        \n",
    "        if score >= thresholds['high_confidence'][0] and agreement >= thresholds['high_confidence'][1]:\n",
    "            confidence_labels[i] = 'high_confidence_anomaly'\n",
    "        elif score >= thresholds['confirmed'][0] and agreement >= thresholds['confirmed'][1]:\n",
    "            confidence_labels[i] = 'confirmed_anomaly'\n",
    "        elif score >= thresholds['likely'][0] and agreement >= thresholds['likely'][1]:\n",
    "            confidence_labels[i] = 'likely_anomaly'\n",
    "    \n",
    "    return confidence_labels, method_agreement\n",
    "\n",
    "\n",
    "# Assign confidence labels\n",
    "confidence_labels, method_agreement = assign_anomaly_confidence(ensemble_score, all_scores)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nANOMALY CONFIDENCE DISTRIBUTION:\")\n",
    "print(\"=\"*60)\n",
    "conf_counts = pd.Series(confidence_labels).value_counts()\n",
    "for conf, count in conf_counts.items():\n",
    "    pct = count / len(confidence_labels) * 100\n",
    "    print(f\"  {conf}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate confidence with original labels\n",
    "print(\"\\nCONFIDENCE vs ORIGINAL LABEL:\")\n",
    "print(\"=\"*60)\n",
    "crosstab = pd.crosstab(y_label, confidence_labels, margins=True)\n",
    "print(crosstab)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "crosstab_pct = pd.crosstab(y_label, confidence_labels, normalize='index') * 100\n",
    "\n",
    "crosstab_pct.plot(kind='bar', stacked=True, ax=ax, \n",
    "                  colormap='RdYlGn_r', edgecolor='white')\n",
    "ax.set_title('Confidence Distribution by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Original Anomaly Label')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.legend(title='Confidence', bbox_to_anchor=(1.02, 1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Incident Coherence Validation\n",
    "\n",
    "Validate that samples from the same incident cluster together in feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_incident_coherence(X, incidents):\n",
    "    \"\"\"\n",
    "    Compute coherence metrics for each incident.\n",
    "    \n",
    "    Metrics:\n",
    "    - Intra-incident variance: How spread out are samples within an incident\n",
    "    - Inter-incident distance: How far is this incident from others\n",
    "    - Silhouette score: Cluster quality metric\n",
    "    \"\"\"\n",
    "    unique_incidents = incidents.unique()\n",
    "    coherence_results = []\n",
    "    \n",
    "    for incident in unique_incidents:\n",
    "        mask = incidents == incident\n",
    "        X_incident = X[mask]\n",
    "        X_other = X[~mask]\n",
    "        \n",
    "        if len(X_incident) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Intra-incident variance (lower = more coherent)\n",
    "        centroid = X_incident.mean(axis=0)\n",
    "        intra_variance = np.mean(np.sum((X_incident - centroid)**2, axis=1))\n",
    "        \n",
    "        # Inter-incident distance (higher = more distinct)\n",
    "        other_centroid = X_other.mean(axis=0) if len(X_other) > 0 else centroid\n",
    "        inter_distance = np.sqrt(np.sum((centroid - other_centroid)**2))\n",
    "        \n",
    "        # Coherence score (higher = better)\n",
    "        coherence = inter_distance / (np.sqrt(intra_variance) + 1e-10)\n",
    "        \n",
    "        coherence_results.append({\n",
    "            'incident': incident,\n",
    "            'n_samples': mask.sum(),\n",
    "            'intra_variance': intra_variance,\n",
    "            'inter_distance': inter_distance,\n",
    "            'coherence_score': coherence\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(coherence_results)\n",
    "\n",
    "\n",
    "# Compute incident coherence\n",
    "coherence_df = compute_incident_coherence(X_scaled, y_incident)\n",
    "coherence_df = coherence_df.sort_values('coherence_score', ascending=False)\n",
    "\n",
    "print(\"\\nINCIDENT COHERENCE SCORES:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Incident':<45} {'Samples':>8} {'Coherence':>12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in coherence_df.head(15).iterrows():\n",
    "    print(f\"{row['incident']:<45} {row['n_samples']:>8,} {row['coherence_score']:>12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coherence scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Coherence score distribution\n",
    "ax = axes[0]\n",
    "ax.hist(coherence_df['coherence_score'], bins=30, color='steelblue', edgecolor='white')\n",
    "ax.axvline(coherence_df['coherence_score'].median(), color='red', linestyle='--', \n",
    "           label=f\"Median: {coherence_df['coherence_score'].median():.2f}\")\n",
    "ax.set_title('Incident Coherence Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Coherence Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "\n",
    "# Top/bottom incidents\n",
    "ax = axes[1]\n",
    "top_5 = coherence_df.head(5)\n",
    "bottom_5 = coherence_df.tail(5)\n",
    "combined = pd.concat([top_5, bottom_5])\n",
    "\n",
    "colors = ['green']*5 + ['red']*5\n",
    "y_pos = range(len(combined))\n",
    "ax.barh(y_pos, combined['coherence_score'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([inc[:30] + '...' if len(inc) > 30 else inc \n",
    "                    for inc in combined['incident']])\n",
    "ax.set_xlabel('Coherence Score')\n",
    "ax.set_title('Top 5 (Green) vs Bottom 5 (Red) Incidents', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cross-Validation with Supervised Learning\n",
    "\n",
    "Use supervised classifiers to verify anomaly type labels and identify potentially mislabeled samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_label)\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"  {i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest classifier\n",
    "print(\"\\nTRAINING RANDOM FOREST FOR CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Train\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=20, \n",
    "                            random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Anomaly Type Classification Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potentially mislabeled samples using prediction probabilities\n",
    "print(\"\\nIDENTIFYING POTENTIALLY MISLABELED SAMPLES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get prediction probabilities for all data\n",
    "proba = rf.predict_proba(X_scaled)\n",
    "predictions = rf.predict(X_scaled)\n",
    "\n",
    "# Find samples where prediction differs from label\n",
    "mismatch_mask = predictions != y_encoded\n",
    "mismatch_count = mismatch_mask.sum()\n",
    "\n",
    "print(f\"Samples with prediction mismatch: {mismatch_count:,} ({mismatch_count/len(y_encoded)*100:.1f}%)\")\n",
    "\n",
    "# Find samples with low confidence in their assigned label\n",
    "assigned_proba = proba[np.arange(len(y_encoded)), y_encoded]\n",
    "low_confidence_mask = assigned_proba < 0.5\n",
    "low_conf_count = low_confidence_mask.sum()\n",
    "\n",
    "print(f\"Samples with low confidence (<50%): {low_conf_count:,} ({low_conf_count/len(y_encoded)*100:.1f}%)\")\n",
    "\n",
    "# Combined: mismatch AND low confidence = potentially mislabeled\n",
    "potentially_mislabeled = mismatch_mask & low_confidence_mask\n",
    "print(f\"Potentially mislabeled: {potentially_mislabeled.sum():,} ({potentially_mislabeled.sum()/len(y_encoded)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "print(\"\\nFEATURE IMPORTANCE FOR ANOMALY TYPE CLASSIFICATION:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Features for Anomaly Type Discrimination', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Clustering Validation (HDBSCAN)\n",
    "\n",
    "Verify that natural clusters align with anomaly type labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality for clustering\n",
    "print(\"Performing dimensionality reduction for clustering...\")\n",
    "\n",
    "# PCA first for speed\n",
    "pca = PCA(n_components=min(10, X_scaled.shape[1]), random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# UMAP for visualization\n",
    "try:\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=30, min_dist=0.1)\n",
    "    X_umap = reducer.fit_transform(X_pca)\n",
    "    print(\"UMAP reduction complete.\")\n",
    "except:\n",
    "    print(\"UMAP not available, using t-SNE...\")\n",
    "    X_umap = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN clustering\n",
    "print(\"\\nPerforming HDBSCAN clustering...\")\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10, \n",
    "                            cluster_selection_epsilon=0.5)\n",
    "cluster_labels = clusterer.fit_predict(X_pca)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = (cluster_labels == -1).sum()\n",
    "\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Noise points: {n_noise:,} ({n_noise/len(cluster_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clusters with labels\n",
    "print(\"\\nCLUSTER vs LABEL ALIGNMENT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute alignment metrics\n",
    "valid_mask = cluster_labels != -1\n",
    "if valid_mask.sum() > 100:\n",
    "    ari = adjusted_rand_score(y_encoded[valid_mask], cluster_labels[valid_mask])\n",
    "    nmi = normalized_mutual_info_score(y_encoded[valid_mask], cluster_labels[valid_mask])\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi:.3f}\")\n",
    "\n",
    "# Cross-tabulation\n",
    "cluster_label_crosstab = pd.crosstab(cluster_labels, y_label, margins=True)\n",
    "print(\"\\nCluster vs Anomaly Type:\")\n",
    "print(cluster_label_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters vs labels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot by anomaly type\n",
    "ax = axes[0]\n",
    "for label in y_label.unique():\n",
    "    mask = y_label == label\n",
    "    ax.scatter(X_umap[mask, 0], X_umap[mask, 1], alpha=0.5, s=10, label=label)\n",
    "ax.set_title('2D Projection by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('UMAP 1')\n",
    "ax.set_ylabel('UMAP 2')\n",
    "ax.legend()\n",
    "\n",
    "# Plot by cluster\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(X_umap[:, 0], X_umap[:, 1], c=cluster_labels, \n",
    "                     cmap='tab20', alpha=0.5, s=10)\n",
    "ax.set_title('2D Projection by HDBSCAN Cluster', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('UMAP 1')\n",
    "ax.set_ylabel('UMAP 2')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Autoencoder Reconstruction Error (Deep Learning)\n",
    "\n",
    "Train an autoencoder on anomaly data and use reconstruction error as additional validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    print(\"Building Autoencoder for reconstruction-based validation...\")\n",
    "    \n",
    "    # Build autoencoder\n",
    "    input_dim = X_scaled.shape[1]\n",
    "    encoding_dim = 8\n",
    "    \n",
    "    # Encoder\n",
    "    inputs = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    encoded = layers.Dense(encoding_dim, activation='relu', name='encoding')(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Dense(32, activation='relu')(encoded)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    decoded = layers.Dense(input_dim, activation='linear')(x)\n",
    "    \n",
    "    # Model\n",
    "    autoencoder = keras.Model(inputs, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    print(autoencoder.summary())\n",
    "else:\n",
    "    print(\"TensorFlow not available. Skipping autoencoder analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Train autoencoder\n",
    "    print(\"\\nTraining autoencoder...\")\n",
    "    \n",
    "    history = autoencoder.fit(\n",
    "        X_scaled, X_scaled,\n",
    "        epochs=50,\n",
    "        batch_size=256,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Loss')\n",
    "    plt.title('Autoencoder Training History', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TF_AVAILABLE:\n",
    "    # Compute reconstruction error\n",
    "    reconstructed = autoencoder.predict(X_scaled, verbose=0)\n",
    "    reconstruction_error = np.mean((X_scaled - reconstructed)**2, axis=1)\n",
    "    \n",
    "    print(\"\\nRECONSTRUCTION ERROR BY ANOMALY TYPE:\")\n",
    "    print(\"=\"*60)\n",
    "    for label in y_label.unique():\n",
    "        mask = y_label == label\n",
    "        errors = reconstruction_error[mask]\n",
    "        print(f\"{label}:\")\n",
    "        print(f\"  Mean: {errors.mean():.4f}, Std: {errors.std():.4f}\")\n",
    "        print(f\"  Range: [{errors.min():.4f}, {errors.max():.4f}]\")\n",
    "    \n",
    "    # Add to scores\n",
    "    all_scores['autoencoder'] = (reconstruction_error - reconstruction_error.min()) / \\\n",
    "                                 (reconstruction_error.max() - reconstruction_error.min())\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for label in y_label.unique():\n",
    "        mask = y_label == label\n",
    "        plt.hist(reconstruction_error[mask], bins=50, alpha=0.6, label=label, density=True)\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Autoencoder Reconstruction Error by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Final Reinforced Labels and Confidence Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_reinforcement(original_labels, ensemble_scores, method_scores,\n",
    "                                classifier_proba, cluster_labels, incidents,\n",
    "                                coherence_df):\n",
    "    \"\"\"\n",
    "    Compute final reinforced labels with comprehensive confidence scoring.\n",
    "    \n",
    "    Components:\n",
    "    1. Anomaly deviation score (ensemble)\n",
    "    2. Classifier confidence\n",
    "    3. Incident coherence\n",
    "    4. Cluster alignment\n",
    "    \"\"\"\n",
    "    n_samples = len(original_labels)\n",
    "    results = pd.DataFrame({\n",
    "        'original_label': original_labels,\n",
    "        'incident': incidents\n",
    "    })\n",
    "    \n",
    "    # 1. Anomaly deviation score\n",
    "    results['anomaly_score'] = ensemble_scores\n",
    "    \n",
    "    # 2. Classifier confidence (probability of assigned class)\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(original_labels)\n",
    "    results['classifier_confidence'] = classifier_proba[np.arange(n_samples), y_enc]\n",
    "    \n",
    "    # 3. Incident coherence (lookup from coherence_df)\n",
    "    coherence_map = dict(zip(coherence_df['incident'], coherence_df['coherence_score']))\n",
    "    max_coherence = coherence_df['coherence_score'].max()\n",
    "    results['incident_coherence'] = [coherence_map.get(inc, 0) / max_coherence \n",
    "                                      for inc in incidents]\n",
    "    \n",
    "    # 4. Cluster alignment (is sample in majority cluster for its label?)\n",
    "    label_cluster_majority = {}\n",
    "    for label in original_labels.unique():\n",
    "        mask = original_labels == label\n",
    "        clusters_for_label = cluster_labels[mask]\n",
    "        valid_clusters = clusters_for_label[clusters_for_label != -1]\n",
    "        if len(valid_clusters) > 0:\n",
    "            majority_cluster = Counter(valid_clusters).most_common(1)[0][0]\n",
    "            label_cluster_majority[label] = majority_cluster\n",
    "    \n",
    "    results['cluster_aligned'] = [\n",
    "        1.0 if cluster_labels[i] == label_cluster_majority.get(original_labels.iloc[i], -99) else 0.5\n",
    "        for i in range(n_samples)\n",
    "    ]\n",
    "    \n",
    "    # Combined confidence score\n",
    "    weights = {\n",
    "        'anomaly_score': 0.3,\n",
    "        'classifier_confidence': 0.35,\n",
    "        'incident_coherence': 0.2,\n",
    "        'cluster_aligned': 0.15\n",
    "    }\n",
    "    \n",
    "    results['final_confidence'] = (\n",
    "        weights['anomaly_score'] * results['anomaly_score'] +\n",
    "        weights['classifier_confidence'] * results['classifier_confidence'] +\n",
    "        weights['incident_coherence'] * results['incident_coherence'] +\n",
    "        weights['cluster_aligned'] * results['cluster_aligned']\n",
    "    )\n",
    "    \n",
    "    # Assign reinforced confidence labels\n",
    "    def assign_label(conf):\n",
    "        if conf >= 0.8:\n",
    "            return 'very_high_confidence'\n",
    "        elif conf >= 0.65:\n",
    "            return 'high_confidence'\n",
    "        elif conf >= 0.5:\n",
    "            return 'medium_confidence'\n",
    "        elif conf >= 0.35:\n",
    "            return 'low_confidence'\n",
    "        else:\n",
    "            return 'needs_review'\n",
    "    \n",
    "    results['reinforced_label'] = results['final_confidence'].apply(assign_label)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Compute final reinforcement\n",
    "final_results = compute_final_reinforcement(\n",
    "    y_label, ensemble_score, all_scores, proba, cluster_labels, y_incident, coherence_df\n",
    ")\n",
    "\n",
    "print(\"\\nFINAL REINFORCEMENT SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "reinforced_counts = final_results['reinforced_label'].value_counts()\n",
    "for label, count in reinforced_counts.items():\n",
    "    pct = count / len(final_results) * 100\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate with original labels\n",
    "print(\"\\nREINFORCED CONFIDENCE vs ORIGINAL LABEL:\")\n",
    "print(\"=\"*60)\n",
    "crosstab_final = pd.crosstab(final_results['original_label'], \n",
    "                              final_results['reinforced_label'], \n",
    "                              margins=True)\n",
    "print(crosstab_final)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "crosstab_pct = pd.crosstab(final_results['original_label'], \n",
    "                           final_results['reinforced_label'], \n",
    "                           normalize='index') * 100\n",
    "\n",
    "# Reorder columns\n",
    "col_order = ['very_high_confidence', 'high_confidence', 'medium_confidence', \n",
    "             'low_confidence', 'needs_review']\n",
    "crosstab_pct = crosstab_pct[[c for c in col_order if c in crosstab_pct.columns]]\n",
    "\n",
    "crosstab_pct.plot(kind='bar', stacked=True, ax=ax, \n",
    "                  colormap='RdYlGn', edgecolor='white')\n",
    "ax.set_title('Reinforced Confidence by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Original Anomaly Label')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.legend(title='Confidence', bbox_to_anchor=(1.02, 1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed statistics\n",
    "print(\"\\nDETAILED CONFIDENCE STATISTICS BY ANOMALY TYPE:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for label in final_results['original_label'].unique():\n",
    "    mask = final_results['original_label'] == label\n",
    "    subset = final_results[mask]\n",
    "    \n",
    "    print(f\"\\n{label.upper()}:\")\n",
    "    print(f\"  Samples: {len(subset):,}\")\n",
    "    print(f\"  Final Confidence: mean={subset['final_confidence'].mean():.3f}, \"\n",
    "          f\"std={subset['final_confidence'].std():.3f}\")\n",
    "    print(f\"  Anomaly Score: mean={subset['anomaly_score'].mean():.3f}\")\n",
    "    print(f\"  Classifier Confidence: mean={subset['classifier_confidence'].mean():.3f}\")\n",
    "    print(f\"  Incident Coherence: mean={subset['incident_coherence'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Export Reinforced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge reinforcement results with original data\n",
    "reinforced_df = anomaly_df.copy()\n",
    "\n",
    "# Add reinforcement columns\n",
    "reinforced_df['anomaly_deviation_score'] = final_results['anomaly_score'].values\n",
    "reinforced_df['classifier_confidence'] = final_results['classifier_confidence'].values\n",
    "reinforced_df['incident_coherence'] = final_results['incident_coherence'].values\n",
    "reinforced_df['cluster_aligned'] = final_results['cluster_aligned'].values\n",
    "reinforced_df['final_confidence_score'] = final_results['final_confidence'].values\n",
    "reinforced_df['reinforced_confidence_label'] = final_results['reinforced_label'].values\n",
    "\n",
    "# Add individual method scores\n",
    "for method, scores in all_scores.items():\n",
    "    reinforced_df[f'score_{method}'] = scores\n",
    "\n",
    "# Add method agreement count\n",
    "reinforced_df['method_agreement_count'] = method_agreement\n",
    "\n",
    "print(f\"Reinforced dataset shape: {reinforced_df.shape}\")\n",
    "print(f\"New columns added: {len(reinforced_df.columns) - len(anomaly_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reinforced dataset\n",
    "output_path = 'all_incidents_anomalies_reinforced.csv'\n",
    "reinforced_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nReinforced dataset saved to: {output_path}\")\n",
    "print(f\"File size: {reinforced_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "print(\"\\nSample of reinforced dataset:\")\n",
    "reinforced_df[['label', 'Incident', 'final_confidence_score', \n",
    "               'reinforced_confidence_label', 'method_agreement_count']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BGP ANOMALY LABEL REINFORCEMENT - SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Total samples: {len(anomaly_df):,}\")\n",
    "print(f\"   Anomaly types: {anomaly_df['label'].nunique()}\")\n",
    "print(f\"   Unique incidents: {anomaly_df['Incident'].nunique()}\")\n",
    "print(f\"   Features used: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\n2. ANOMALY TYPE DISTRIBUTION\")\n",
    "print(\"-\"*40)\n",
    "for label, count in anomaly_df['label'].value_counts().items():\n",
    "    pct = count / len(anomaly_df) * 100\n",
    "    print(f\"   {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n3. REINFORCEMENT METHODS USED\")\n",
    "print(\"-\"*40)\n",
    "print(\"   1. Mahalanobis Distance from Normal Baseline\")\n",
    "print(\"   2. One-Class SVM Decision Function\")\n",
    "print(\"   3. Statistical Z-Score Aggregation\")\n",
    "print(\"   4. Local Outlier Factor (LOF)\")\n",
    "print(\"   5. Isolation Forest Anomaly Score\")\n",
    "if TF_AVAILABLE:\n",
    "    print(\"   6. Autoencoder Reconstruction Error\")\n",
    "\n",
    "print(\"\\n4. REINFORCED CONFIDENCE DISTRIBUTION\")\n",
    "print(\"-\"*40)\n",
    "for label, count in final_results['reinforced_label'].value_counts().items():\n",
    "    pct = count / len(final_results) * 100\n",
    "    print(f\"   {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n5. CLASSIFIER PERFORMANCE (Random Forest)\")\n",
    "print(\"-\"*40)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print(f\"   Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"   Macro F1 Score: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
    "\n",
    "print(\"\\n6. CLUSTERING VALIDATION\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Clusters found: {n_clusters}\")\n",
    "print(f\"   Noise points: {n_noise:,} ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "if valid_mask.sum() > 100:\n",
    "    print(f\"   Adjusted Rand Index: {ari:.3f}\")\n",
    "    print(f\"   Normalized Mutual Info: {nmi:.3f}\")\n",
    "\n",
    "print(\"\\n7. INCIDENT COHERENCE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Mean coherence score: {coherence_df['coherence_score'].mean():.2f}\")\n",
    "print(f\"   Median coherence score: {coherence_df['coherence_score'].median():.2f}\")\n",
    "print(f\"   Most coherent incident: {coherence_df.iloc[0]['incident']}\")\n",
    "\n",
    "print(\"\\n8. OUTPUT FILES\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Reinforced dataset: {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REINFORCEMENT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Methodology Summary\n",
    "\n",
    "### Anomaly Label Reinforcement Algorithm\n",
    "\n",
    "```\n",
    "Algorithm: BGP Anomaly Label Reinforcement\n",
    "\n",
    "Input: Anomaly dataset X with labels Y, incidents I\n",
    "Output: Reinforced labels with confidence scores\n",
    "\n",
    "1. BASELINE CONSTRUCTION\n",
    "   a. Load real normal traffic data (if available)\n",
    "   b. OR construct synthetic normal baseline from domain knowledge\n",
    "   c. Compute normal centroid and covariance\n",
    "\n",
    "2. ANOMALY DEVIATION SCORING (5 Methods)\n",
    "   For each method m  {Mahalanobis, OCSVM, Statistical, LOF, IF}:\n",
    "      a. Compute deviation score s_m(x) for each sample\n",
    "      b. Normalize to [0, 1]\n",
    "   Ensemble score: S(x) = mean(s_1(x), ..., s_M(x))\n",
    "\n",
    "3. INCIDENT COHERENCE VALIDATION\n",
    "   For each incident i:\n",
    "      a. Compute intra-incident variance\n",
    "      b. Compute inter-incident distance\n",
    "      c. Coherence(i) = inter_distance / sqrt(intra_variance)\n",
    "\n",
    "4. SUPERVISED CROSS-VALIDATION\n",
    "   a. Train Random Forest on anomaly types\n",
    "   b. Compute prediction probabilities\n",
    "   c. Identify potentially mislabeled samples\n",
    "\n",
    "5. CLUSTERING VALIDATION\n",
    "   a. Apply HDBSCAN to discover natural clusters\n",
    "   b. Compute cluster-label alignment metrics\n",
    "   c. Flag samples in minority clusters\n",
    "\n",
    "6. FINAL CONFIDENCE SCORING\n",
    "   Confidence(x) = wS(x) + wP(y|x) + wC(i) + wA(x)\n",
    "   Where:\n",
    "      S(x) = anomaly deviation score\n",
    "      P(y|x) = classifier probability\n",
    "      C(i) = incident coherence\n",
    "      A(x) = cluster alignment\n",
    "\n",
    "7. ASSIGN REINFORCED LABELS\n",
    "   very_high_confidence: Confidence >= 0.8\n",
    "   high_confidence: Confidence >= 0.65\n",
    "   medium_confidence: Confidence >= 0.5\n",
    "   low_confidence: Confidence >= 0.35\n",
    "   needs_review: Confidence < 0.35\n",
    "\n",
    "Return: Reinforced dataset with confidence scores and labels\n",
    "```\n",
    "\n",
    "### Key Differences from Normal Traffic Labeling\n",
    "\n",
    "| Aspect | Normal Traffic Labeling | Anomaly Label Reinforcement |\n",
    "|--------|------------------------|-----------------------------|\n",
    "| Goal | Discover anomalies in unlabeled data | Validate pre-labeled anomalies |\n",
    "| Baseline | Data itself | External normal profile |\n",
    "| Methods | Anomaly detection | Deviation + Classification |\n",
    "| Output | Binary (normal/anomaly) | Multi-level confidence |\n",
    "| Validation | Cluster discovery | Incident coherence |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
