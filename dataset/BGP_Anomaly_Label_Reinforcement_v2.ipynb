{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGP Anomaly Label Reinforcement Methodology v2\n",
    "\n",
    "## Purpose\n",
    "This notebook implements a **reinforcement methodology for anomaly labeling** - validating and strengthening the confidence of anomaly labels assigned to real-world BGP incidents.\n",
    "\n",
    "## Version 2 Changes\n",
    "- **Method Agreement Scoring**: Confidence based on how many methods agree (not ensemble score)\n",
    "- **Automatic Plot Saving**: All visualizations saved to `plots/` directory\n",
    "- **Dynamic Feature Plotting**: All common features visualized\n",
    "\n",
    "## Methodology Overview\n",
    "\n",
    "### Confidence Assignment (Method Agreement)\n",
    "| Methods Agreeing | Confidence Level |\n",
    "|-----------------|------------------|\n",
    "| 5-6 methods | `very_high_confidence` |\n",
    "| 4 methods | `high_confidence` |\n",
    "| 3 methods | `medium_confidence` |\n",
    "| 2 methods | `low_confidence` |\n",
    "| 0-1 methods | `needs_review` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create plots directory\n",
    "PLOTS_DIR = 'plots'\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "\n",
    "# Anomaly Detection Methods\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy import stats\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import hdbscan\n",
    "\n",
    "# Classification for cross-validation\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, silhouette_score\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "# Dimensionality Reduction\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "try:\n",
    "    import umap\n",
    "    UMAP_AVAILABLE = True\n",
    "except ImportError:\n",
    "    UMAP_AVAILABLE = False\n",
    "\n",
    "# Deep Learning for Autoencoder\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers, Model\n",
    "    TF_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available. Autoencoder method will be skipped.\")\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"TensorFlow available: {TF_AVAILABLE}\")\n",
    "print(f\"UMAP available: {UMAP_AVAILABLE}\")\n",
    "print(f\"Plots will be saved to: {os.path.abspath(PLOTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load and Explore Anomaly Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the anomaly dataset\n",
    "anomaly_df = pd.read_csv('all_incidents_anomalies_only.csv')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANOMALY DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(anomaly_df):,}\")\n",
    "print(f\"Features: {anomaly_df.shape[1]}\")\n",
    "print(f\"\\nColumns: {list(anomaly_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "anomaly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "print(\"\\nLABEL DISTRIBUTION:\")\n",
    "print(\"-\"*40)\n",
    "label_counts = anomaly_df['label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    pct = count / len(anomaly_df) * 100\n",
    "    print(f\"  {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Pie chart\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "axes[0].pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, explode=[0.02]*len(label_counts))\n",
    "axes[0].set_title('Anomaly Type Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar chart\n",
    "bars = axes[1].bar(label_counts.index, label_counts.values, color=colors)\n",
    "axes[1].set_title('Sample Count by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Count')\n",
    "for bar, count in zip(bars, label_counts.values):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200, \n",
    "                 f'{count:,}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/01_anomaly_type_distribution.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved: {PLOTS_DIR}/01_anomaly_type_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incident distribution\n",
    "print(\"\\nINCIDENT DISTRIBUTION:\")\n",
    "print(\"-\"*60)\n",
    "incident_counts = anomaly_df['Incident'].value_counts()\n",
    "for incident, count in incident_counts.head(15).items():\n",
    "    label = anomaly_df[anomaly_df['Incident'] == incident]['label'].iloc[0]\n",
    "    print(f\"  {incident}: {count:,} samples [{label}]\")\n",
    "if len(incident_counts) > 15:\n",
    "    print(f\"  ... and {len(incident_counts) - 15} more incidents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (exclude metadata columns)\n",
    "metadata_cols = ['label', 'Incident', 'window_start', 'window_end']\n",
    "feature_cols = [col for col in anomaly_df.columns if col not in metadata_cols]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and handle missing values\n",
    "X = anomaly_df[feature_cols].copy()\n",
    "y_label = anomaly_df['label'].copy()\n",
    "y_incident = anomaly_df['Incident'].copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing = X.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(\"Missing values found:\")\n",
    "    print(missing[missing > 0])\n",
    "    X = X.fillna(X.median())\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Check for infinite values\n",
    "inf_mask = np.isinf(X.values)\n",
    "if inf_mask.any():\n",
    "    print(f\"\\nInfinite values found: {inf_mask.sum()}\")\n",
    "    X = X.replace([np.inf, -np.inf], np.nan).fillna(X.median())\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols)\n",
    "\n",
    "print(\"Features normalized with RobustScaler\")\n",
    "print(f\"Scaled feature statistics:\")\n",
    "print(f\"  Mean range: [{X_scaled.mean(axis=0).min():.3f}, {X_scaled.mean(axis=0).max():.3f}]\")\n",
    "print(f\"  Std range: [{X_scaled.std(axis=0).min():.3f}, {X_scaled.std(axis=0).max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Baseline Normal Profile Construction\n",
    "\n",
    "Using real normal traffic data from RIPE RRC collectors with `likely_normal` labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalBaselineProfile:\n",
    "    \"\"\"Constructs a statistical profile of normal BGP traffic from real data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fitted = False\n",
    "        self.mean_ = None\n",
    "        self.cov_ = None\n",
    "        self.cov_inv_ = None\n",
    "        self.n_samples_ = 0\n",
    "        self.feature_names_ = None\n",
    "        self.source_files_ = []\n",
    "    \n",
    "    def fit_from_data(self, normal_data, feature_names=None):\n",
    "        \"\"\"Fit baseline from actual normal traffic data.\"\"\"\n",
    "        self.mean_ = np.mean(normal_data, axis=0)\n",
    "        self.cov_ = np.cov(normal_data.T)\n",
    "        if self.cov_.ndim == 0:\n",
    "            self.cov_ = np.array([[self.cov_]])\n",
    "        try:\n",
    "            self.cov_inv_ = np.linalg.pinv(self.cov_)\n",
    "        except:\n",
    "            self.cov_inv_ = np.eye(normal_data.shape[1])\n",
    "        self.n_samples_ = len(normal_data)\n",
    "        self.feature_names_ = feature_names\n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def mahalanobis_distance(self, X):\n",
    "        \"\"\"Calculate Mahalanobis distance from normal baseline.\"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Profile not fitted.\")\n",
    "        distances = []\n",
    "        for x in X:\n",
    "            try:\n",
    "                d = mahalanobis(x, self.mean_, self.cov_inv_)\n",
    "            except:\n",
    "                d = np.sqrt(np.sum((x - self.mean_)**2))\n",
    "            distances.append(d)\n",
    "        return np.array(distances)\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        if not self.fitted:\n",
    "            return None\n",
    "        return {\n",
    "            'n_samples': self.n_samples_,\n",
    "            'n_features': len(self.mean_),\n",
    "            'mean_range': (self.mean_.min(), self.mean_.max()),\n",
    "            'source_files': self.source_files_\n",
    "        }\n",
    "\n",
    "\n",
    "def load_real_normal_data(file_paths, target_label='likely_normal', feature_cols=None):\n",
    "    \"\"\"Load real normal traffic data from multiple files.\"\"\"\n",
    "    all_normal_data = []\n",
    "    loaded_files = []\n",
    "    common_features = None\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING REAL NORMAL TRAFFIC DATA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Target label: '{target_label}'\")\n",
    "    print(f\"Files to load: {len(file_paths)}\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    for path in file_paths:\n",
    "        try:\n",
    "            print(f\"\\nLoading: {path}\")\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"  Total rows: {len(df):,}\")\n",
    "            \n",
    "            # Check for label column - prioritize discovered_label\n",
    "            label_col = None\n",
    "            for col in ['discovered_label', 'label', 'Label', 'LABEL']:\n",
    "                if col in df.columns:\n",
    "                    label_col = col\n",
    "                    break\n",
    "            \n",
    "            if label_col is None:\n",
    "                print(f\"  WARNING: No label column found. Skipping file.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Label column: '{label_col}'\")\n",
    "            print(f\"  Available labels: {df[label_col].unique().tolist()}\")\n",
    "            \n",
    "            # Filter for target label\n",
    "            normal_samples = df[df[label_col] == target_label]\n",
    "            print(f\"  Samples with '{target_label}': {len(normal_samples):,}\")\n",
    "            \n",
    "            if len(normal_samples) == 0:\n",
    "                normal_samples = df[df[label_col].str.lower().str.contains('likely_normal', na=False)]\n",
    "                print(f\"  (Case-insensitive match): {len(normal_samples):,}\")\n",
    "            \n",
    "            if len(normal_samples) < 10:\n",
    "                print(f\"  WARNING: Too few samples. Skipping file.\")\n",
    "                continue\n",
    "            \n",
    "            # Determine features\n",
    "            if feature_cols is not None:\n",
    "                available_features = [f for f in feature_cols if f in normal_samples.columns]\n",
    "            else:\n",
    "                exclude_cols = ['label', 'Label', 'discovered_label', 'LABEL', \n",
    "                               'window_start', 'window_end', 'Incident', 'incident',\n",
    "                               'timestamp', 'Timestamp', 'file', 'File']\n",
    "                available_features = [col for col in normal_samples.columns \n",
    "                                     if col not in exclude_cols and \n",
    "                                     normal_samples[col].dtype in ['int64', 'float64']]\n",
    "            \n",
    "            if common_features is None:\n",
    "                common_features = available_features\n",
    "            else:\n",
    "                common_features = [f for f in common_features if f in available_features]\n",
    "            \n",
    "            print(f\"  Available features: {len(available_features)}\")\n",
    "            X_normal = normal_samples[available_features].values\n",
    "            X_normal = np.nan_to_num(X_normal, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "            \n",
    "            all_normal_data.append((X_normal, available_features, path))\n",
    "            loaded_files.append(path)\n",
    "            print(f\"  SUCCESS: Loaded {len(normal_samples):,} normal samples\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ERROR: File not found: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR: {str(e)}\")\n",
    "    \n",
    "    if len(all_normal_data) == 0:\n",
    "        print(\"\\nERROR: No normal data could be loaded!\")\n",
    "        return None, None, []\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"COMBINING DATA FROM ALL SOURCES\")\n",
    "    print(\"-\"*70)\n",
    "    print(f\"Common features across all files: {len(common_features)}\")\n",
    "    \n",
    "    combined_data = []\n",
    "    for X, features, path in all_normal_data:\n",
    "        feature_indices = [features.index(f) for f in common_features if f in features]\n",
    "        if len(feature_indices) == len(common_features):\n",
    "            X_common = X[:, feature_indices]\n",
    "            combined_data.append(X_common)\n",
    "            print(f\"  {path.split('/')[-1]}: {len(X_common):,} samples\")\n",
    "    \n",
    "    if len(combined_data) == 0:\n",
    "        return None, None, []\n",
    "    \n",
    "    normal_data = np.vstack(combined_data)\n",
    "    print(f\"\\nTotal combined normal samples: {len(normal_data):,}\")\n",
    "    print(f\"Feature dimensions: {normal_data.shape[1]}\")\n",
    "    \n",
    "    return normal_data, common_features, loaded_files\n",
    "\n",
    "\n",
    "# Load real normal traffic data\n",
    "normal_data_paths = [\n",
    "    '/home/smotaali/BGP_Traffic_Generation/results_zend/final_label_results_HDBSCAN/rrc04_updates_20251116_extracted_discovered.csv',\n",
    "    '/home/smotaali/BGP_Traffic_Generation/results_zend/final_label_results_HDBSCAN/rrc05_updates_20251216_extracted_discovered.csv',\n",
    "]\n",
    "\n",
    "normal_data, common_features, loaded_files = load_real_normal_data(\n",
    "    file_paths=normal_data_paths,\n",
    "    target_label='likely_normal',\n",
    "    feature_cols=feature_cols\n",
    ")\n",
    "\n",
    "# Initialize baseline\n",
    "normal_baseline = NormalBaselineProfile()\n",
    "\n",
    "if normal_data is not None and len(normal_data) > 0:\n",
    "    normal_data_scaled = scaler.transform(\n",
    "        pd.DataFrame(normal_data, columns=common_features)[feature_cols].values\n",
    "    ) if set(feature_cols).issubset(set(common_features)) else scaler.fit_transform(normal_data)\n",
    "    \n",
    "    normal_baseline.fit_from_data(normal_data_scaled, feature_names=common_features)\n",
    "    normal_baseline.source_files_ = loaded_files\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"NORMAL BASELINE PROFILE - SUCCESSFULLY FITTED\")\n",
    "    print(\"=\"*70)\n",
    "    stats = normal_baseline.get_statistics()\n",
    "    print(f\"  Total normal samples: {stats['n_samples']:,}\")\n",
    "    print(f\"  Features used: {stats['n_features']}\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Could not load real normal data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Normal Baseline vs Anomaly Data Distribution\n",
    "if normal_baseline.fitted and normal_data is not None:\n",
    "    print(\"\\nNORMAL vs ANOMALY FEATURE COMPARISON\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    features_to_plot = [f for f in common_features if f in feature_cols]\n",
    "    n_features = len(features_to_plot)\n",
    "    \n",
    "    if n_features >= 1:\n",
    "        n_cols = 4\n",
    "        n_rows = (n_features + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 4 * n_rows))\n",
    "        axes = axes.flatten() if n_features > 1 else [axes]\n",
    "        \n",
    "        for idx, feat in enumerate(features_to_plot):\n",
    "            ax = axes[idx]\n",
    "            feat_idx_anomaly = feature_cols.index(feat) if feat in feature_cols else None\n",
    "            feat_idx_normal = common_features.index(feat) if feat in common_features else None\n",
    "            \n",
    "            if feat_idx_anomaly is not None and feat_idx_normal is not None:\n",
    "                normal_vals = normal_data[:, feat_idx_normal]\n",
    "                ax.hist(normal_vals, bins=50, alpha=0.6, label='Normal (likely_normal)', \n",
    "                       color='green', density=True)\n",
    "                \n",
    "                for label in y_label.unique():\n",
    "                    mask = y_label == label\n",
    "                    anomaly_vals = X.values[mask, feat_idx_anomaly]\n",
    "                    ax.hist(anomaly_vals, bins=50, alpha=0.4, label=f'Anomaly: {label}', \n",
    "                           density=True)\n",
    "                \n",
    "                ax.set_title(f'{feat}', fontsize=10, fontweight='bold')\n",
    "                ax.set_xlabel('Value')\n",
    "                ax.set_ylabel('Density')\n",
    "                ax.legend(fontsize=6)\n",
    "        \n",
    "        for idx in range(n_features, len(axes)):\n",
    "            axes[idx].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Feature Distributions: Normal Baseline vs Anomaly Types', \n",
    "                    fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{PLOTS_DIR}/02_normal_vs_anomaly_features.png', dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved: {PLOTS_DIR}/02_normal_vs_anomaly_features.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistics comparison\n",
    "        print(\"\\nFeature Statistics Comparison:\")\n",
    "        print(\"-\"*70)\n",
    "        print(f\"{'Feature':<25} {'Normal Mean':>12} {'Normal Std':>12} {'Anomaly Mean':>12} {'Anomaly Std':>12}\")\n",
    "        print(\"-\"*70)\n",
    "        \n",
    "        for feat in features_to_plot[:15]:\n",
    "            feat_idx_anomaly = feature_cols.index(feat) if feat in feature_cols else None\n",
    "            feat_idx_normal = common_features.index(feat) if feat in common_features else None\n",
    "            if feat_idx_anomaly is not None and feat_idx_normal is not None:\n",
    "                normal_mean = normal_data[:, feat_idx_normal].mean()\n",
    "                normal_std = normal_data[:, feat_idx_normal].std()\n",
    "                anomaly_mean = X.iloc[:, feat_idx_anomaly].mean()\n",
    "                anomaly_std = X.iloc[:, feat_idx_anomaly].std()\n",
    "                print(f\"{feat:<25} {normal_mean:>12.2f} {normal_std:>12.2f} {anomaly_mean:>12.2f} {anomaly_std:>12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Anomaly Deviation Scoring (6 Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyReinforcementScorer:\n",
    "    \"\"\"Ensemble anomaly reinforcement scoring using multiple methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, normal_baseline=None):\n",
    "        self.normal_baseline = normal_baseline\n",
    "        self.scores = {}\n",
    "        self.models = {}\n",
    "        self.binary_flags = {}  # Store binary anomaly flags for method agreement\n",
    "    \n",
    "    def score_mahalanobis(self, X):\n",
    "        print(\"[1/6] Computing Mahalanobis distances...\")\n",
    "        if self.normal_baseline is not None and self.normal_baseline.fitted:\n",
    "            distances = self.normal_baseline.mahalanobis_distance(X)\n",
    "        else:\n",
    "            mean = X.mean(axis=0)\n",
    "            cov = np.cov(X.T)\n",
    "            try:\n",
    "                cov_inv = np.linalg.pinv(cov)\n",
    "                distances = np.array([mahalanobis(x, mean, cov_inv) for x in X])\n",
    "            except:\n",
    "                distances = np.sqrt(np.sum((X - mean)**2, axis=1))\n",
    "        \n",
    "        self.scores['mahalanobis'] = self._normalize_scores(distances)\n",
    "        # Flag as anomaly if distance > median + 1.5*IQR\n",
    "        q75, q25 = np.percentile(distances, [75, 25])\n",
    "        threshold = q75 + 1.5 * (q75 - q25)\n",
    "        self.binary_flags['mahalanobis'] = (distances > threshold).astype(int)\n",
    "        return self.scores['mahalanobis']\n",
    "    \n",
    "    def score_one_class_svm(self, X):\n",
    "        print(\"[2/6] Fitting One-Class SVM...\")\n",
    "        if len(X) > 10000:\n",
    "            np.random.seed(42)\n",
    "            sample_idx = np.random.choice(len(X), 10000, replace=False)\n",
    "            X_train = X[sample_idx]\n",
    "        else:\n",
    "            X_train = X\n",
    "        \n",
    "        ocsvm = OneClassSVM(kernel='rbf', nu=0.1, gamma='scale')\n",
    "        ocsvm.fit(X_train)\n",
    "        self.models['ocsvm'] = ocsvm\n",
    "        \n",
    "        decision = -ocsvm.decision_function(X)\n",
    "        self.scores['ocsvm'] = self._normalize_scores(decision)\n",
    "        self.binary_flags['ocsvm'] = (ocsvm.predict(X) == -1).astype(int)\n",
    "        return self.scores['ocsvm']\n",
    "    \n",
    "    def score_statistical(self, X):\n",
    "        print(\"[3/6] Computing statistical z-scores...\")\n",
    "        mean = X.mean(axis=0)\n",
    "        std = X.std(axis=0) + 1e-10\n",
    "        z_scores = np.abs((X - mean) / std)\n",
    "        \n",
    "        max_z = z_scores.max(axis=1)\n",
    "        extreme_count = (z_scores > 3).sum(axis=1)\n",
    "        combined = max_z + extreme_count * 0.5\n",
    "        \n",
    "        self.scores['statistical'] = self._normalize_scores(combined)\n",
    "        self.binary_flags['statistical'] = (max_z > 3).astype(int)\n",
    "        return self.scores['statistical']\n",
    "    \n",
    "    def score_lof(self, X):\n",
    "        print(\"[4/6] Computing Local Outlier Factor...\")\n",
    "        n_neighbors = min(int(np.sqrt(len(X))), 50)\n",
    "        n_neighbors = max(n_neighbors, 10)\n",
    "        \n",
    "        lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=0.1, novelty=False)\n",
    "        predictions = lof.fit_predict(X)\n",
    "        \n",
    "        lof_scores = -lof.negative_outlier_factor_\n",
    "        self.scores['lof'] = self._normalize_scores(lof_scores)\n",
    "        self.binary_flags['lof'] = (predictions == -1).astype(int)\n",
    "        return self.scores['lof']\n",
    "    \n",
    "    def score_isolation_forest(self, X):\n",
    "        print(\"[5/6] Fitting Isolation Forest...\")\n",
    "        iso_forest = IsolationForest(n_estimators=200, contamination=0.1, \n",
    "                                      random_state=42, n_jobs=-1)\n",
    "        predictions = iso_forest.fit_predict(X)\n",
    "        self.models['isolation_forest'] = iso_forest\n",
    "        \n",
    "        iso_scores = -iso_forest.score_samples(X)\n",
    "        self.scores['isolation_forest'] = self._normalize_scores(iso_scores)\n",
    "        self.binary_flags['isolation_forest'] = (predictions == -1).astype(int)\n",
    "        return self.scores['isolation_forest']\n",
    "    \n",
    "    def score_elliptic_envelope(self, X):\n",
    "        print(\"[6/6] Fitting Elliptic Envelope...\")\n",
    "        try:\n",
    "            ee = EllipticEnvelope(contamination=0.1, random_state=42)\n",
    "            predictions = ee.fit_predict(X)\n",
    "            self.models['elliptic_envelope'] = ee\n",
    "            \n",
    "            ee_scores = -ee.decision_function(X)\n",
    "            self.scores['elliptic_envelope'] = self._normalize_scores(ee_scores)\n",
    "            self.binary_flags['elliptic_envelope'] = (predictions == -1).astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Elliptic Envelope failed ({e}), using fallback\")\n",
    "            self.scores['elliptic_envelope'] = np.zeros(len(X))\n",
    "            self.binary_flags['elliptic_envelope'] = np.zeros(len(X), dtype=int)\n",
    "        return self.scores['elliptic_envelope']\n",
    "    \n",
    "    def _normalize_scores(self, scores):\n",
    "        scores = np.array(scores)\n",
    "        min_s, max_s = scores.min(), scores.max()\n",
    "        if max_s - min_s > 0:\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        return np.zeros_like(scores)\n",
    "    \n",
    "    def compute_all_scores(self, X):\n",
    "        print(\"=\"*60)\n",
    "        print(\"COMPUTING ANOMALY REINFORCEMENT SCORES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.score_mahalanobis(X)\n",
    "        self.score_one_class_svm(X)\n",
    "        self.score_statistical(X)\n",
    "        self.score_lof(X)\n",
    "        self.score_isolation_forest(X)\n",
    "        self.score_elliptic_envelope(X)\n",
    "        \n",
    "        print(\"\\nAll scores computed.\")\n",
    "        return self.scores\n",
    "    \n",
    "    def get_method_agreement(self):\n",
    "        \"\"\"Count how many methods flag each sample as anomaly.\"\"\"\n",
    "        if not self.binary_flags:\n",
    "            raise ValueError(\"No scores computed.\")\n",
    "        \n",
    "        agreement = np.zeros(len(list(self.binary_flags.values())[0]))\n",
    "        for method, flags in self.binary_flags.items():\n",
    "            agreement += flags\n",
    "        return agreement\n",
    "    \n",
    "    def get_ensemble_score(self, weights=None):\n",
    "        if not self.scores:\n",
    "            raise ValueError(\"No scores computed.\")\n",
    "        if weights is None:\n",
    "            weights = {k: 1.0 for k in self.scores.keys()}\n",
    "        total_weight = sum(weights.values())\n",
    "        ensemble = np.zeros(len(list(self.scores.values())[0]))\n",
    "        for method, score in self.scores.items():\n",
    "            w = weights.get(method, 1.0)\n",
    "            ensemble += w * score\n",
    "        return ensemble / total_weight\n",
    "\n",
    "\n",
    "# Initialize scorer and compute all scores\n",
    "scorer = AnomalyReinforcementScorer(normal_baseline=normal_baseline)\n",
    "all_scores = scorer.compute_all_scores(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "n_methods = len(all_scores)\n",
    "n_cols = 3\n",
    "n_rows = (n_methods + 1 + n_cols - 1) // n_cols  # +1 for ensemble\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (method, scores) in enumerate(all_scores.items()):\n",
    "    ax = axes[i]\n",
    "    for label in y_label.unique():\n",
    "        mask = y_label == label\n",
    "        ax.hist(scores[mask], bins=50, alpha=0.6, label=label, density=True)\n",
    "    ax.set_title(f'{method.upper()} Score Distribution', fontweight='bold')\n",
    "    ax.set_xlabel('Anomaly Score')\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.legend()\n",
    "\n",
    "# Ensemble score\n",
    "ensemble_score = scorer.get_ensemble_score()\n",
    "ax = axes[n_methods]\n",
    "for label in y_label.unique():\n",
    "    mask = y_label == label\n",
    "    ax.hist(ensemble_score[mask], bins=50, alpha=0.6, label=label, density=True)\n",
    "ax.set_title('ENSEMBLE Score Distribution', fontweight='bold')\n",
    "ax.set_xlabel('Anomaly Score')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "# Hide empty\n",
    "for idx in range(n_methods + 1, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/03_anomaly_score_distributions.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/03_anomaly_score_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score correlation analysis\n",
    "score_df = pd.DataFrame(all_scores)\n",
    "score_df['ensemble'] = ensemble_score\n",
    "\n",
    "print(\"\\nSCORE CORRELATION MATRIX:\")\n",
    "corr_matrix = score_df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            fmt='.2f', square=True, linewidths=0.5)\n",
    "plt.title('Anomaly Score Method Correlation', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/04_score_correlation_matrix.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/04_score_correlation_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Method Agreement Based Confidence Assignment\n",
    "\n",
    "Confidence is assigned based on **how many methods agree** that a sample is anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_confidence_by_method_agreement(method_agreement, n_methods=6):\n",
    "    \"\"\"\n",
    "    Assign confidence levels based on method agreement count.\n",
    "    \n",
    "    Confidence Levels:\n",
    "    - very_high_confidence: 5-6 methods agree (>=83%)\n",
    "    - high_confidence: 4 methods agree (67%)\n",
    "    - medium_confidence: 3 methods agree (50%)\n",
    "    - low_confidence: 2 methods agree (33%)\n",
    "    - needs_review: 0-1 methods agree (<33%)\n",
    "    \"\"\"\n",
    "    n_samples = len(method_agreement)\n",
    "    confidence_labels = np.array(['needs_review'] * n_samples, dtype=object)\n",
    "    \n",
    "    # Assign based on agreement count\n",
    "    confidence_labels[method_agreement >= 5] = 'very_high_confidence'\n",
    "    confidence_labels[(method_agreement == 4)] = 'high_confidence'\n",
    "    confidence_labels[(method_agreement == 3)] = 'medium_confidence'\n",
    "    confidence_labels[(method_agreement == 2)] = 'low_confidence'\n",
    "    # 0-1 remains 'needs_review'\n",
    "    \n",
    "    return confidence_labels\n",
    "\n",
    "\n",
    "# Get method agreement\n",
    "method_agreement = scorer.get_method_agreement()\n",
    "\n",
    "# Assign confidence labels\n",
    "confidence_labels = assign_confidence_by_method_agreement(method_agreement)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nMETHOD AGREEMENT DISTRIBUTION:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(7):\n",
    "    count = (method_agreement == i).sum()\n",
    "    pct = count / len(method_agreement) * 100\n",
    "    print(f\"  {i} methods agree: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nCONFIDENCE DISTRIBUTION (Method Agreement Based):\")\n",
    "print(\"=\"*60)\n",
    "conf_counts = pd.Series(confidence_labels).value_counts()\n",
    "for conf, count in conf_counts.items():\n",
    "    pct = count / len(confidence_labels) * 100\n",
    "    print(f\"  {conf}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize method agreement\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Method agreement histogram\n",
    "ax = axes[0]\n",
    "agreement_counts = [int((method_agreement == i).sum()) for i in range(7)]\n",
    "colors_agree = ['#d73027', '#fc8d59', '#fee090', '#e0f3f8', '#91bfdb', '#4575b4', '#313695']\n",
    "bars = ax.bar(range(7), agreement_counts, color=colors_agree, edgecolor='black')\n",
    "ax.set_xlabel('Number of Methods Agreeing')\n",
    "ax.set_ylabel('Sample Count')\n",
    "ax.set_title('Method Agreement Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(7))\n",
    "for bar, count in zip(bars, agreement_counts):\n",
    "    if count > 0:\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "                f'{count:,}', ha='center', fontsize=9)\n",
    "\n",
    "# Confidence pie chart\n",
    "ax = axes[1]\n",
    "conf_order = ['very_high_confidence', 'high_confidence', 'medium_confidence', \n",
    "              'low_confidence', 'needs_review']\n",
    "conf_colors = ['#1a9850', '#91cf60', '#d9ef8b', '#fee08b', '#d73027']\n",
    "sizes = [conf_counts.get(c, 0) for c in conf_order]\n",
    "labels_pie = [f\"{c}\\n({s:,})\" for c, s in zip(conf_order, sizes) if s > 0]\n",
    "sizes_filtered = [s for s in sizes if s > 0]\n",
    "colors_filtered = [c for c, s in zip(conf_colors, sizes) if s > 0]\n",
    "\n",
    "ax.pie(sizes_filtered, labels=labels_pie, colors=colors_filtered, autopct='%1.1f%%',\n",
    "       explode=[0.02]*len(sizes_filtered))\n",
    "ax.set_title('Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/05_method_agreement_distribution.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/05_method_agreement_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulate confidence with original labels\n",
    "print(\"\\nCONFIDENCE vs ORIGINAL LABEL:\")\n",
    "print(\"=\"*60)\n",
    "crosstab = pd.crosstab(y_label, confidence_labels, margins=True)\n",
    "print(crosstab)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "crosstab_pct = pd.crosstab(y_label, confidence_labels, normalize='index') * 100\n",
    "\n",
    "col_order = ['very_high_confidence', 'high_confidence', 'medium_confidence', \n",
    "             'low_confidence', 'needs_review']\n",
    "crosstab_pct = crosstab_pct[[c for c in col_order if c in crosstab_pct.columns]]\n",
    "\n",
    "crosstab_pct.plot(kind='bar', stacked=True, ax=ax, \n",
    "                  color=['#1a9850', '#91cf60', '#d9ef8b', '#fee08b', '#d73027'],\n",
    "                  edgecolor='white')\n",
    "ax.set_title('Confidence Distribution by Anomaly Type (Method Agreement)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Original Anomaly Label')\n",
    "ax.set_ylabel('Percentage')\n",
    "ax.legend(title='Confidence', bbox_to_anchor=(1.02, 1))\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/06_confidence_by_anomaly_type.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved: {PLOTS_DIR}/06_confidence_by_anomaly_type.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Incident Coherence Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_incident_coherence(X, incidents):\n",
    "    unique_incidents = incidents.unique()\n",
    "    coherence_results = []\n",
    "    \n",
    "    for incident in unique_incidents:\n",
    "        mask = incidents == incident\n",
    "        X_incident = X[mask]\n",
    "        X_other = X[~mask]\n",
    "        \n",
    "        if len(X_incident) < 2:\n",
    "            continue\n",
    "        \n",
    "        centroid = X_incident.mean(axis=0)\n",
    "        intra_variance = np.mean(np.sum((X_incident - centroid)**2, axis=1))\n",
    "        other_centroid = X_other.mean(axis=0) if len(X_other) > 0 else centroid\n",
    "        inter_distance = np.sqrt(np.sum((centroid - other_centroid)**2))\n",
    "        coherence = inter_distance / (np.sqrt(intra_variance) + 1e-10)\n",
    "        \n",
    "        coherence_results.append({\n",
    "            'incident': incident,\n",
    "            'n_samples': mask.sum(),\n",
    "            'intra_variance': intra_variance,\n",
    "            'inter_distance': inter_distance,\n",
    "            'coherence_score': coherence\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(coherence_results)\n",
    "\n",
    "\n",
    "coherence_df = compute_incident_coherence(X_scaled, y_incident)\n",
    "coherence_df = coherence_df.sort_values('coherence_score', ascending=False)\n",
    "\n",
    "print(\"\\nINCIDENT COHERENCE SCORES:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Incident':<45} {'Samples':>8} {'Coherence':>12}\")\n",
    "print(\"-\"*80)\n",
    "for _, row in coherence_df.head(15).iterrows():\n",
    "    print(f\"{row['incident']:<45} {row['n_samples']:>8,} {row['coherence_score']:>12.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize coherence scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "ax = axes[0]\n",
    "ax.hist(coherence_df['coherence_score'], bins=30, color='steelblue', edgecolor='white')\n",
    "ax.axvline(coherence_df['coherence_score'].median(), color='red', linestyle='--', \n",
    "           label=f\"Median: {coherence_df['coherence_score'].median():.2f}\")\n",
    "ax.set_title('Incident Coherence Score Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Coherence Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "top_5 = coherence_df.head(5)\n",
    "bottom_5 = coherence_df.tail(5)\n",
    "combined = pd.concat([top_5, bottom_5])\n",
    "colors = ['green']*5 + ['red']*5\n",
    "y_pos = range(len(combined))\n",
    "ax.barh(y_pos, combined['coherence_score'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([inc[:30] + '...' if len(inc) > 30 else inc for inc in combined['incident']])\n",
    "ax.set_xlabel('Coherence Score')\n",
    "ax.set_title('Top 5 (Green) vs Bottom 5 (Red) Incidents', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/07_incident_coherence.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/07_incident_coherence.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Cross-Validation with Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y_label)\n",
    "\n",
    "print(\"Label encoding:\")\n",
    "for i, label in enumerate(le.classes_):\n",
    "    print(f\"  {i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTRAINING RANDOM FOREST FOR CROSS-VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, max_depth=20, \n",
    "                            random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Anomaly Type Classification Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/08_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/08_confusion_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "proba = rf.predict_proba(X_scaled)\n",
    "predictions = rf.predict(X_scaled)\n",
    "\n",
    "mismatch_mask = predictions != y_encoded\n",
    "print(f\"\\nSamples with prediction mismatch: {mismatch_mask.sum():,} ({mismatch_mask.sum()/len(y_encoded)*100:.1f}%)\")\n",
    "\n",
    "assigned_proba = proba[np.arange(len(y_encoded)), y_encoded]\n",
    "low_confidence_mask = assigned_proba < 0.5\n",
    "print(f\"Samples with low classifier confidence (<50%): {low_confidence_mask.sum():,} ({low_confidence_mask.sum()/len(y_encoded)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTOP 15 FEATURES FOR ANOMALY TYPE CLASSIFICATION:\")\n",
    "print(importance_df.head(15).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = importance_df.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'], color='steelblue')\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 15 Features for Anomaly Type Discrimination', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/09_feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"\\nPlot saved: {PLOTS_DIR}/09_feature_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Clustering Validation (HDBSCAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performing dimensionality reduction for clustering...\")\n",
    "\n",
    "pca = PCA(n_components=min(10, X_scaled.shape[1]), random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "print(f\"PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "if UMAP_AVAILABLE:\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=30, min_dist=0.1)\n",
    "    X_2d = reducer.fit_transform(X_pca)\n",
    "    print(\"UMAP reduction complete.\")\n",
    "else:\n",
    "    print(\"Using t-SNE (UMAP not available)...\")\n",
    "    X_2d = TSNE(n_components=2, random_state=42, perplexity=30).fit_transform(X_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPerforming HDBSCAN clustering...\")\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=10, cluster_selection_epsilon=0.5)\n",
    "cluster_labels = clusterer.fit_predict(X_pca)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = (cluster_labels == -1).sum()\n",
    "\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "print(f\"Noise points: {n_noise:,} ({n_noise/len(cluster_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare clusters with labels\n",
    "print(\"\\nCLUSTER vs LABEL ALIGNMENT:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "valid_mask = cluster_labels != -1\n",
    "if valid_mask.sum() > 100:\n",
    "    ari = adjusted_rand_score(y_encoded[valid_mask], cluster_labels[valid_mask])\n",
    "    nmi = normalized_mutual_info_score(y_encoded[valid_mask], cluster_labels[valid_mask])\n",
    "    print(f\"Adjusted Rand Index: {ari:.3f}\")\n",
    "    print(f\"Normalized Mutual Information: {nmi:.3f}\")\n",
    "\n",
    "cluster_label_crosstab = pd.crosstab(cluster_labels, y_label, margins=True)\n",
    "print(\"\\nCluster vs Anomaly Type:\")\n",
    "print(cluster_label_crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters vs labels\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "ax = axes[0]\n",
    "for label in y_label.unique():\n",
    "    mask = y_label == label\n",
    "    ax.scatter(X_2d[mask, 0], X_2d[mask, 1], alpha=0.5, s=10, label=label)\n",
    "ax.set_title('2D Projection by Anomaly Type', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(X_2d[:, 0], X_2d[:, 1], c=cluster_labels, cmap='tab20', alpha=0.5, s=10)\n",
    "ax.set_title('2D Projection by HDBSCAN Cluster', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Dimension 1')\n",
    "ax.set_ylabel('Dimension 2')\n",
    "plt.colorbar(scatter, ax=ax, label='Cluster')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{PLOTS_DIR}/10_clustering_visualization.png', dpi=150, bbox_inches='tight')\n",
    "print(f\"Plot saved: {PLOTS_DIR}/10_clustering_visualization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Final Reinforced Labels and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final results dataframe\n",
    "final_results = pd.DataFrame({\n",
    "    'original_label': y_label,\n",
    "    'incident': y_incident,\n",
    "    'method_agreement': method_agreement,\n",
    "    'confidence_label': confidence_labels,\n",
    "    'ensemble_score': ensemble_score,\n",
    "    'classifier_confidence': proba[np.arange(len(y_encoded)), y_encoded]\n",
    "})\n",
    "\n",
    "# Add incident coherence\n",
    "coherence_map = dict(zip(coherence_df['incident'], coherence_df['coherence_score']))\n",
    "max_coherence = coherence_df['coherence_score'].max()\n",
    "final_results['incident_coherence'] = [coherence_map.get(inc, 0) / max_coherence for inc in y_incident]\n",
    "\n",
    "print(\"\\nFINAL REINFORCEMENT SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total samples: {len(final_results):,}\")\n",
    "print(\"\\nConfidence distribution:\")\n",
    "for conf, count in final_results['confidence_label'].value_counts().items():\n",
    "    pct = count / len(final_results) * 100\n",
    "    print(f\"  {conf}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with original data and export\n",
    "reinforced_df = anomaly_df.copy()\n",
    "\n",
    "# Add reinforcement columns\n",
    "reinforced_df['method_agreement_count'] = method_agreement\n",
    "reinforced_df['reinforced_confidence_label'] = confidence_labels\n",
    "reinforced_df['ensemble_score'] = ensemble_score\n",
    "reinforced_df['classifier_confidence'] = proba[np.arange(len(y_encoded)), y_encoded]\n",
    "reinforced_df['incident_coherence'] = final_results['incident_coherence'].values\n",
    "\n",
    "# Add individual method scores and flags\n",
    "for method, scores in all_scores.items():\n",
    "    reinforced_df[f'score_{method}'] = scores\n",
    "    reinforced_df[f'flag_{method}'] = scorer.binary_flags[method]\n",
    "\n",
    "print(f\"Reinforced dataset shape: {reinforced_df.shape}\")\n",
    "print(f\"New columns added: {len(reinforced_df.columns) - len(anomaly_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save reinforced dataset\n",
    "output_path = 'all_incidents_anomalies_reinforced_v2.csv'\n",
    "reinforced_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nReinforced dataset saved to: {output_path}\")\n",
    "print(f\"File size: {reinforced_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview\n",
    "print(\"\\nSample of reinforced dataset:\")\n",
    "reinforced_df[['label', 'Incident', 'method_agreement_count', \n",
    "               'reinforced_confidence_label', 'ensemble_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BGP ANOMALY LABEL REINFORCEMENT v2 - SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET OVERVIEW\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Total samples: {len(anomaly_df):,}\")\n",
    "print(f\"   Anomaly types: {anomaly_df['label'].nunique()}\")\n",
    "print(f\"   Unique incidents: {anomaly_df['Incident'].nunique()}\")\n",
    "print(f\"   Features used: {len(feature_cols)}\")\n",
    "\n",
    "print(\"\\n2. ANOMALY TYPE DISTRIBUTION\")\n",
    "print(\"-\"*40)\n",
    "for label, count in anomaly_df['label'].value_counts().items():\n",
    "    pct = count / len(anomaly_df) * 100\n",
    "    print(f\"   {label}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n3. REINFORCEMENT METHODS USED\")\n",
    "print(\"-\"*40)\n",
    "print(\"   1. Mahalanobis Distance from Normal Baseline\")\n",
    "print(\"   2. One-Class SVM Decision Function\")\n",
    "print(\"   3. Statistical Z-Score Aggregation\")\n",
    "print(\"   4. Local Outlier Factor (LOF)\")\n",
    "print(\"   5. Isolation Forest Anomaly Score\")\n",
    "print(\"   6. Elliptic Envelope\")\n",
    "\n",
    "print(\"\\n4. METHOD AGREEMENT DISTRIBUTION\")\n",
    "print(\"-\"*40)\n",
    "for i in range(7):\n",
    "    count = (method_agreement == i).sum()\n",
    "    pct = count / len(method_agreement) * 100\n",
    "    print(f\"   {i} methods agree: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n5. REINFORCED CONFIDENCE DISTRIBUTION\")\n",
    "print(\"-\"*40)\n",
    "for conf, count in final_results['confidence_label'].value_counts().items():\n",
    "    pct = count / len(final_results) * 100\n",
    "    print(f\"   {conf}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n6. CLASSIFIER PERFORMANCE (Random Forest)\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"   Macro F1 Score: {f1_score(y_test, y_pred, average='macro'):.3f}\")\n",
    "\n",
    "print(\"\\n7. CLUSTERING VALIDATION\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Clusters found: {n_clusters}\")\n",
    "print(f\"   Noise points: {n_noise:,} ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "if valid_mask.sum() > 100:\n",
    "    print(f\"   Adjusted Rand Index: {ari:.3f}\")\n",
    "    print(f\"   Normalized Mutual Info: {nmi:.3f}\")\n",
    "\n",
    "print(\"\\n8. INCIDENT COHERENCE\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Mean coherence score: {coherence_df['coherence_score'].mean():.2f}\")\n",
    "print(f\"   Median coherence score: {coherence_df['coherence_score'].median():.2f}\")\n",
    "print(f\"   Most coherent incident: {coherence_df.iloc[0]['incident']}\")\n",
    "\n",
    "print(\"\\n9. OUTPUT FILES\")\n",
    "print(\"-\"*40)\n",
    "print(f\"   Reinforced dataset: {output_path}\")\n",
    "print(f\"   Plots directory: {os.path.abspath(PLOTS_DIR)}\")\n",
    "\n",
    "# List saved plots\n",
    "print(\"\\n   Saved plots:\")\n",
    "for f in sorted(os.listdir(PLOTS_DIR)):\n",
    "    if f.endswith('.png'):\n",
    "        print(f\"     - {f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REINFORCEMENT COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
