{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified BGP Feature Extraction\n",
    "\n",
    "This notebook extracts BGP features from incident data.\n",
    "\n",
    "Based on the working feature extraction logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# For incident data processing\n",
    "INCIDENT_BASE_DIR = Path(\"/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS\")\n",
    "\n",
    "# Feature extraction settings\n",
    "WINDOW_SIZE = '1s'\n",
    "LABEL_STRATEGY = 'majority'  # Options: 'majority', 'conservative', 'weighted'\n",
    "SKIP_EXISTING = False  # Set to True to skip already processed files\n",
    "\n",
    "print(f\"Base directory: {INCIDENT_BASE_DIR}\")\n",
    "print(f\"Window size: {WINDOW_SIZE}\")\n",
    "print(f\"Skip existing: {SKIP_EXISTING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edit_distance(as_path1, as_path2):\n",
    "    \"\"\"\n",
    "    Calculate edit distance between two AS paths\n",
    "    \"\"\"\n",
    "    if not as_path1 or not as_path2:\n",
    "        return 0\n",
    "    \n",
    "    # Handle integer AS paths by converting them to single-item lists\n",
    "    if isinstance(as_path1, int):\n",
    "        as_path1 = [as_path1]\n",
    "    \n",
    "    if isinstance(as_path2, int):\n",
    "        as_path2 = [as_path2]\n",
    "    \n",
    "    # Convert paths to lists if they are strings\n",
    "    if isinstance(as_path1, str):\n",
    "        as_path1 = as_path1.replace('{', '').replace('}', '')\n",
    "        as_path1 = [int(as_num) for as_num in as_path1.split() if as_num.isdigit()]\n",
    "    \n",
    "    if isinstance(as_path2, str):\n",
    "        as_path2 = as_path2.replace('{', '').replace('}', '')\n",
    "        as_path2 = [int(as_num) for as_num in as_path2.split() if as_num.isdigit()]\n",
    "    \n",
    "    if not as_path1 or not as_path2:\n",
    "        return 0\n",
    "    \n",
    "    m, n = len(as_path1), len(as_path2)\n",
    "    \n",
    "    # Initialize the distance matrix\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Fill the first row and column\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Calculate edit distance\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if as_path1[i-1] == as_path2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "print(\"Edit distance function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df_window, entry_type_col='Entry_Type'):\n",
    "    \"\"\"\n",
    "    Extract BGP features from a dataframe within a specific time window.\n",
    "    This is the exact working logic.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # Define announcement and withdrawal types\n",
    "    announce_types = ['A', 'ANNOUNCE']\n",
    "    withdrawal_types = ['W', 'WITHDRAW', 'WITHDRAW_MP_UNREACH_NLRI_AFI2']\n",
    "    \n",
    "    # 1. ANNOUNCEMENTS\n",
    "    announcements = df_window[df_window[entry_type_col].isin(announce_types)]\n",
    "    \n",
    "    # 2. WITHDRAWALS\n",
    "    withdrawals = df_window[df_window[entry_type_col].isin(withdrawal_types)]\n",
    "    \n",
    "    # Store counts\n",
    "    features['announcements'] = len(announcements)\n",
    "    features['withdrawals'] = len(withdrawals)\n",
    "    \n",
    "    # 3. NLRI_ANN\n",
    "    features['nlri_ann'] = features['announcements']\n",
    "    \n",
    "    # 4. DUPLICATES\n",
    "    if not announcements.empty:\n",
    "        dup_cols = ['Peer_IP', 'Peer_ASN', 'Prefix', 'AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "        dup_cols = [col for col in dup_cols if col in announcements.columns]\n",
    "        \n",
    "        if dup_cols:\n",
    "            announcement_counts = announcements.groupby(dup_cols, dropna=False).size()\n",
    "            duplicates = sum(count - 1 for count in announcement_counts if count > 1)\n",
    "            features['dups'] = duplicates\n",
    "        else:\n",
    "            features['dups'] = 0\n",
    "    else:\n",
    "        features['dups'] = 0\n",
    "    \n",
    "    # 5-7. ORIGIN ATTRIBUTES\n",
    "    if not announcements.empty and 'Origin' in announcements.columns:\n",
    "        origin_counts = announcements['Origin'].value_counts()\n",
    "        # Handle both string ('IGP', 'INCOMPLETE') and numeric (0, 2) formats\n",
    "        features['origin_0'] = origin_counts.get('IGP', 0) + origin_counts.get(0, 0) + origin_counts.get('0', 0)\n",
    "        features['origin_2'] = origin_counts.get('INCOMPLETE', 0) + origin_counts.get(2, 0) + origin_counts.get('2', 0)\n",
    "        \n",
    "        # 8. ORIGIN CHANGES - prefixes with more than one unique origin\n",
    "        unique_prefix_origins = announcements.groupby('Prefix')['Origin'].nunique()\n",
    "        features['origin_changes'] = (unique_prefix_origins > 1).sum()\n",
    "    else:\n",
    "        features['origin_0'] = 0\n",
    "        features['origin_2'] = 0\n",
    "        features['origin_changes'] = 0\n",
    "    \n",
    "    # 9-10. AS PATH METRICS\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        valid_as_paths = announcements[announcements['AS_Path'].notna() & (announcements['AS_Path'] != '')]\n",
    "        \n",
    "        if not valid_as_paths.empty:\n",
    "            # Calculate AS path lengths\n",
    "            as_path_lengths = valid_as_paths['AS_Path'].apply(\n",
    "                lambda path: len([p for p in str(path).split() if p.isdigit()]) if pd.notna(path) else 0\n",
    "            )\n",
    "            \n",
    "            features['as_path_max'] = int(as_path_lengths.max()) if not as_path_lengths.empty else 0\n",
    "            \n",
    "            unique_paths_per_prefix = valid_as_paths.groupby('Prefix')['AS_Path'].nunique()\n",
    "            features['unique_as_path_max'] = int(unique_paths_per_prefix.max()) if not unique_paths_per_prefix.empty else 0\n",
    "            \n",
    "            # 11-20. EDIT DISTANCE FEATURES\n",
    "            edit_distances = []\n",
    "            edit_distance_dict = defaultdict(list)\n",
    "            \n",
    "            for prefix, group in valid_as_paths.groupby('Prefix'):\n",
    "                if len(group) >= 2:\n",
    "                    sorted_group = group.sort_values('Timestamp')\n",
    "                    prev_path = None\n",
    "                    \n",
    "                    for _, row in sorted_group.iterrows():\n",
    "                        current_path = row['AS_Path']\n",
    "                        \n",
    "                        if prev_path is not None:\n",
    "                            dist = calculate_edit_distance(prev_path, current_path)\n",
    "                            edit_distances.append(dist)\n",
    "                            edit_distance_dict[prefix].append(dist)\n",
    "                        \n",
    "                        prev_path = current_path\n",
    "            \n",
    "            if edit_distances:\n",
    "                features['edit_distance_avg'] = np.mean(edit_distances)\n",
    "                features['edit_distance_max'] = max(edit_distances)\n",
    "                \n",
    "                edit_dist_counter = Counter(edit_distances)\n",
    "                for i in range(7):\n",
    "                    features[f'edit_distance_dict_{i}'] = edit_dist_counter.get(i, 0)\n",
    "                \n",
    "                unique_edit_dists = {}\n",
    "                for prefix, dists in edit_distance_dict.items():\n",
    "                    unique_dists = set(dists)\n",
    "                    for dist in unique_dists:\n",
    "                        if dist in unique_edit_dists:\n",
    "                            unique_edit_dists[dist] += 1\n",
    "                        else:\n",
    "                            unique_edit_dists[dist] = 1\n",
    "                \n",
    "                for i in range(2):\n",
    "                    features[f'edit_distance_unique_dict_{i}'] = unique_edit_dists.get(i, 0)\n",
    "            else:\n",
    "                features['edit_distance_avg'] = 0\n",
    "                features['edit_distance_max'] = 0\n",
    "                for i in range(7):\n",
    "                    features[f'edit_distance_dict_{i}'] = 0\n",
    "                for i in range(2):\n",
    "                    features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "        else:\n",
    "            features['as_path_max'] = 0\n",
    "            features['unique_as_path_max'] = 0\n",
    "            features['edit_distance_avg'] = 0\n",
    "            features['edit_distance_max'] = 0\n",
    "            for i in range(7):\n",
    "                features[f'edit_distance_dict_{i}'] = 0\n",
    "            for i in range(2):\n",
    "                features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "    else:\n",
    "        features['as_path_max'] = 0\n",
    "        features['unique_as_path_max'] = 0\n",
    "        features['edit_distance_avg'] = 0\n",
    "        features['edit_distance_max'] = 0\n",
    "        for i in range(7):\n",
    "            features[f'edit_distance_dict_{i}'] = 0\n",
    "        for i in range(2):\n",
    "            features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "    \n",
    "    # 22-24. IMPLICIT WITHDRAWAL FEATURES\n",
    "    if not announcements.empty:\n",
    "        prefix_peer_groups = announcements.groupby(['Prefix', 'Peer_IP'])\n",
    "        \n",
    "        imp_wd_prefixes = 0\n",
    "        imp_wd_spath_prefixes = 0\n",
    "        imp_wd_dpath_prefixes = 0\n",
    "        \n",
    "        for (prefix, peer), group in prefix_peer_groups:\n",
    "            if len(group) > 1:\n",
    "                imp_wd_prefixes += 1\n",
    "                \n",
    "                if 'AS_Path' in group.columns:\n",
    "                    unique_paths = group['AS_Path'].nunique()\n",
    "                    if unique_paths == 1:\n",
    "                        imp_wd_spath_prefixes += 1\n",
    "                    else:\n",
    "                        imp_wd_dpath_prefixes += 1\n",
    "        \n",
    "        features['imp_wd'] = imp_wd_prefixes\n",
    "        features['imp_wd_spath'] = imp_wd_spath_prefixes\n",
    "        features['imp_wd_dpath'] = imp_wd_dpath_prefixes\n",
    "    else:\n",
    "        features['imp_wd'] = 0\n",
    "        features['imp_wd_spath'] = 0\n",
    "        features['imp_wd_dpath'] = 0\n",
    "    \n",
    "    # 25-26. RARE AS FEATURES\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        all_asns = []\n",
    "        \n",
    "        for as_path in announcements['AS_Path']:\n",
    "            if pd.isnull(as_path) or as_path == '':\n",
    "                continue\n",
    "            \n",
    "            as_path_str = str(as_path)\n",
    "            \n",
    "            if as_path_str.isdigit():\n",
    "                all_asns.append(as_path_str)\n",
    "                continue\n",
    "            \n",
    "            as_path_str = as_path_str.replace('{', '').replace('}', '')\n",
    "            path_asns = [asn for asn in as_path_str.split() if asn.isdigit()]\n",
    "            all_asns.extend(path_asns)\n",
    "        \n",
    "        asn_counts = Counter(all_asns)\n",
    "        rare_threshold = 3\n",
    "        rare_asns = [asn for asn, count in asn_counts.items() if count < rare_threshold]\n",
    "        \n",
    "        features['number_rare_ases'] = len(rare_asns)\n",
    "        features['rare_ases_avg'] = len(rare_asns) / len(all_asns) if all_asns else 0\n",
    "    else:\n",
    "        features['number_rare_ases'] = 0\n",
    "        features['rare_ases_avg'] = 0\n",
    "    \n",
    "    # 27-28. FLAP AND NADAS FEATURES\n",
    "    if not df_window.empty:\n",
    "        if not withdrawals.empty and not announcements.empty:\n",
    "            withdrawn_prefixes = set(withdrawals['Prefix'].dropna())\n",
    "            announced_prefixes = set(announcements['Prefix'].dropna())\n",
    "            flapped_prefixes = withdrawn_prefixes.intersection(announced_prefixes)\n",
    "            features['flaps'] = len(flapped_prefixes)\n",
    "        else:\n",
    "            features['flaps'] = 0\n",
    "        \n",
    "        very_specific_prefixes = 0\n",
    "        if 'Prefix' in df_window.columns:\n",
    "            very_specific_prefixes = sum(1 for prefix in df_window['Prefix'].dropna() \n",
    "                                       if isinstance(prefix, str) and prefix.endswith('/32'))\n",
    "        \n",
    "        wd_ann_ratio = features['withdrawals'] / features['announcements'] if features['announcements'] > 0 else 0\n",
    "        features['nadas'] = very_specific_prefixes + (wd_ann_ratio > 0.5) * 10\n",
    "    else:\n",
    "        features['flaps'] = 0\n",
    "        features['nadas'] = 0\n",
    "    \n",
    "    # LABEL\n",
    "    if 'Label' in df_window.columns:\n",
    "        labels = df_window['Label'].value_counts()\n",
    "        if not labels.empty:\n",
    "            if LABEL_STRATEGY == 'majority':\n",
    "                features['label'] = labels.idxmax()\n",
    "            elif LABEL_STRATEGY == 'conservative':\n",
    "                if any(label != 'normal' for label in labels.index):\n",
    "                    abnormal_labels = [label for label in labels.index if label != 'normal']\n",
    "                    features['label'] = abnormal_labels[0]\n",
    "                else:\n",
    "                    features['label'] = 'normal'\n",
    "            elif LABEL_STRATEGY == 'weighted':\n",
    "                total = labels.sum()\n",
    "                abnormal_weight = sum(count for label, count in labels.items() if label != 'normal') / total\n",
    "                if abnormal_weight > 0.4:\n",
    "                    abnormal_labels = [label for label in labels.index if label != 'normal']\n",
    "                    features['label'] = abnormal_labels[0] if abnormal_labels else 'normal'\n",
    "                else:\n",
    "                    features['label'] = 'normal'\n",
    "        else:\n",
    "            features['label'] = 'unknown'\n",
    "    else:\n",
    "        features['label'] = 'unknown'\n",
    "    \n",
    "    # Incident name\n",
    "    if 'Incident' in df_window.columns:\n",
    "        features['Incident'] = df_window['Incident'].iloc[0]\n",
    "    \n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Process a single BGP data file and extract features.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {input_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Read data\n",
    "    df = pd.read_csv(input_path, low_memory=False)\n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "    \n",
    "    # Standardize column names\n",
    "    if 'Time' in df.columns and 'Timestamp' not in df.columns:\n",
    "        df['Timestamp'] = pd.to_datetime(df['Time'])\n",
    "    else:\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "    \n",
    "    # Determine entry type column\n",
    "    if 'Entry_Type' in df.columns:\n",
    "        entry_type_col = 'Entry_Type'\n",
    "    elif 'Subtype' in df.columns:\n",
    "        entry_type_col = 'Subtype'\n",
    "    else:\n",
    "        print(\"ERROR: No Entry_Type or Subtype column found!\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Entry type column: {entry_type_col}\")\n",
    "    print(f\"Entry types: {df[entry_type_col].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Check Origin column\n",
    "    if 'Origin' in df.columns:\n",
    "        print(f\"Origin values: {df['Origin'].value_counts().to_dict()}\")\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('Timestamp')\n",
    "    \n",
    "    # Get time range\n",
    "    start_time = df['Timestamp'].min()\n",
    "    end_time = df['Timestamp'].max()\n",
    "    print(f\"Time range: {start_time} to {end_time}\")\n",
    "    \n",
    "    # Set index for grouping\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "    \n",
    "    # Extract features per window\n",
    "    all_features = []\n",
    "    grouped = df.groupby(pd.Grouper(freq=WINDOW_SIZE))\n",
    "    window_count = 0\n",
    "    total_windows = len(grouped)\n",
    "    \n",
    "    for window_start, window_df in grouped:\n",
    "        if not window_df.empty:\n",
    "            window_df = window_df.reset_index()\n",
    "            features = extract_features(window_df, entry_type_col)\n",
    "            \n",
    "            if features:\n",
    "                window_end = window_start + pd.Timedelta(WINDOW_SIZE)\n",
    "                features['window_start'] = window_start\n",
    "                features['window_end'] = window_end\n",
    "                all_features.append(features)\n",
    "                window_count += 1\n",
    "                \n",
    "                if window_count % 500 == 0:\n",
    "                    print(f\"  Processed {window_count}/{total_windows} windows...\")\n",
    "    \n",
    "    print(f\"Total windows: {window_count}\")\n",
    "    \n",
    "    # Save features\n",
    "    if all_features:\n",
    "        features_df = pd.DataFrame(all_features)\n",
    "        features_df.to_csv(output_path, index=False)\n",
    "        print(f\"Saved to: {output_path}\")\n",
    "        print(f\"Shape: {features_df.shape}\")\n",
    "        \n",
    "        # Show sample of key features\n",
    "        print(f\"\\nSample feature values (first non-zero):\")\n",
    "        for col in ['origin_0', 'origin_2', 'origin_changes', 'edit_distance_avg', 'imp_wd']:\n",
    "            if col in features_df.columns:\n",
    "                non_zero = features_df[features_df[col] > 0][col]\n",
    "                if not non_zero.empty:\n",
    "                    print(f\"  {col}: {non_zero.iloc[0]:.2f} (first non-zero)\")\n",
    "                else:\n",
    "                    print(f\"  {col}: all zeros\")\n",
    "        \n",
    "        return features_df\n",
    "    else:\n",
    "        print(\"No features extracted!\")\n",
    "        return None\n",
    "\n",
    "print(\"File processing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process All Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"PROCESSING INCIDENT DATA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Find all incident directories\n",
    "incident_dirs = [d for d in INCIDENT_BASE_DIR.iterdir() if d.is_dir() and d.name != 'temp_mrt']\n",
    "print(f\"Found {len(incident_dirs)} incident directories\")\n",
    "\n",
    "processed = 0\n",
    "skipped = 0\n",
    "failed = 0\n",
    "all_features_dfs = []\n",
    "\n",
    "for incident_dir in sorted(incident_dirs):\n",
    "    labeled_files = list(incident_dir.glob(\"*_labeled.csv\"))\n",
    "    \n",
    "    if not labeled_files:\n",
    "        print(f\"\\n[SKIP] No labeled file in {incident_dir.name}\")\n",
    "        continue\n",
    "    \n",
    "    csv_path = labeled_files[0]\n",
    "    out_path = incident_dir / (csv_path.stem + \"_features.csv\")\n",
    "    \n",
    "    if SKIP_EXISTING and out_path.exists():\n",
    "        print(f\"\\n[SKIP] Already exists: {out_path.name}\")\n",
    "        skipped += 1\n",
    "        try:\n",
    "            existing_df = pd.read_csv(out_path)\n",
    "            all_features_dfs.append(existing_df)\n",
    "        except:\n",
    "            pass\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        features_df = process_single_file(csv_path, out_path)\n",
    "        if features_df is not None:\n",
    "            all_features_dfs.append(features_df)\n",
    "            processed += 1\n",
    "        else:\n",
    "            failed += 1\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] {incident_dir.name}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        failed += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Processed: {processed}\")\n",
    "print(f\"Skipped: {skipped}\")\n",
    "print(f\"Failed: {failed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge Anomaly Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MERGING ANOMALY FEATURES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Collect all feature files\n",
    "feature_files = list(INCIDENT_BASE_DIR.glob(\"*/*_features.csv\"))\n",
    "print(f\"Found {len(feature_files)} feature files\")\n",
    "\n",
    "all_anomalies = []\n",
    "\n",
    "for f in sorted(feature_files):\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        anomaly_df = df[df['label'] != 'normal']\n",
    "        if not anomaly_df.empty:\n",
    "            anomaly_df = anomaly_df.copy()\n",
    "            anomaly_df['source_file'] = f.parent.name\n",
    "            all_anomalies.append(anomaly_df)\n",
    "            print(f\"  {f.parent.name}: {len(anomaly_df)} anomalies\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading {f.name}: {e}\")\n",
    "\n",
    "if all_anomalies:\n",
    "    merged_anomalies = pd.concat(all_anomalies, ignore_index=True)\n",
    "    \n",
    "    output_file = INCIDENT_BASE_DIR / \"all_incidents_anomalies_only.csv\"\n",
    "    merged_anomalies.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"MERGED ANOMALIES SUMMARY\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total anomaly samples: {len(merged_anomalies):,}\")\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(merged_anomalies['label'].value_counts())\n",
    "    print(f\"\\nSaved to: {output_file}\")\n",
    "else:\n",
    "    print(\"\\nNo anomalies found to merge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and check one feature file\n",
    "if all_features_dfs:\n",
    "    sample_df = all_features_dfs[0]\n",
    "    print(\"Sample feature file columns:\")\n",
    "    print(sample_df.columns.tolist())\n",
    "    print(f\"\\nShape: {sample_df.shape}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(sample_df.head())\n",
    "    \n",
    "    # Check for logical consistency\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"LOGICAL CONSISTENCY CHECK\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check origin consistency\n",
    "    origin_issues = sample_df[(sample_df['origin_0'] == 0) & \n",
    "                              (sample_df['origin_2'] == 0) & \n",
    "                              (sample_df['origin_changes'] > 0)]\n",
    "    if len(origin_issues) > 0:\n",
    "        print(f\"WARNING: {len(origin_issues)} rows have origin_changes > 0 but origin_0=origin_2=0\")\n",
    "    else:\n",
    "        print(\"Origin features: OK\")\n",
    "    \n",
    "    # Check edit distance consistency\n",
    "    ed_issues = sample_df[(sample_df['edit_distance_avg'] > 0) & \n",
    "                          (sample_df['edit_distance_max'] == 0)]\n",
    "    if len(ed_issues) > 0:\n",
    "        print(f\"WARNING: {len(ed_issues)} rows have edit_distance_avg > 0 but max=0\")\n",
    "    else:\n",
    "        print(\"Edit distance features: OK\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
