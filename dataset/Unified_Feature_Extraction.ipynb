{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unified BGP Feature Extraction\n",
    "\n",
    "This notebook extracts consistent features from BGP update data for both:\n",
    "- **Normal traffic data** (from RIPE RRC collectors)\n",
    "- **Anomaly/Incident data** (from labeled incident datasets)\n",
    "\n",
    "## Features Extracted (27 total)\n",
    "\n",
    "| Category | Features |\n",
    "|----------|----------|\n",
    "| Volume | announcements, withdrawals, nlri_ann, dups |\n",
    "| Origin | origin_0 (IGP), origin_2 (INCOMPLETE), origin_changes |\n",
    "| Implicit Withdrawals | imp_wd, imp_wd_spath, imp_wd_dpath |\n",
    "| AS Path | as_path_max, unique_as_path_max |\n",
    "| Edit Distance | edit_distance_avg, edit_distance_max, edit_distance_dict_0-6, edit_distance_unique_dict_0-1 |\n",
    "| Rare AS | number_rare_ases, rare_ases_avg |\n",
    "| Stability | nadas, flaps |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - MODIFY THESE PATHS FOR YOUR ENVIRONMENT\n",
    "# =============================================================================\n",
    "\n",
    "# Mode selection: 'incident' or 'normal'\n",
    "PROCESSING_MODE = 'incident'  # Change to 'normal' for normal traffic\n",
    "\n",
    "# For incident data processing\n",
    "INCIDENT_BASE_DIR = Path(\"/home/smotaali/BGP_Traffic_Generation/RIPE/RIPE_INCIDENTS\")\n",
    "\n",
    "# For normal traffic processing\n",
    "NORMAL_INPUT_FILE = Path(\"/home/smotaali/BGP_Traffic_Generation/RIPE/normal_traffic.csv\")\n",
    "NORMAL_OUTPUT_FILE = Path(\"/home/smotaali/BGP_Traffic_Generation/RIPE/normal_traffic_features.csv\")\n",
    "\n",
    "# Feature extraction settings\n",
    "WINDOW_SIZE = '1s'  # Time window for feature aggregation\n",
    "RARE_AS_THRESHOLD = 3  # ASNs appearing less than this are considered \"rare\"\n",
    "SKIP_EXISTING = True  # Skip incidents that already have features extracted\n",
    "\n",
    "print(f\"Processing mode: {PROCESSING_MODE}\")\n",
    "print(f\"Window size: {WINDOW_SIZE}\")\n",
    "print(f\"Rare AS threshold: {RARE_AS_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edit_distance(as_path1, as_path2):\n",
    "    \"\"\"\n",
    "    Calculate Levenshtein edit distance between two AS paths.\n",
    "    \"\"\"\n",
    "    if not as_path1 or not as_path2:\n",
    "        return 0\n",
    "\n",
    "    def path_to_list(path):\n",
    "        if isinstance(path, int):\n",
    "            return [path]\n",
    "        if isinstance(path, list):\n",
    "            return path\n",
    "        if isinstance(path, str):\n",
    "            path = path.replace('{', '').replace('}', '')\n",
    "            return [int(a) for a in path.split() if a.isdigit()]\n",
    "        return []\n",
    "\n",
    "    list1 = path_to_list(as_path1)\n",
    "    list2 = path_to_list(as_path2)\n",
    "\n",
    "    if not list1 or not list2:\n",
    "        return 0\n",
    "\n",
    "    m, n = len(list1), len(list2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if list1[i-1] == list2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "def get_path_length(as_path):\n",
    "    \"\"\"\n",
    "    Get the length of an AS path.\n",
    "    \"\"\"\n",
    "    if pd.isnull(as_path) or as_path == '':\n",
    "        return 0\n",
    "    if isinstance(as_path, int):\n",
    "        return 1\n",
    "    if isinstance(as_path, str):\n",
    "        as_path = as_path.replace('{', '').replace('}', '')\n",
    "        return len([a for a in as_path.split() if a.isdigit()])\n",
    "    return 0\n",
    "\n",
    "\n",
    "def attributes_are_same(row1, row2, attrs=None):\n",
    "    \"\"\"\n",
    "    Check if BGP attributes are the same between two announcements.\n",
    "    \"\"\"\n",
    "    if attrs is None:\n",
    "        attrs = ['AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "\n",
    "    for attr in attrs:\n",
    "        if attr in row1.index and attr in row2.index:\n",
    "            v1, v2 = row1[attr], row2[attr]\n",
    "            if pd.isna(v1) and pd.isna(v2):\n",
    "                continue\n",
    "            if pd.isna(v1) or pd.isna(v2):\n",
    "                return False\n",
    "            if v1 != v2:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "print(\"Helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nadas_and_flaps(df_window, entry_type_col='Entry_Type'):\n",
    "    \"\"\"\n",
    "    Calculate NADAS and FLAPS using proper state-based tracking.\n",
    "\n",
    "    NADAS: Re-announcement after withdrawal with DIFFERENT attributes\n",
    "    FLAPS: Re-announcement after withdrawal with SAME attributes\n",
    "    \"\"\"\n",
    "    nadas_count = 0\n",
    "    flap_count = 0\n",
    "\n",
    "    withdrawal_types = ['W', 'WITHDRAW', 'WITHDRAW_MP_UNREACH_NLRI_AFI2']\n",
    "    announce_types = ['A', 'ANNOUNCE']\n",
    "\n",
    "    df_sorted = df_window.sort_values('Timestamp')\n",
    "    prefix_state = {}\n",
    "\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        key = (row.get('Prefix', ''), row.get('Peer_IP', ''))\n",
    "        entry_type = row.get(entry_type_col, '')\n",
    "\n",
    "        is_announce = entry_type in announce_types\n",
    "        is_withdraw = entry_type in withdrawal_types\n",
    "\n",
    "        if is_announce:\n",
    "            if key in prefix_state and prefix_state[key].get('withdrawn', False):\n",
    "                last_ann = prefix_state[key].get('last_ann')\n",
    "                if last_ann is not None:\n",
    "                    if attributes_are_same(last_ann, row):\n",
    "                        flap_count += 1\n",
    "                    else:\n",
    "                        nadas_count += 1\n",
    "                else:\n",
    "                    nadas_count += 1\n",
    "                prefix_state[key]['withdrawn'] = False\n",
    "\n",
    "            prefix_state.setdefault(key, {})\n",
    "            prefix_state[key]['last_ann'] = row\n",
    "            prefix_state[key]['withdrawn'] = False\n",
    "\n",
    "        elif is_withdraw:\n",
    "            if key in prefix_state:\n",
    "                prefix_state[key]['withdrawn'] = True\n",
    "            else:\n",
    "                prefix_state[key] = {'last_ann': None, 'withdrawn': True}\n",
    "\n",
    "    return nadas_count, flap_count\n",
    "\n",
    "print(\"NADAS/FLAPS function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main Feature Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df_window, entry_type_col='Entry_Type'):\n",
    "    \"\"\"\n",
    "    Extract all BGP features from a time window.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "\n",
    "    announce_types = ['A', 'ANNOUNCE']\n",
    "    withdrawal_types = ['W', 'WITHDRAW', 'WITHDRAW_MP_UNREACH_NLRI_AFI2']\n",
    "\n",
    "    announcements = df_window[df_window[entry_type_col].isin(announce_types)]\n",
    "    withdrawals = df_window[df_window[entry_type_col].isin(withdrawal_types)]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1-3. VOLUME FEATURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    features['announcements'] = len(announcements)\n",
    "    features['withdrawals'] = len(withdrawals)\n",
    "    features['nlri_ann'] = announcements['Prefix'].nunique() if not announcements.empty else 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. DUPLICATES\n",
    "    # -------------------------------------------------------------------------\n",
    "    if not announcements.empty:\n",
    "        dup_cols = ['Peer_IP', 'Peer_ASN', 'Prefix', 'AS_Path', 'Origin',\n",
    "                    'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "        dup_cols = [c for c in dup_cols if c in announcements.columns]\n",
    "\n",
    "        if dup_cols:\n",
    "            counts = announcements.groupby(dup_cols, dropna=False).size()\n",
    "            features['dups'] = sum(c - 1 for c in counts if c > 1)\n",
    "        else:\n",
    "            features['dups'] = 0\n",
    "    else:\n",
    "        features['dups'] = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5-7. ORIGIN ATTRIBUTES\n",
    "    # -------------------------------------------------------------------------\n",
    "    if not announcements.empty and 'Origin' in announcements.columns:\n",
    "        origin_counts = announcements['Origin'].value_counts()\n",
    "        features['origin_0'] = origin_counts.get('IGP', 0)\n",
    "        features['origin_2'] = origin_counts.get('INCOMPLETE', 0)\n",
    "        unique_origins = announcements.groupby('Prefix')['Origin'].nunique()\n",
    "        features['origin_changes'] = (unique_origins > 1).sum()\n",
    "    else:\n",
    "        features['origin_0'] = 0\n",
    "        features['origin_2'] = 0\n",
    "        features['origin_changes'] = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 8-10. IMPLICIT WITHDRAWALS\n",
    "    # -------------------------------------------------------------------------\n",
    "    imp_wd = 0\n",
    "    imp_wd_spath = 0\n",
    "    imp_wd_dpath = 0\n",
    "    edit_distances = []\n",
    "    edit_distance_dict = defaultdict(list)\n",
    "\n",
    "    attrs_to_check = ['AS_Path', 'Origin', 'Next_Hop', 'MED', 'Local_Pref', 'Communities']\n",
    "    attrs_available = [a for a in attrs_to_check if a in announcements.columns]\n",
    "\n",
    "    if not announcements.empty and len(attrs_available) > 0:\n",
    "        for (prefix, peer), grp in announcements.groupby(['Prefix', 'Peer_IP']):\n",
    "            if len(grp) < 2:\n",
    "                continue\n",
    "\n",
    "            grp = grp.sort_values('Timestamp')\n",
    "            prev = None\n",
    "\n",
    "            for _, row in grp.iterrows():\n",
    "                if prev is not None:\n",
    "                    changed = False\n",
    "                    as_path_changed = False\n",
    "\n",
    "                    for attr in attrs_available:\n",
    "                        pv, cv = prev.get(attr), row.get(attr)\n",
    "                        pv_nan, cv_nan = pd.isna(pv), pd.isna(cv)\n",
    "\n",
    "                        if pv_nan and cv_nan:\n",
    "                            continue\n",
    "                        if pv_nan or cv_nan or pv != cv:\n",
    "                            changed = True\n",
    "                            if attr == 'AS_Path':\n",
    "                                as_path_changed = True\n",
    "\n",
    "                    if changed:\n",
    "                        imp_wd += 1\n",
    "                        if as_path_changed:\n",
    "                            imp_wd_dpath += 1\n",
    "                            d = calculate_edit_distance(\n",
    "                                prev.get('AS_Path', ''),\n",
    "                                row.get('AS_Path', '')\n",
    "                            )\n",
    "                            edit_distances.append(d)\n",
    "                            edit_distance_dict[prefix].append(d)\n",
    "                        else:\n",
    "                            imp_wd_spath += 1\n",
    "\n",
    "                prev = row\n",
    "\n",
    "    features['imp_wd'] = imp_wd\n",
    "    features['imp_wd_spath'] = imp_wd_spath\n",
    "    features['imp_wd_dpath'] = imp_wd_dpath\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 11-12. AS PATH METRICS\n",
    "    # -------------------------------------------------------------------------\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        valid_paths = announcements[\n",
    "            announcements['AS_Path'].notna() &\n",
    "            (announcements['AS_Path'] != '')\n",
    "        ]\n",
    "\n",
    "        if not valid_paths.empty:\n",
    "            lengths = valid_paths['AS_Path'].apply(get_path_length)\n",
    "            features['as_path_max'] = int(lengths.max()) if not lengths.empty else 0\n",
    "            unique_paths = valid_paths.groupby('Prefix')['AS_Path'].nunique()\n",
    "            features['unique_as_path_max'] = int(unique_paths.max()) if not unique_paths.empty else 0\n",
    "        else:\n",
    "            features['as_path_max'] = 0\n",
    "            features['unique_as_path_max'] = 0\n",
    "    else:\n",
    "        features['as_path_max'] = 0\n",
    "        features['unique_as_path_max'] = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 13-21. EDIT DISTANCE FEATURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    if edit_distances:\n",
    "        features['edit_distance_avg'] = float(np.mean(edit_distances))\n",
    "        features['edit_distance_max'] = int(max(edit_distances))\n",
    "\n",
    "        dist_counter = Counter(edit_distances)\n",
    "        for i in range(7):\n",
    "            features[f'edit_distance_dict_{i}'] = dist_counter.get(i, 0)\n",
    "\n",
    "        unique_ed = defaultdict(int)\n",
    "        for prefix, dists in edit_distance_dict.items():\n",
    "            for d in set(dists):\n",
    "                unique_ed[d] += 1\n",
    "\n",
    "        for i in range(2):\n",
    "            features[f'edit_distance_unique_dict_{i}'] = unique_ed.get(i, 0)\n",
    "    else:\n",
    "        features['edit_distance_avg'] = 0.0\n",
    "        features['edit_distance_max'] = 0\n",
    "        for i in range(7):\n",
    "            features[f'edit_distance_dict_{i}'] = 0\n",
    "        for i in range(2):\n",
    "            features[f'edit_distance_unique_dict_{i}'] = 0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 22-23. RARE AS FEATURES\n",
    "    # -------------------------------------------------------------------------\n",
    "    if not announcements.empty and 'AS_Path' in announcements.columns:\n",
    "        all_asns = []\n",
    "\n",
    "        for as_path in announcements['AS_Path']:\n",
    "            if pd.isnull(as_path) or as_path == '':\n",
    "                continue\n",
    "            path_str = str(as_path).replace('{', '').replace('}', '')\n",
    "            all_asns.extend([a for a in path_str.split() if a.isdigit()])\n",
    "\n",
    "        if all_asns:\n",
    "            asn_counts = Counter(all_asns)\n",
    "            rare_asns = [a for a, c in asn_counts.items() if c < RARE_AS_THRESHOLD]\n",
    "            features['number_rare_ases'] = len(rare_asns)\n",
    "            features['rare_ases_avg'] = len(rare_asns) / len(all_asns)\n",
    "        else:\n",
    "            features['number_rare_ases'] = 0\n",
    "            features['rare_ases_avg'] = 0.0\n",
    "    else:\n",
    "        features['number_rare_ases'] = 0\n",
    "        features['rare_ases_avg'] = 0.0\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 24-25. NADAS AND FLAPS (State-based)\n",
    "    # -------------------------------------------------------------------------\n",
    "    nadas, flaps = calculate_nadas_and_flaps(df_window, entry_type_col)\n",
    "    features['nadas'] = nadas\n",
    "    features['flaps'] = flaps\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # LABEL AGGREGATION (for incident data)\n",
    "    # -------------------------------------------------------------------------\n",
    "    if 'Label' in df_window.columns:\n",
    "        labels = df_window['Label'].value_counts()\n",
    "        if not labels.empty:\n",
    "            # Use majority label, prefer non-normal if tie\n",
    "            abnormal = [l for l in labels.index if l != 'normal']\n",
    "            if abnormal:\n",
    "                features['label'] = abnormal[0]\n",
    "            else:\n",
    "                features['label'] = 'normal'\n",
    "        else:\n",
    "            features['label'] = 'unknown'\n",
    "    else:\n",
    "        features['label'] = 'unknown'\n",
    "\n",
    "    if 'Incident' in df_window.columns:\n",
    "        features['Incident'] = df_window['Incident'].iloc[0]\n",
    "\n",
    "    return features\n",
    "\n",
    "print(\"Feature extraction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_dataframe(df):\n",
    "    \"\"\"\n",
    "    Standardize DataFrame column names and types for consistent processing.\n",
    "    Handles both incident data (Entry_Type='A'/'W') and normal data (Subtype='ANNOUNCE'/'WITHDRAW').\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Standardize timestamp\n",
    "    if 'Timestamp' not in df.columns:\n",
    "        if 'Time' in df.columns:\n",
    "            df['Timestamp'] = pd.to_datetime(df['Time'])\n",
    "        else:\n",
    "            raise ValueError(\"No timestamp column found (expected 'Time' or 'Timestamp')\")\n",
    "    else:\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "\n",
    "    # Standardize column names\n",
    "    column_mapping = {\n",
    "        'Peer_AS': 'Peer_ASN',\n",
    "        'Origin_AS': 'Origin',\n",
    "        'Community': 'Communities',\n",
    "        'Subtype': 'Entry_Type'\n",
    "    }\n",
    "\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df.columns and new_name not in df.columns:\n",
    "            df.rename(columns={old_name: new_name}, inplace=True)\n",
    "\n",
    "    entry_type_col = 'Entry_Type'\n",
    "\n",
    "    # Map entry types if needed (ANNOUNCE/WITHDRAW -> A/W)\n",
    "    if entry_type_col in df.columns:\n",
    "        unique_types = df[entry_type_col].unique()\n",
    "        if 'ANNOUNCE' in unique_types:\n",
    "            type_map = {\n",
    "                'ANNOUNCE': 'A',\n",
    "                'WITHDRAW': 'W',\n",
    "                'WITHDRAW_MP_UNREACH_NLRI_AFI2': 'W'\n",
    "            }\n",
    "            df[entry_type_col] = df[entry_type_col].map(lambda x: type_map.get(x, x))\n",
    "\n",
    "    return df, entry_type_col\n",
    "\n",
    "print(\"Standardization function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. File Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_file(input_path, output_path, window_size=WINDOW_SIZE, show_progress=True):\n",
    "    \"\"\"\n",
    "    Process a single BGP data file and extract features.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {input_path.name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Read data\n",
    "    df = pd.read_csv(input_path, low_memory=False)\n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "\n",
    "    # Standardize\n",
    "    df, entry_type_col = standardize_dataframe(df)\n",
    "\n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('Timestamp')\n",
    "\n",
    "    # Get time range\n",
    "    start_time = df['Timestamp'].min()\n",
    "    end_time = df['Timestamp'].max()\n",
    "    print(f\"Time range: {start_time} to {end_time}\")\n",
    "\n",
    "    # Show entry type distribution\n",
    "    print(f\"\\nEntry type distribution:\")\n",
    "    print(df[entry_type_col].value_counts())\n",
    "\n",
    "    # Set index for grouping\n",
    "    df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "    # Extract features per window\n",
    "    features_list = []\n",
    "    window_count = 0\n",
    "\n",
    "    for window_start, window_df in df.groupby(pd.Grouper(freq=window_size)):\n",
    "        if window_df.empty:\n",
    "            continue\n",
    "\n",
    "        window_df = window_df.reset_index()\n",
    "        features = extract_features(window_df, entry_type_col)\n",
    "\n",
    "        if features:\n",
    "            window_end = window_start + pd.Timedelta(window_size)\n",
    "            features['window_start'] = window_start\n",
    "            features['window_end'] = window_end\n",
    "            features_list.append(features)\n",
    "            window_count += 1\n",
    "\n",
    "            if show_progress and window_count % 1000 == 0:\n",
    "                print(f\"  Processed {window_count} windows...\")\n",
    "\n",
    "    print(f\"\\nTotal windows: {window_count}\")\n",
    "\n",
    "    # Save features\n",
    "    if features_list:\n",
    "        features_df = pd.DataFrame(features_list)\n",
    "\n",
    "        # Ensure consistent column order\n",
    "        ordered_cols = [\n",
    "            'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "            'origin_0', 'origin_2', 'origin_changes',\n",
    "            'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "            'as_path_max', 'unique_as_path_max',\n",
    "            'edit_distance_avg', 'edit_distance_max',\n",
    "            'edit_distance_dict_0', 'edit_distance_dict_1', 'edit_distance_dict_2',\n",
    "            'edit_distance_dict_3', 'edit_distance_dict_4', 'edit_distance_dict_5',\n",
    "            'edit_distance_dict_6',\n",
    "            'edit_distance_unique_dict_0', 'edit_distance_unique_dict_1',\n",
    "            'number_rare_ases', 'rare_ases_avg',\n",
    "            'nadas', 'flaps',\n",
    "            'label', 'Incident', 'window_start', 'window_end'\n",
    "        ]\n",
    "\n",
    "        final_cols = [c for c in ordered_cols if c in features_df.columns]\n",
    "        extra_cols = [c for c in features_df.columns if c not in final_cols]\n",
    "        final_cols.extend(extra_cols)\n",
    "\n",
    "        features_df = features_df[final_cols]\n",
    "        features_df.to_csv(output_path, index=False)\n",
    "        \n",
    "        print(f\"\\nSaved features to: {output_path}\")\n",
    "        print(f\"Shape: {features_df.shape}\")\n",
    "\n",
    "        return features_df\n",
    "    else:\n",
    "        print(f\"No features extracted!\")\n",
    "        return None\n",
    "\n",
    "print(\"File processing function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Process Incident Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESSING_MODE == 'incident':\n",
    "    print(\"=\"*70)\n",
    "    print(\"PROCESSING INCIDENT DATA\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nBase directory: {INCIDENT_BASE_DIR}\")\n",
    "    \n",
    "    # Find all incident directories\n",
    "    incident_dirs = [d for d in INCIDENT_BASE_DIR.iterdir() if d.is_dir() and d.name != 'temp_mrt']\n",
    "    print(f\"Found {len(incident_dirs)} incident directories\")\n",
    "    \n",
    "    # Summary\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    failed = 0\n",
    "    all_features = []\n",
    "    \n",
    "    for incident_dir in sorted(incident_dirs):\n",
    "        # Find labeled CSV file\n",
    "        labeled_files = list(incident_dir.glob(\"*_labeled.csv\"))\n",
    "        \n",
    "        if not labeled_files:\n",
    "            print(f\"\\n[SKIP] No labeled file in {incident_dir.name}\")\n",
    "            continue\n",
    "        \n",
    "        csv_path = labeled_files[0]\n",
    "        out_path = incident_dir / (csv_path.stem + \"_features.csv\")\n",
    "        \n",
    "        if SKIP_EXISTING and out_path.exists():\n",
    "            print(f\"\\n[SKIP] Already exists: {out_path.name}\")\n",
    "            skipped += 1\n",
    "            # Load existing features for summary\n",
    "            try:\n",
    "                existing_df = pd.read_csv(out_path)\n",
    "                all_features.append(existing_df)\n",
    "            except:\n",
    "                pass\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            features_df = process_single_file(csv_path, out_path)\n",
    "            if features_df is not None:\n",
    "                all_features.append(features_df)\n",
    "                processed += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "        except Exception as e:\n",
    "            print(f\"\\n[ERROR] {incident_dir.name}: {e}\")\n",
    "            failed += 1\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"INCIDENT PROCESSING COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Processed: {processed}\")\n",
    "    print(f\"Skipped: {skipped}\")\n",
    "    print(f\"Failed: {failed}\")\n",
    "else:\n",
    "    print(\"Skipping incident processing (PROCESSING_MODE != 'incident')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Normal Traffic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESSING_MODE == 'normal':\n",
    "    print(\"=\"*70)\n",
    "    print(\"PROCESSING NORMAL TRAFFIC DATA\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if NORMAL_INPUT_FILE.exists():\n",
    "        features_df = process_single_file(NORMAL_INPUT_FILE, NORMAL_OUTPUT_FILE)\n",
    "        print(\"\\nNormal traffic processing complete!\")\n",
    "    else:\n",
    "        print(f\"\\nInput file not found: {NORMAL_INPUT_FILE}\")\n",
    "else:\n",
    "    print(\"Skipping normal traffic processing (PROCESSING_MODE != 'normal')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all features for visualization\n",
    "if PROCESSING_MODE == 'incident' and all_features:\n",
    "    combined_df = pd.concat(all_features, ignore_index=True)\n",
    "    print(f\"Combined features shape: {combined_df.shape}\")\n",
    "    print(f\"\\nLabel distribution:\")\n",
    "    print(combined_df['label'].value_counts())\n",
    "elif PROCESSING_MODE == 'normal' and 'features_df' in dir() and features_df is not None:\n",
    "    combined_df = features_df\n",
    "    print(f\"Features shape: {combined_df.shape}\")\n",
    "else:\n",
    "    print(\"No features available for visualization\")\n",
    "    combined_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_df is not None:\n",
    "    # Feature columns (exclude metadata)\n",
    "    feature_cols = [\n",
    "        'announcements', 'withdrawals', 'nlri_ann', 'dups',\n",
    "        'origin_0', 'origin_2', 'origin_changes',\n",
    "        'imp_wd', 'imp_wd_spath', 'imp_wd_dpath',\n",
    "        'as_path_max', 'unique_as_path_max',\n",
    "        'nadas', 'flaps'\n",
    "    ]\n",
    "    feature_cols = [c for c in feature_cols if c in combined_df.columns]\n",
    "    \n",
    "    # Create visualization\n",
    "    n_cols = 4\n",
    "    n_rows = (len(feature_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 3*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(feature_cols):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        if 'label' in combined_df.columns:\n",
    "            for label in combined_df['label'].unique():\n",
    "                data = combined_df[combined_df['label'] == label][col]\n",
    "                ax.hist(data, bins=50, alpha=0.5, label=label)\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.hist(combined_df[col], bins=50, alpha=0.7)\n",
    "        \n",
    "        ax.set_title(col)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(feature_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nSaved: feature_distributions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if combined_df is not None and 'label' in combined_df.columns:\n",
    "    # Feature statistics by label\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FEATURE STATISTICS BY LABEL\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    stats = combined_df.groupby('label')[numeric_cols].agg(['mean', 'std', 'max'])\n",
    "    \n",
    "    # Display key features\n",
    "    key_features = ['announcements', 'withdrawals', 'nlri_ann', 'imp_wd', 'nadas', 'flaps']\n",
    "    key_features = [f for f in key_features if f in numeric_cols]\n",
    "    \n",
    "    for feature in key_features:\n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(stats[feature].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merge Anomaly Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROCESSING_MODE == 'incident':\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"MERGING ANOMALY FEATURES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Collect all feature files\n",
    "    feature_files = list(INCIDENT_BASE_DIR.glob(\"*/*_features.csv\"))\n",
    "    print(f\"Found {len(feature_files)} feature files\")\n",
    "    \n",
    "    all_anomalies = []\n",
    "    \n",
    "    for f in feature_files:\n",
    "        try:\n",
    "            df = pd.read_csv(f)\n",
    "            # Filter for anomalies only\n",
    "            anomaly_df = df[df['label'] != 'normal']\n",
    "            if not anomaly_df.empty:\n",
    "                anomaly_df['source_file'] = f.parent.name\n",
    "                all_anomalies.append(anomaly_df)\n",
    "                print(f\"  {f.parent.name}: {len(anomaly_df)} anomalies\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error reading {f}: {e}\")\n",
    "    \n",
    "    if all_anomalies:\n",
    "        merged_anomalies = pd.concat(all_anomalies, ignore_index=True)\n",
    "        \n",
    "        # Save merged file\n",
    "        output_file = INCIDENT_BASE_DIR / \"all_incidents_anomalies_only.csv\"\n",
    "        merged_anomalies.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"MERGED ANOMALIES SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total anomaly samples: {len(merged_anomalies):,}\")\n",
    "        print(f\"\\nLabel distribution:\")\n",
    "        print(merged_anomalies['label'].value_counts())\n",
    "        print(f\"\\nSaved to: {output_file}\")\n",
    "    else:\n",
    "        print(\"\\nNo anomalies found to merge!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE EXTRACTION COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if PROCESSING_MODE == 'incident':\n",
    "    print(f\"\\nMode: Incident Data Processing\")\n",
    "    print(f\"Base directory: {INCIDENT_BASE_DIR}\")\n",
    "    \n",
    "    if combined_df is not None:\n",
    "        print(f\"\\nTotal windows processed: {len(combined_df):,}\")\n",
    "        print(f\"Total features: {len(combined_df.columns) - 4}\")\n",
    "        \n",
    "        if 'label' in combined_df.columns:\n",
    "            anomaly_count = (combined_df['label'] != 'normal').sum()\n",
    "            print(f\"\\nAnomaly windows: {anomaly_count:,} ({anomaly_count/len(combined_df)*100:.1f}%)\")\n",
    "            print(f\"Normal windows: {len(combined_df) - anomaly_count:,}\")\n",
    "else:\n",
    "    print(f\"\\nMode: Normal Traffic Processing\")\n",
    "    print(f\"Input file: {NORMAL_INPUT_FILE}\")\n",
    "    print(f\"Output file: {NORMAL_OUTPUT_FILE}\")\n",
    "\n",
    "print(f\"\\nWindow size: {WINDOW_SIZE}\")\n",
    "print(f\"Rare AS threshold: {RARE_AS_THRESHOLD}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
